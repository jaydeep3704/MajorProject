00:00 Learn relational database design from
00:02 

00:02 the ground up in this comprehensive
00:05 

00:05 course. You'll learn about SQL
00:07 

00:07 fundamentals, entity relationship
00:09 

00:09 modeling, normalization, data types and
00:12 

00:12 constraints, and more. This course was
00:15 

00:15 created by Dr. How recently published a
00:17 

00:17 book on the topic from Manning
00:19 

00:19 Publications.
00:20 

00:20 >> You are keeping track of 50 club members
00:23 

00:23 in a spreadsheet. It starts simple, a
00:26 

00:26 name, email, and phone numbers.
00:29 

00:29 But then someone gets added twice.
00:32 

00:32 Someone else email gets overwriten.
00:35 

00:35 Now you're trying to track payments,
00:37 

00:37 committees, even RSVPs and it's chaos.
00:41 

00:41 That's when you realize you need
00:43 

00:43 structure. You need a better way to
00:46 

00:46 organize, connect, and protect your
00:48 

00:48 data. This is where databases come in.
00:51 

00:51 But what exactly is a database? Why do
00:54 

00:54 we organize data into tables? And what
00:57 

00:57 makes something a primary key? Why do
00:60 

00:60 all modern apps from social media to
01:02 

01:02 your banks rely on something called an
01:05 

01:05 RMDBS?
01:06 

01:06 In this video, we'll unpack the building
01:09 

01:09 blocks that makes it all work. We will
01:12 

01:12 start by defining a set of fundamental
01:14 

01:14 concepts before giving you the first
01:16 

01:16 taste of SQL RCSQL.
01:20 

01:20 We will start with a simple question.
01:22 

01:22 What are relational databases and
01:24 

01:24 tables?
01:26 

01:26 First, what is a relational database?
01:29 

01:29 A relational database is simply a
01:32 

01:32 collection of tables. So, what's a
01:34 

01:34 table? Then a table is something like a
01:37 

01:37 spreadsheet with data. As you already
01:40 

01:40 know, data in a spreadsheet is organized
01:42 

01:42 into rows and columns. The same can be
01:45 

01:45 said for a table. However, a table is
01:48 

01:48 different from a spreadsheet with data
01:51 

01:51 because a table is used to represent an
01:54 

01:54 entity or a relationship between
01:56 

01:56 entities. So to fully understand what a
01:59 

01:59 table is, we need to understand what an
02:01 

02:01 entity is first. So what exactly is an
02:04 

02:04 entity?
02:06 

02:06 An entity is an object or say a concept
02:10 

02:10 that can be described by many
02:12 

02:12 attributes. Suppose that we are running
02:14 

02:14 an online store that called the sci-fi
02:17 

02:17 collective which sells the sci-fi
02:20 

02:20 products such as time machines that take
02:23 

02:23 you back to only five minutes in case
02:25 

02:25 you forget your keys.
02:28 

02:28 Products sold by the sci-fi collective
02:30 

02:30 are entities and each can be described
02:33 

02:33 by at least four attributes. The name,
02:37 

02:37 description, price and manufacturer.
02:41 

02:41 When we map all the products to a table
02:44 

02:44 in the database supporting the online
02:46 

02:46 store of the sci-fi collective, the four
02:49 

02:49 attributes will be mapped to four
02:52 

02:52 individual columns and each product will
02:55 

02:55 be represented as a row in this table
02:58 

02:58 like what you see here on the screen. In
03:01 

03:01 addition to the four columns, you may
03:03 

03:03 notice that we have added another column
03:06 

03:06 product ID in the preceding table. All
03:09 

03:09 the values in the product ID column are
03:12 

03:12 unique and can be used to identify an
03:15 

03:15 individual row. We call product ID the
03:18 

03:18 primary key of the product table. Each
03:21 

03:21 table should have a single primary key.
03:24 

03:24 We will discuss more on primary keys in
03:27 

03:27 the future videos. That said, a table is
03:31 

03:31 similar to a spreadsheet with data, but
03:34 

03:34 also bears important differences because
03:37 

03:37 a single table is supposed to represent
03:39 

03:39 a single entity or say a single
03:42 

03:42 relationship between different entities.
03:44 

03:44 If you try to squeeze more than one
03:47 

03:47 entities together into a single table,
03:50 

03:50 bad things can happen. For example, if
03:53 

03:53 we decide to store the information of
03:55 

03:55 customers and products into one table
03:59 

03:59 for the sci-fi collective store. For
04:02 

04:02 example, the table may look like this.
04:05 

04:05 This table is a typically poorly
04:07 

04:07 designed table. You can spot the data
04:10 

04:10 redundancy issues first like this.
04:13 

04:13 Beyond that, such a design can cause
04:16 

04:16 many unexpected problems. For example,
04:20 

04:20 if a customer's information appears in
04:23 

04:23 only one row when we want to delete a
04:26 

04:26 product, we will have to delete the
04:29 

04:29 customer in the same row from our
04:31 

04:31 database. This problem is known as
04:35 

04:35 delete anomaly.
04:37 

04:37 For another example, from time to time,
04:40 

04:40 we need to insert into this table a
04:43 

04:43 product that no customers have bought
04:45 

04:45 yet. But the table requires us to
04:48 

04:48 provide a valid customer information
04:51 

04:51 whenever we try to add a new row into
04:53 

04:53 this table. The contradicting
04:55 

04:55 requirements leave us in an awkward
04:57 

04:57 situation. We can't add any new products
04:60 

05:00 at all. This problem is known as the
05:03 

05:03 insertion anomaly. In short, a table can
05:07 

05:07 only represent a single entity or a
05:10 

05:10 single relationship between different
05:12 

05:12 entities. Now we have come to know quite
05:15 

05:15 a few important concepts. Let's move to
05:18 

05:18 the next the relational database
05:21 

05:21 management system short for RDBMS.
05:25 

05:25 What's an RDBMS?
05:28 

05:28 First an RDBMS is a software. is
05:32 

05:32 responsible for interacting with the
05:34 

05:34 underlying hardware and operating system
05:37 

05:37 so that it can take care of how to
05:39 

05:39 physically store and manage data in
05:42 

05:42 relational databases.
05:44 

05:44 You may be familiar with some commonly
05:46 

05:46 used RDBMS such as MySQL, Marin DB or
05:51 

05:51 Postgress. When you need to deploy a
05:54 

05:54 database that you designed, you will
05:56 

05:56 need to interact with one of the
05:58 

05:58 available RDBMS on the market.
06:02 

06:02 So to link RDBMS back to the concepts of
06:05 

06:05 database and tables and RDBMS is used to
06:10 

06:10 manage and maintain different databases.
06:13 

06:13 A database organizes data into different
06:16 

06:16 tables. Another important purpose an
06:19 

06:19 RDBMS serves is to provide tools and
06:22 

06:22 functions to manage databases.
06:26 

06:26 One of the most notable tools that
06:28 

06:28 nearly all RDBMS supports is the
06:32 

06:32 structured query language, short for SQL
06:35 

06:35 or the SQL, a programming language that
06:38 

06:38 you can use to create, modify, and query
06:42 

06:42 data stored in tables. Although
06:44 

06:44 different RDBMS vendors may have
06:47 

06:47 implemented their own variations and
06:49 

06:49 extensions, SQL has been standardized
06:52 

06:52 over decades. As a result, the
06:55 

06:55 consistency of SQL among different RDBMS
06:58 

06:58 is very very high and as variations are
07:01 

07:01 small. In the context of database design
07:04 

07:04 book, of course, SQL may seem less
07:07 

07:07 important. Database design doesn't
07:09 

07:09 necessarily require you to use SQL. Some
07:13 

07:13 database systems even come with
07:15 

07:15 graphical tools to generate SQL scripts
07:17 

07:17 that automatically create databases and
07:20 

07:20 tables based on your design. But having
07:23 

07:23 some understanding of SQL can make it
07:26 

07:26 easier to learn database design,
07:28 

07:28 especially when it relates to structural
07:31 

07:31 or design problems such as data
07:33 

07:33 integrity, optimization, and
07:36 

07:36 scalability. After all, SQL is a
07:39 

07:39 standardized language that most RDBMS
07:42 

07:42 uses. So knowing SQL will allow you to
07:46 

07:46 rely less on the graphical tools and
07:48 

07:48 work with different types of database
07:50 

07:50 systems. Next, you will learn SQL by
07:54 

07:54 executing your first SQL query. We will
07:57 

07:57 use the example that you saw in the
07:58 

07:58 preceding section, the database of the
08:01 

08:01 sci-fi collective. The database contains
08:04 

08:04 many tables. Okay, I mean the database
08:06 

08:06 for the sci-fi collective, it contains
08:09 

08:09 many tables, but the product table is
08:12 

08:12 all you need to focus your attention on
08:14 

08:14 for now. The product table looks like
08:17 

08:17 this.
08:18 

08:18 The goal of our first query is simple.
08:22 

08:22 We aim at getting the product names
08:25 

08:25 whose price is above $20 from the
08:29 

08:29 product table. The query says select a
08:32 

08:32 name from product where price is bigger
08:35 

08:35 than 20 with a semicolon at the end.
08:38 

08:38 Before we break the query down and get
08:41 

08:41 to its details, let's run this query in
08:44 

08:44 an actual database first.
08:48 

08:48 You can find the GitHub repo that
08:50 

08:50 contains the data we showed to you in
08:52 

08:52 this video at this address. It's also
08:55 

08:55 linked in the description of this video.
08:58 

08:58 You can clone the GitHub repo to your
08:60 

08:60 local machine or download it by clicking
09:03 

09:03 the button of code here.
09:05 

09:05 Then there's a choice named download as
09:08 

09:08 a zip file. I already have the ripple
09:11 

09:11 cloned locally on my machine. So I'm
09:13 

09:13 going to open it in the VS code
09:15 

09:15 environment. Everything you need is from
09:18 

09:18 the folder named chapter one. Here in
09:22 

09:22 this folder you can see a set of SQL
09:24 

09:24 scripts and a readme file.
09:27 

09:27 Here
09:29 

09:29 you can follow the instruction in the
09:31 

09:31 readmi file to install your preferred
09:33 

09:33 database system and run the prepared
09:36 

09:36 scripts accordingly on your local
09:38 

09:38 machine. However, here we will go with
09:40 

09:40 the easiest approach which is to use a
09:43 

09:43 tool named SQL online. Essentially, SQL
09:47 

09:47 online is a database running on the
09:50 

09:50 cloud. Let's open that.
09:53 

09:53 As you can see in the left sidebar, you
09:56 

09:56 can choose the target database system
09:58 

09:58 that you wanted to use. So, you have
10:00 

10:00 one, two, three, four, four different
10:02 

10:02 options including SQLit, Maran DB,
10:05 

10:05 Postgress, and MSSQL. So we are going to
10:09 

10:09 go with this choice sketlet. That means
10:12 

10:12 we need to firstly import and run the
10:16 

10:16 prepared SQL script from the chapter 1
10:19 

10:19 folder. In our case that file will be
10:22 

10:22 SQL online.sql.
10:25 

10:25 So we are going to import this file. If
10:31 

10:31 you ever get confused, you can use
10:34 

10:34 readme as a reference. It basically
10:37 

10:37 lists everything you need to do to get a
10:40 

10:40 things done. Okay. So this is the target
10:42 

10:42 file skit online.sql.
10:46 

10:46 So let's import that file. We are going
10:49 

10:49 to click this import button. After that
10:52 

10:52 we are going to click open and try our
10:55 

10:55 best to find the file. In my case that
10:58 

10:58 file is saved in this folder named
11:00 

11:00 graing relational database design. then
11:03 

11:03 chapter one then escalate online. So I'm
11:07 

11:07 going to click open. After that I'm
11:10 

11:10 going to click the okay button.
11:13 

11:13 There would be some info popped up
11:16 

11:16 telling you that the script has run. So
11:19 

11:19 what does the script do? The script
11:22 

11:22 handles creating a database and a table
11:25 

11:25 then populated with the same data that
11:27 

11:27 you saw from the last few slides. Before
11:30 

11:30 we write our first SQL query, let's take
11:32 

11:32 a look at the table again in this
11:34 

11:34 interface. So we are going to run this
11:37 

11:37 provided SQL query. Select a star which
11:40 

11:40 means everything from the product table
11:43 

11:43 and we are going to click run. As you
11:46 

11:46 can see every row from the table has
11:50 

11:50 been presented here on the screen. Same
11:52 

11:52 as what you saw in the last slide. The
11:55 

11:55 five columns are product ID, name,
11:58 

11:58 description,
11:59 

11:59 price,
12:01 

12:01 and manufacturer. Product ID is the
12:05 

12:05 primary key. Note that all column names
12:07 

12:07 are lowercased. The same can be said for
12:10 

12:10 the table name. The 10 rows representing
12:13 

12:13 10 different products. Five of them are
12:16 

12:16 sold at a price below $20 and the other
12:19 

12:19 five are sold at a price above 20. After
12:23 

12:23 all these steps, you will be ready to
12:25 

12:25 query the product table. You can type
12:27 

12:27 the query into the code editor here on
12:30 

12:30 the SQL online interface. So that will
12:33 

12:33 be select
12:37 

12:37 name
12:39 

12:39 from product
12:42 

12:42 where price bigger than 20. After that
12:46 

12:46 don't forget as a semicolon.
12:49 

12:49 In the end, we are going to click run.
12:52 

12:52 As you can see, a single column of five
12:54 

12:54 rows or say five product names are
12:57 

12:57 returned. Congratulations on running
12:60 

12:60 your first SQL query. Just in case if
13:03 

13:03 you have typos in the column name or
13:06 

13:06 table names, SQL will know and complain.
13:10 

13:10 For example, if we misspel the column
13:13 

13:13 name as names instead of name. If we
13:17 

13:17 click run, you are going to see some
13:20 

13:20 complaints from SQLlet. So be careful
13:22 

13:22 with the column names. Well, how does
13:25 

13:25 the query achieve our goal? The part
13:27 

13:27 where price bigger than 20 may be a data
13:30 

13:30 gave away of what it does. The query
13:33 

13:33 retrieves the names of products whose
13:35 

13:35 prices are higher than 20. That's why it
13:38 

13:38 returns only five of them. I mean the
13:41 

13:41 five products that are sold at a price
13:44 

13:44 bigger than 20. Now let's break down the
13:46 

13:46 query and try to digest every bit of it.
13:50 

13:50 You may notice that this query has a lot
13:52 

13:52 of similarities to plain English. To be
13:56 

13:56 specific, SEO is like English, but it
13:59 

13:59 comes with little to no small talk and
14:02 

14:02 you don't have the same freedom in word
14:05 

14:05 choices when it comes to putting
14:06 

14:06 together a SQL query. You must use a set
14:09 

14:09 of SQL clauses and follow some rules. In
14:13 

14:13 your first query, you use the following
14:15 

14:15 three clauses.
14:17 

14:17 The first is the select clause. The
14:20 

14:20 select clause allows you to specify the
14:22 

14:22 columns you want to retrieve from a
14:25 

14:25 table. In your first query, it only
14:28 

14:28 asked for the name column. Thus, the
14:31 

14:31 select statement or say the select
14:33 

14:33 clause was select name. Second, the from
14:37 

14:37 clause specifies the source you want to
14:40 

14:40 retrieve the data from. It can be one or
14:42 

14:42 more tables. In your first query, you
14:45 

14:45 asked only for data from a single
14:48 

14:48 product table. Thus, the from clause was
14:50 

14:50 from product. Third, the wear clause
14:54 

14:54 allows you to specify the conditions to
14:56 

14:56 filter the data retrieved by the select
14:58 

14:58 clause. In your first query, you want
15:01 

15:01 only the names of those products whose
15:03 

15:03 prices are higher than $20. Thus the
15:06 

15:06 query was selected name from product
15:09 

15:09 where price bigger than 20. Last you
15:12 

15:12 should always use a semicolon to
15:14 

15:14 indicate the end of a query. The
15:16 

15:16 semicolon essentially tells the database
15:18 

15:18 system that this is the end of a query
15:21 

15:21 and anything that comes afterward will
15:24 

15:24 belong to a new query. Let's quickly
15:26 

15:26 review what we covered. You saw how
15:28 

15:28 messy a spreadsheet can get and why
15:31 

15:31 structure matters. You learn what are
15:33 

15:33 relational databases and how it
15:35 

15:35 organizes data into tables based on
15:38 

15:38 entities using primary keys. You also
15:41 

15:41 wrote your first various SQL query to
15:43 

15:43 retrieve data from a table. These are
15:45 

15:45 the essential building blocks of every
15:47 

15:47 modern database and they're just the
15:50 

15:50 start. If you want to dive deeper into
15:53 

15:53 database design thinking, the series is
15:56 

15:56 based on the book growing relational
15:58 

15:58 database design which walks through
15:60 

15:60 these ideas step by step with real world
16:03 

16:03 examples. The link is in the first
16:05 

16:05 comment. What was your biggest aha
16:08 

16:08 moment from this introduction or what
16:11 

16:11 still feels fuzzy? Please drop a
16:14 

16:14 comment. I would like to know where you
16:16 

16:16 are at. And if this helps to lay a solid
16:19 

16:19 foundation, feel free to like the video
16:21 

16:21 or subscribe to follow the full series.
16:24 

16:24 In the next video, we'll walk through
16:26 

16:26 the core building blocks of SEL so that
16:29 

16:29 you can start reading and writing SEL
16:31 

16:31 queries with confidence. Have you ever
16:33 

16:33 wondered how databases know exactly what
16:36 

16:36 data to show you and how to crunch the
16:38 

16:38 numbers to? In the next 15 minutes, you
16:42 

16:42 will learn how SQL filtering and
16:44 

16:44 aggregation work together to answer
16:46 

16:46 questions like which customers spend the
16:48 

16:48 most or how many orders were placed last
16:52 

16:52 month. In this video, we will explore
16:55 

16:55 two fundamental concepts that are key to
16:57 

16:57 mastering SQL basics, filtering and
16:60 

16:60 aggregation. In case if you are new to
17:02 

17:02 SQL, we will start with a twominut
17:05 

17:05 refresher of the basic select from wear
17:08 

17:08 query. Let's start with an introduction
17:11 

17:11 to the table that we are going to work
17:13 

17:13 with. First, think about an online store
17:16 

17:16 named the sci-fi collective that sells
17:19 

17:19 only sci-fi products. Imagine that
17:22 

17:22 there's a table representing products in
17:24 

17:24 this database. There are five columns in
17:27 

17:27 this table including product ID, name,
17:31 

17:31 description, price, and a manufacturer.
17:34 

17:34 When the table is populated with data,
17:37 

17:37 it will look like this.
17:40 

17:40 We can ask a simple question that
17:42 

17:42 requires the help from a SQL query like
17:45 

17:45 this.
17:46 

17:46 How do we get the names of the products
17:49 

17:49 that are sold at a price above $20?
17:53 

17:53 This query will help answer this
17:55 

17:55 question. Select a name from product
17:59 

17:59 where price bigger than 20. Then it's
18:02 

18:02 going to be followed by semicolon.
18:05 

18:05 You may notice that this SQL query has a
18:08 

18:08 lot of similarities with plain English.
18:11 

18:11 To be specific, SQL is like English but
18:14 

18:14 with little to no small talks. But you
18:17 

18:17 don't have the same freedom in word
18:19 

18:19 choices when it comes to putting
18:21 

18:21 together a SQL query. You must use a set
18:25 

18:25 of SQL clauses or known as statements
18:29 

18:29 and you also need to follow some strict
18:32 

18:32 rules. In the query that you just saw on
18:35 

18:35 the last slide, you used the following
18:38 

18:38 three clauses. First, the select clause
18:41 

18:41 allows you to specify the columns you
18:44 

18:44 want to retrieve from a table. In this
18:47 

18:47 query, you only asked for the name
18:49 

18:49 column. As a result, the select clause
18:53 

18:53 was select name. Second, the from clause
18:57 

18:57 specifies the source that you want to
18:59 

18:59 retrieve data from. By source, I mean
19:02 

19:02 one or more tables. In this query, you
19:05 

19:05 only asked for the data from the product
19:08 

19:08 table. As a result, the from clause was
19:11 

19:11 from product.
19:14 

19:14 Third, the wear clause allows you to
19:17 

19:17 specify the condition to filter the data
19:20 

19:20 retrieved by the select clause. In your
19:23 

19:23 first query, you only want the names of
19:26 

19:26 the products whose prices are higher
19:29 

19:29 than 20. As a result, the query that you
19:32 

19:32 put it together was select name from
19:34 

19:34 product where price bigger than 20.
19:38 

19:38 Last, you should always use a semicolon
19:40 

19:40 to indicate an end of a query. The
19:43 

19:43 semicolon tells a database system that
19:46 

19:46 this is the end of a SQL query and
19:49 

19:49 anything comes afterward will belong to
19:52 

19:52 a new query. That's a quick refresher of
19:56 

19:56 the SQL basics. Now let's dive deeper
19:58 

19:58 into the filtering topic. Filtering is a
20:01 

20:01 common data retrieval task.
20:04 

20:04 Whenever you need only a subset of data
20:07 

20:07 that meets some criteria, you will need
20:10 

20:10 the help of the wear clause to filter
20:12 

20:12 the data. You just saw the application
20:14 

20:14 of the wear clause in your first SQL
20:17 

20:17 query. However, it's worth to be more
20:20 

20:20 detailed in terms of what it means by a
20:23 

20:23 subset of data. To be more specific, a
20:27 

20:27 subset of data here means rows, a bunch
20:31 

20:31 of rows. The wear clause filters rows
20:34 

20:34 based on the condition that follows. It
20:36 

20:36 doesn't care about the columns at all.
20:39 

20:39 Selecting which columns to include in
20:42 

20:42 the result is the job of the select
20:44 

20:44 clause. For example, if you want both
20:47 

20:47 the name and description columns in the
20:50 

20:50 result from the product table, you can
20:52 

20:52 list both of them one after another in
20:55 

20:55 the select clause and separate them
20:58 

20:58 using a comma like this. In both of
21:00 

21:00 these two queries, the condition of the
21:03 

21:03 work clause is price bigger than 20.
21:06 

21:06 What if we have a different condition
21:08 

21:08 such as products from a specific
21:11 

21:11 manufacturer like Matt inventors? To
21:14 

21:14 achieve this goal, we need to check if
21:16 

21:16 the manufacturer is mad inventors. In
21:19 

21:19 the very condition, the query will look
21:22 

21:22 like this. As you can see, to check
21:24 

21:24 equality in SQL query, it's a single
21:27 

21:27 equal sign. You may also notice that the
21:29 

21:29 manufacturer name is wrapped up in
21:32 

21:32 single quotes. It indicates that this is
21:35 

21:35 a string data type. Does SQL have
21:37 

21:37 different data types? Yes, SQL data can
21:40 

21:40 be broadly divided into six categories
21:43 

21:43 such as numeric data, string data, date
21:47 

21:47 or time and more. In the product table,
21:50 

21:50 the data type of the price column is
21:53 

21:53 numeric. That's why you can filter price
21:55 

21:55 via the logical operators such as bigger
21:59 

21:59 than or bigger than or equal to. The
22:01 

22:01 manufacturer column on the other hand
22:04 

22:04 contains only string data. And that's
22:07 

22:07 why you can compare manufacturer to Matt
22:10 

22:10 inventor's ink and also why this string
22:14 

22:14 matt inventor's ink needs to be wrapped
22:16 

22:16 up in single quote which indicates a
22:19 

22:19 string. We will cover data types in the
22:21 

22:21 future in detail. Now that you know how
22:23 

22:23 to filter both numeric and a string data
22:26 

22:26 at a list, you can create one filter
22:29 

22:29 that combines the two criteria by using
22:32 

22:32 some logical operators such as and or
22:37 

22:37 or. For example, to get products that
22:40 

22:40 are made by the M inventor inc and have
22:43 

22:43 a price below 30, you can construct this
22:46 

22:46 query like this.
22:49 

22:49 The query says select a star from
22:52 

22:52 product where price smaller than 30 and
22:56 

22:56 a manufacturer equal to matt inventors
22:58 

22:58 inc. The end operator combines the two
23:02 

23:02 conditions. Only the rows meet both
23:05 

23:05 conditions will be filtered and included
23:08 

23:08 in the result which means that the
23:10 

23:10 products that are manufactured by Matt
23:13 

23:13 inventor's ink and have a price below
23:16 

23:16 30. By the way, the order of the result
23:19 

23:19 rows don't matter at all. You shouldn't
23:22 

23:22 have any assumptions or expectations in
23:24 

23:24 terms of how the rows are ordered in the
23:27 

23:27 results. The star here indicates all
23:30 

23:30 columns and that's why the result
23:32 

23:32 includes all columns from the product
23:35 

23:35 table. If you want to select all columns
23:37 

23:37 instead of specific ones, you can always
23:40 

23:40 use the help of the star symbol. We
23:42 

23:42 touched on two concepts that are
23:44 

23:44 relevant to filtering including data
23:46 

23:46 types and logical operators. Now that
23:49 

23:49 you have seen enough on filtering, we
23:51 

23:51 are ready to move to the next topic
23:53 

23:53 which is the aggregation.
23:56 

23:56 So what is aggregation about?
23:58 

23:58 Aggregation is about performing
24:00 

24:00 calculations on a set of rows to produce
24:04 

24:04 a single result. By aggregating data,
24:06 

24:06 you can gain insights into the trend and
24:09 

24:09 the patterns in the data that may not be
24:12 

24:12 visible at the individual record level.
24:15 

24:15 For example, you may want to count the
24:18 

24:18 number of rows in the product table. And
24:21 

24:21 that will tell you how many products are
24:23 

24:23 sold by the online store, the sci-fi
24:26 

24:26 collective. To achieve that, you can
24:28 

24:28 write your query like this. Select count
24:32 

24:32 star from product.
24:35 

24:35 If there are 10 rows in total or say 10
24:38 

24:38 products in total from the product
24:40 

24:40 table, the result will look like this.
24:43 

24:43 You may notice that the count follows by
24:47 

24:47 a bracket which is used to wrap up the
24:50 

24:50 star. In other programming languages,
24:53 

24:53 this is a sign for functions. The same
24:55 

24:55 can be said for SQL queries. Count is a
24:59 

24:59 function that counts the number of rows
25:02 

25:02 and it takes a single parameter
25:05 

25:05 regardless of if you gave it a star
25:08 

25:08 representing all columns or a single
25:10 

25:10 column like name. It leads to the same
25:13 

25:13 result because it counts the number of
25:16 

25:16 rows in the product table. Aggregation
25:18 

25:18 is about performing calculation on a set
25:21 

25:21 of rows to produce a single result and
25:24 

25:24 the calculation rely on using some of
25:27 

25:27 the existing functions in SQL. Some of
25:30 

25:30 the most frequently used aggregation
25:32 

25:32 functions include count, sum, average,
25:36 

25:36 max, and mean. As you can see, the names
25:40 

25:40 tell a lot about what they do. For
25:42 

25:42 example, AVG function calculates the
25:46 

25:46 average value in a numeric column. For
25:49 

25:49 another example, max finds a max value
25:52 

25:52 in a column. All functions are
25:55 

25:55 structured the same. The function name
25:57 

25:57 is followed by a run bracket. Most
25:60 

25:60 functions are only applicable to a
26:02 

26:02 single column with proper data type
26:05 

26:05 except for the count function. Let's see
26:08 

26:08 a few examples of the aggregate
26:10 

26:10 functions to get a better sense of how
26:12 

26:12 they get applied. Think about we are
26:15 

26:15 still dealing with the product table
26:17 

26:17 from the sci-fi collective database. Our
26:20 

26:20 goal this time is to calculate the
26:22 

26:22 average price of products that are made
26:25 

26:25 by Matt inventor sync. If you find this
26:28 

26:28 overwhelming, you can take a step back
26:31 

26:31 to get all the prices of the products
26:33 

26:33 that are made by Matt inventor sync
26:36 

26:36 which will be translated to a query like
26:38 

26:38 this. Select price from product where
26:43 

26:43 manufacturer is equal to Matt inventors
26:46 

26:46 inc. The matt inventors inc wrapped up
26:49 

26:49 in a single quotes because it represents
26:51 

26:51 a string. Having access to all the
26:54 

26:54 prices of such targeted products makes
26:56 

26:56 calculating the average price simple.
26:59 

26:59 All you need to do is to apply the
27:01 

27:01 corresponding aggregate function average
27:04 

27:04 to the price column in the select clause
27:07 

27:07 like this.
27:09 

27:09 The result of this query will be a data
27:12 

27:12 point in a single row and a single
27:15 

27:15 column representing the average price.
27:19 

27:19 Its column name by default could simply
27:21 

27:21 be count bracket in many database
27:24 

27:24 systems. If you don't want to settle
27:26 

27:26 with the default column name, you can
27:29 

27:29 use the as clause to rename it like
27:32 

27:32 this. And the result looks like this. As
27:35 

27:35 you can see, the name up to your choice
27:38 

27:38 that follows the as clause will be used
27:41 

27:41 to name this column. Let's look at
27:43 

27:43 another example to learn how to perform
27:45 

27:45 aggregation by groups. This time our
27:48 

27:48 goal is to get the number of products
27:51 

27:51 per manufacturer. This goal requires us
27:54 

27:54 to group products by manufacturer first
27:57 

27:57 before counting the number of products
27:59 

27:59 per group. To achieve this goal, we need
28:03 

28:03 the help from a class we haven't seen
28:05 

28:05 yet. The query that achieves this goal
28:09 

28:09 and the result are presented here on the
28:11 

28:11 screen. The result is literally product
28:15 

28:15 numbers grouped by manufacturer. This
28:18 

28:18 query is using a clause that we didn't
28:21 

28:21 cover in the past, which is a group by
28:24 

28:24 clause. The group by clause in SQL is
28:27 

28:27 used to group rows that have the same
28:31 

28:31 values in one or more columns so that
28:35 

28:35 you can apply aggregate functions to
28:37 

28:37 each group individually. In this
28:40 

28:40 example, we grouped the rows in the
28:42 

28:42 product table by manufacturer. After
28:46 

28:46 that, we applied the count function to
28:49 

28:49 count the number of rows in each group.
28:52 

28:52 That's why this query helps us achieve
28:54 

28:54 the given goal. Now you know what group
28:58 

28:58 by does and how it is used in concert
29:02 

29:02 with aggregate functions. Let's talk
29:04 

29:04 about what you need to pay attention to
29:07 

29:07 when it comes to aggregation by groups.
29:10 

29:10 First, the group by clause is rarely
29:13 

29:13 used alone. It is used with one of the
29:17 

29:17 aggregated functions. In such a case,
29:20 

29:20 you need to remember to include the
29:22 

29:22 column followed by the group by clause
29:25 

29:25 in your select clause.
29:28 

29:28 For example, in this query, as you see
29:30 

29:30 here on the screen, we group rows from
29:34 

29:34 the product table by manufacturer. So,
29:37 

29:37 we need to make sure that the
29:39 

29:39 manufacturer column shows up here in the
29:42 

29:42 select clause. Why? If you forget to do
29:46 

29:46 so, your result will become something
29:48 

29:48 that's very difficult to interpret. For
29:51 

29:51 example, you may forget to include the
29:54 

29:54 manufacturer in the same query that we
29:56 

29:56 just covered. And you will get a result
29:60 

29:60 looking like this.
30:03 

30:03 What does the query do? The query still
30:06 

30:06 does the same thing. A group rows by
30:08 

30:08 manufacturer and then count as the
30:10 

30:10 number of rows per group. However, when
30:14 

30:14 you forget to include the manufacturer
30:16 

30:16 as a column in the select clause, the
30:19 

30:19 manufacturer information won't show up
30:21 

30:21 in the result. So, the product numbers
30:24 

30:24 per group is kind of pointless standing
30:26 

30:26 by itself. Okay, that's one of the
30:29 

30:29 important things about the group by
30:31 

30:31 clause. Another important thing about
30:34 

30:34 the group by clause is that you need to
30:37 

30:37 remember to exclude any irrelevant
30:40 

30:40 columns in the select clause when you
30:43 

30:43 are doing aggregation by group.
30:46 

30:46 Specifically, you need to exclude any
30:50 

30:50 columns that are neither in the group by
30:52 

30:52 clause or the aggregate function.
30:57 

30:57 Otherwise, SQL will complain. For
30:60 

30:60 example, when you try to add another
31:02 

31:02 irrelevant column to this query like the
31:06 

31:06 name column,
31:08 

31:08 you will get an error message like this.
31:11 

31:11 When using group by clause, you need to
31:14 

31:14 make sure two things. First, you need to
31:17 

31:17 include the grouped column in your
31:19 

31:19 select statement. Otherwise, you may end
31:22 

31:22 up with a confusing result. Second, you
31:26 

31:26 need to avoid selecting columns that are
31:28 

31:28 not in the group by clause or inside an
31:31 

31:31 aggregation function. Otherwise, SQL
31:34 

31:34 will throw an error. In case if you
31:36 

31:36 wanted to run the queries that were
31:38 

31:38 covered in this video and replicate what
31:40 

31:40 you saw, you can find everything you
31:42 

31:42 need from this GitHub repository. The
31:46 

31:46 URL of this repository can be found in
31:48 

31:48 the description of this video.
31:52 

31:52 The easiest way to replicate what you
31:54 

31:54 saw from this video is to load the data
31:57 

31:57 into a tool named SQL online.
32:00 

32:00 You can follow the readme files in each
32:03 

32:03 of the corresponding folders. For
32:05 

32:05 example, chapter one to see the detailed
32:08 

32:08 instruction. Of course, you can follow
32:10 

32:10 the instruction to install your
32:12 

32:12 preferred database system locally and
32:15 

32:15 then load the scripts. But we are going
32:17 

32:17 to simply use a tool that makes things
32:21 

32:21 easier for us to start. The tool is
32:23 

32:23 named SQL online. Essentially that's SQL
32:27 

32:27 database system running on the cloud.
32:31 

32:31 Once you follow the instruction in the
32:33 

32:33 readme file to load the corresponding
32:35 

32:35 script from what we prepared for you,
32:39 

32:39 you will be able to run all the queries
32:41 

32:41 that were covered in this video and see
32:43 

32:43 the same results. Let's quickly recap
32:46 

32:46 what we covered. We started with a very
32:49 

32:49 basic SQL query and then explored two
32:52 

32:52 fundamental concepts based on that
32:55 

32:55 filtering and aggregation.
32:57 

32:57 If you find this video helpful, give it
32:60 

32:60 a thumb up, subscribe or drop your
33:02 

33:02 questions in the comments. I would love
33:04 

33:04 to hear what's working for you and what
33:06 

33:06 needs to be changed. If you wanted to go
33:09 

33:09 deeper into database design, this video
33:12 

33:12 is based on the book Groing Relational
33:14 

33:14 Database Design, which walks through
33:16 

33:16 these ideas step by step with real world
33:19 

33:19 examples. You'll find the link to the
33:22 

33:22 book in the first comment. Are you just
33:25 

33:25 starting SQL or need a quick refresher
33:27 

33:27 on the SQL syntax? In this video, we
33:30 

33:30 will walk through how to create tables,
33:33 

33:33 modify them, and manage data. Always
33:35 

33:35 claim non-fluffy examples. No theory,
33:39 

33:39 just the exact SQL that you need to get
33:41 

33:41 us installed. In this video, we will
33:44 

33:44 learn the basics of table and data
33:46 

33:46 management using SQL. Specifically, you
33:50 

33:50 will learn the commands related to table
33:52 

33:52 management and data management. Let's
33:55 

33:55 get started with table management first.
33:58 

33:58 And the first thing you need to know is
33:60 

33:60 how to create tables.
34:02 

34:02 Creating a table using SQL requires the
34:05 

34:05 help from the create table command to
34:08 

34:08 demonstrate how it is used. Let's think
34:11 

34:11 about creating the product table for the
34:14 

34:14 database supporting the online store
34:16 

34:16 named the sci-fi collective. So as you
34:19 

34:19 can see here on the screen, the product
34:22 

34:22 entity has four different features
34:24 

34:24 including name, description, price, and
34:28 

34:28 manufacturer. to create such a table
34:31 

34:31 plus adding a primary key beyond these
34:34 

34:34 four different features. The command
34:37 

34:37 will look like this.
34:39 

34:39 To understand how this create table
34:42 

34:42 command works, we need to answer two
34:45 

34:45 questions at least. First, what is the
34:48 

34:48 general syntax for creating a table
34:50 

34:50 using SQL? Second, what do the different
34:53 

34:53 keywords do in this query example?
34:57 

34:57 What's a general syntax for creating a
34:59 

34:59 table? The general syntax looks like
35:02 

35:02 this. The command starts with create
35:06 

35:06 table keyword. Then it's going to be
35:08 

35:08 followed by the table name of your
35:10 

35:10 choice. In the round bracket, you need
35:13 

35:13 to define all the columns of the table.
35:16 

35:16 Each line will start with a column name
35:19 

35:19 and followed by its corresponding data
35:21 

35:21 type. Lines are separated by commas. You
35:25 

35:25 also need to define the primary key of
35:27 

35:27 the table. You can define that using a
35:29 

35:29 separate line like this. The keyword
35:33 

35:33 primary key is going to be followed by
35:35 

35:35 the column name inside a run bracket. Or
35:39 

35:39 you can put the primary key keyword
35:42 

35:42 after a column which you want to use as
35:44 

35:44 a primary key. That's the general
35:47 

35:47 syntax. The example that we showed you a
35:50 

35:50 few moments ago followed the syntax
35:52 

35:52 strictly.
35:55 

35:55 Product is the table name. Primary key
35:59 

35:59 is followed by the column name product
36:02 

36:02 ID. These two things are required to
36:06 

36:06 create a table. In general, in addition
36:08 

36:08 to that, you may also notice the
36:11 

36:11 different data types for each column.
36:13 

36:13 Product ID is a numeric column, integer
36:17 

36:17 actually. Name, description, and
36:20 

36:20 manufacturer are all string columns.
36:23 

36:23 Price is also numeric. It's a decimal
36:26 

36:26 column. Here we use decimal 52 to define
36:30 

36:30 that the price column will allow five
36:34 

36:34 digits in total and two digits to the
36:36 

36:36 right of the decimal point. In the
36:39 

36:39 future, when it comes to discussing the
36:41 

36:41 choice of appropriate data types for
36:43 

36:43 columns, we will go over the differences
36:46 

36:46 between integers and decimals.
36:50 

36:50 Last, you may also notice that all the
36:53 

36:53 columns except for product ID are
36:55 

36:55 declared not null. That means those
36:59 

36:59 columns don't accept a null values. In
37:02 

37:02 SQL, a null value represents an unknown
37:06 

37:06 value. Similar to when you are trying to
37:08 

37:08 remember someone's name and it's on the
37:11 

37:11 table of your Tom, but is a value either
37:13 

37:13 is missing or unknown. Allowing null
37:16 

37:16 values for columns may lead to SQL to
37:19 

37:19 have unexpected behaviors.
37:21 

37:21 For example, when you try to add a 10
37:24 

37:24 and a null value, it will end up with a
37:26 

37:26 null value. The sum of an unknown value
37:29 

37:29 and a 10 is still unknown. When you do
37:32 

37:32 calculations on no values, all the
37:34 

37:34 results may all end up as null. Then you
37:38 

37:38 may wonder whether the product ID column
37:41 

37:41 as a primary key column would allow null
37:44 

37:44 values and why we didn't declare it as
37:47 

37:47 not null. The reason for that is this
37:51 

37:51 column product ID has already been
37:54 

37:54 declared as the primary key. This will
37:57 

37:57 make sure that the product ID column
37:60 

37:60 will not allow any null values. So
38:03 

38:03 there's no need to further declare this
38:05 

38:05 column as not null. Now you understand
38:09 

38:09 how to create a tables. From time to
38:12 

38:12 time you may need to alter the structure
38:14 

38:14 of a table or even drop a table because
38:17 

38:17 let's face it, sometimes you need to
38:19 

38:19 rearrange furniturees in your data
38:21 

38:21 house. There are many ways to alter a
38:24 

38:24 table such as adding a column, modifying
38:27 

38:27 the data type of a column, or renaming
38:30 

38:30 the entire table.
38:32 

38:32 You can rely on the help of the outer
38:34 

38:34 table command to perform all these
38:36 

38:36 tasks.
38:38 

38:38 The general syntax of altering a table
38:40 

38:40 is simple. It's simply the keyword outer
38:44 

38:44 table. Then it's going to be followed by
38:46 

38:46 the table name. After that, you need to
38:49 

38:49 specify what changes you want. For
38:52 

38:52 example, if you want to add an integer
38:54 

38:54 column to the product table representing
38:57 

38:57 serial number, you can do this.
39:02 

39:02 alter table product add a serial number
39:05 

39:05 then integer.
39:07 

39:07 This command when being executed will
39:10 

39:10 lead to changes like this.
39:14 

39:14 For another example, if you want to
39:16 

39:16 change the data type of the column
39:18 

39:18 serial number from integer to text, you
39:21 

39:21 will write it as outer table product
39:24 

39:24 outer column serial number text.
39:28 

39:28 However, it's worth noting that
39:30 

39:30 different database systems may vary in
39:32 

39:32 terms of what's allowed and what's not
39:35 

39:35 on altering tables. For example, SQL
39:39 

39:39 doesn't allow changing column data
39:41 

39:41 types. You have to delete the old column
39:44 

39:44 and add a new one to achieve what you
39:46 

39:46 want. You will need to check the manual
39:48 

39:48 of your target database system on this.
39:53 

39:53 Although you have many ways to alter a
39:56 

39:56 table, there is only one way to drop a
39:58 

39:58 table. You will use the help from the
40:01 

40:01 drop table command.
40:03 

40:03 The general syntax is simply drop table
40:06 

40:06 keyword then followed by the table name.
40:09 

40:09 For example, if you want to drop the
40:11 

40:11 product table, the command will simply
40:14 

40:14 be drop table product.
40:17 

40:17 However, you need to be careful when
40:20 

40:20 using the drop table command because it
40:23 

40:23 will permanently delete the table and
40:25 

40:25 all its data. Now, you learned
40:28 

40:28 everything you need to know about table
40:30 

40:30 management. Let's move to data
40:32 

40:32 management. We will start with how to
40:35 

40:35 add data to a table.
40:38 

40:38 When the product table is created in the
40:41 

40:41 database, it's ready for you to add data
40:43 

40:43 to it. To add data to a table, you need
40:47 

40:47 the help from the inserting to command.
40:50 

40:50 The insert into command allows you to
40:52 

40:52 insert one or more rows of data into a
40:55 

40:55 table. Here's the basic syntax of adding
40:59 

40:59 data into a table. You will start with
41:01 

41:01 the insert into command. After that,
41:04 

41:04 it's going to be followed by the table
41:06 

41:06 name. Then in the immediate round
41:09 

41:09 bracket, you can list all the column
41:11 

41:11 names.
41:12 

41:12 Then value keyword is going to be
41:14 

41:14 followed by the values of different
41:17 

41:17 rows. Each row is encapsulated in a
41:19 

41:19 round bracket and the rows are separated
41:22 

41:22 using commas. The order of the values in
41:26 

41:26 each row need to correspond to the order
41:29 

41:29 of the column names that you specify
41:31 

41:31 here in the first bracket.
41:35 

41:35 For example, we can insert a single row
41:38 

41:38 into the product table like this. Notice
41:41 

41:41 that the order of the values correspond
41:44 

41:44 to the column names. If you change the
41:46 

41:46 column name order, you also need to
41:49 

41:49 change the order of the values. For
41:51 

41:51 example, we can change the order of the
41:54 

41:54 product ID and a description like this.
41:58 

41:58 Then the order of the values also need
42:01 

42:01 to be updated like this. If you have
42:04 

42:04 more than one rows to add to a table,
42:07 

42:07 you will simply list them one after
42:10 

42:10 another and separate them using commas
42:13 

42:13 like what you see here on the screen.
42:16 

42:16 This command adds two rows into the
42:19 

42:19 product table. Now you know how to add a
42:22 

42:22 data specifically rows of data into
42:25 

42:25 tables.
42:26 

42:26 Let's move to discuss how to delete data
42:29 

42:29 in a table.
42:31 

42:31 The general syntax of deleting data in a
42:34 

42:34 table is as follows. Delete from command
42:38 

42:38 is going to be followed by the table
42:40 

42:40 name. After that, you need to specify a
42:43 

42:43 filtering condition. Essentially, this
42:46 

42:46 command will delete the rows that meet
42:48 

42:48 the given filtering condition from the
42:50 

42:50 table that you specified.
42:53 

42:53 An example of the delete command is as
42:55 

42:55 follows. Delete from product where
42:58 

42:58 product ID is equal to two. This command
43:03 

43:03 will delete the row whose product ID is
43:06 

43:06 two from the product table. When
43:09 

43:09 deleting data from a table, you need to
43:11 

43:11 make sure to specify your filtering
43:14 

43:14 condition. Otherwise, the consequence
43:17 

43:17 can be serious. Without a filtering
43:19 

43:19 condition, the command delete from
43:22 

43:22 product will delete all the rows from
43:25 

43:25 the product table. In other words, all
43:28 

43:28 the data will be gone even if the
43:30 

43:30 structure of the table is incorrect.
43:34 

43:34 That's everything about the delete
43:36 

43:36 command.
43:37 

43:37 Let's move to discuss how to update data
43:40 

43:40 in a table. This task requires the help
43:43 

43:43 from the update set command.
43:46 

43:46 The syntax of such a command is as
43:48 

43:48 follows. It will start with the update
43:51 

43:51 keyword. Then it's going to be followed
43:54 

43:54 by the table name. Then set. The set
43:57 

43:57 keyword is followed by the details of
43:60 

43:60 how you want data to be updated in the
44:02 

44:02 corresponding column. In the end, you
44:05 

44:05 need to specify a filtering condition.
44:09 

44:09 After all, this command aims at updating
44:11 

44:11 data of specific rows.
44:14 

44:14 An example of an update command is as
44:17 

44:17 follows. Update product set price as 18
44:21 

44:21 and a name as not a toaster when product
44:25 

44:25 ID is equal to two. This command will
44:28 

44:28 update both the price column and the
44:31 

44:31 name column in the rows whose product ID
44:34 

44:34 value is equal to two. Same as a delete
44:37 

44:37 command. The filtering condition is very
44:40 

44:40 important here. Without a filtering
44:43 

44:43 condition, an update command will update
44:46 

44:46 the specified column in every rows. For
44:49 

44:49 example, the command that you see here
44:52 

44:52 on the screen will update the price to
44:54 

44:54 18 and a name to not a toaster in every
44:58 

44:58 row in the product table. That's highly
45:01 

45:01 unlikely to be a case that you want. So,
45:04 

45:04 the filtering condition is critical when
45:06 

45:06 it comes to deleting or updating data in
45:09 

45:09 a table. In this video, you'll learn the
45:12 

45:12 basics of table and data management,
45:14 

45:14 such as how to create, alter, or drop a
45:17 

45:17 table, insert, update, or delete data in
45:20 

45:20 a table when needed. This wasn't meant
45:23 

45:23 to be an exhaustive list of SQL
45:25 

45:25 commands, but it's an important step
45:28 

45:28 towards mastering database and a solid
45:30 

45:30 database design. If you find this video
45:33 

45:33 helpful, give it a thumb up, subscribe,
45:36 

45:36 and drop your questions in the comments.
45:38 

45:38 I would love to hear what's working for
45:40 

45:40 you and what needs to change. If you
45:43 

45:43 wanted to go deeper into database
45:44 

45:44 design, this video is based on the book
45:47 

45:47 graing relational database design which
45:49 

45:49 walks through these ideas step by step
45:52 

45:52 with real world examples. You'll find
45:55 

45:55 the link to the book in the first
45:57 

45:57 comment. A database is more than just a
45:60 

46:00 collection of tables. It's a val of
46:03 

46:03 relationships.
46:04 

46:04 If you have ever struggled with broken
46:07 

46:07 data, confusing drawings or unexpected
46:10 

46:10 delets, the root cause might be right
46:13 

46:13 there. Let's define what related tables
46:16 

46:16 are and how foreign keys hold the v of
46:19 

46:19 tables together. In this video, you'll
46:22 

46:22 learn the basics of related tables and
46:24 

46:24 how such tables and their data are
46:26 

46:26 managed. Let's get started by defining
46:29 

46:29 related tables first.
46:31 

46:31 So what makes two or more tables related
46:34 

46:34 to each other? Let's use an example to
46:37 

46:37 explain this concept. Think about an
46:40 

46:40 online store named the sci-fi collective
46:42 

46:42 that sells sci-fi products online. There
46:45 

46:45 are two tables in its database. One
46:48 

46:48 represents the products that are sold
46:51 

46:51 while the other represents the reviews
46:54 

46:54 associated with the products. Since each
46:57 

46:57 review is for a specific product, the
47:00 

47:00 review table needs the help from the
47:02 

47:02 product ID column to remember which
47:05 

47:05 review is for which product. As a
47:08 

47:08 result, the two tables share a common
47:11 

47:11 column, the product ID. If you need to
47:14 

47:14 see what the two tables look like with
47:16 

47:16 some data in them, here you go.
47:20 

47:20 As you can see, the value in the product
47:23 

47:23 ID column helps connect a row in the
47:26 

47:26 product table and a row in the review
47:28 

47:28 table. For example, in the product
47:31 

47:31 table, there's a row with a product ID
47:34 

47:34 value as three. It records a product
47:37 

47:37 named cat poop coffee. In the review
47:40 

47:40 table, the row with a review ID value of
47:43 

47:43 one records a positive review for this
47:46 

47:46 product. How would you know the review
47:48 

47:48 is for the cat poop coffee? The two rows
47:52 

47:52 here have the same product ID value.
47:54 

47:54 That's why. So the product ID column is
47:58 

47:58 shared across the two tables. Its goal
48:00 

48:00 is to help the review table remember
48:03 

48:03 which review is for which product and
48:06 

48:06 that makes the two tables related to
48:08 

48:08 each other. In the product table, the
48:11 

48:11 product ID column is a primary key. In
48:14 

48:14 the review table, the product ID column
48:17 

48:17 is known as a foreign key which refers
48:20 

48:20 to the primary key of another table. In
48:23 

48:23 this case, the other table is the
48:25 

48:25 product table. Now, let's define the
48:28 

48:28 related tables formally. The related
48:31 

48:31 tables are the tables that are connected
48:33 

48:33 by one or more common columns in a
48:35 

48:35 database. As curious as you are, you may
48:39 

48:39 wonder whether product and review tables
48:42 

48:42 can share a few more columns beyond the
48:44 

48:44 product ID. That's a great question
48:47 

48:47 about database design. You can even
48:49 

48:49 generalize this question a bit to all
48:52 

48:52 related tables like this. Should the two
48:56 

48:56 related tables share columns beyond the
48:58 

48:58 primary/foreign
48:60 

48:60 key? The answer is no.
49:03 

49:03 To simplify our discussion, let's look
49:05 

49:05 at the scenario in which two related
49:08 

49:08 tables use single columns as a primary
49:10 

49:10 keys, the product and the review table.
49:14 

49:14 Theoretically speaking, the numbers of
49:16 

49:16 columns shared by the two related tables
49:19 

49:19 can range from only the primary/foreign
49:22 

49:22 key to all columns from both tables.
49:26 

49:26 Using the product and review tables as
49:28 

49:28 an example, the figure that you see here
49:30 

49:30 on the screen summarizes the spectrum.
49:33 

49:33 So it ranges from just one column on the
49:36 

49:36 very left side to all the columns on the
49:39 

49:39 very right side. If two related tables
49:43 

49:43 share every column, I mean the same as
49:45 

49:45 what you can see here on the right side
49:47 

49:47 of the spectrum, it's obvious that one
49:50 

49:50 of them is redundant and that's
49:52 

49:52 unnecessary. If you choose to delete one
49:55 

49:55 of those two tables, you will still find
49:58 

49:58 a trouble in the remaining one. I mean
50:01 

50:01 this is the scenario where you use one
50:04 

50:04 table to represent the two entities and
50:06 

50:06 that will lead to insertion or delete
50:09 

50:09 anomalies and tons of redundancies. Even
50:12 

50:12 with just a few rows, as you can see
50:14 

50:14 here on the screen, you can still spot
50:16 

50:16 the redundancies.
50:18 

50:18 Okay, making two tables share every
50:21 

50:21 column is apparently a bad idea.
50:24 

50:24 How about making the two tables share
50:26 

50:26 only a few columns such as one or two
50:29 

50:29 more columns beyond the primary/foreign
50:32 

50:32 key? That's also a bad idea.
50:35 

50:35 First, you would still have redundancies
50:38 

50:38 even if it's less serious than a
50:41 

50:41 redundant table. Second, you will set a
50:45 

50:45 trap for yourself where you need to
50:47 

50:47 update data in such tables. If you
50:50 

50:50 update data in only one of the two
50:52 

50:52 related tables but forget the other, you
50:55 

50:55 will end up with inconsistent data.
50:58 

50:58 Suppose that you decide to make the
50:59 

50:59 product and review tables share one more
51:02 

51:02 columns beyond the product ID column,
51:05 

51:05 the manufacturer column.
51:07 

51:07 When you need to update the manufacturer
51:10 

51:10 of a product, you need to update both of
51:12 

51:12 the product and the reveal tables.
51:15 

51:15 Otherwise, you will end up with new
51:18 

51:18 manufacturer data in one table but
51:20 

51:20 outdated data in the other. Such a
51:23 

51:23 problem is known as an update anomaly.
51:26 

51:26 That said, given two related tables,
51:29 

51:29 they should share the primary/foreign
51:32 

51:32 key. If the primary/foreign key is a
51:35 

51:35 single column, then the two tables
51:37 

51:37 should share that column. If the
51:39 

51:39 primary/foreign key is composed of one
51:42 

51:42 more columns, then the two tables should
51:45 

51:45 share those columns. However, they
51:48 

51:48 shouldn't share any other columns beyond
51:51 

51:51 as the primary/foreign key.
51:54 

51:54 Now you know what is the related tables
51:56 

51:56 are. Let's move to discuss how to manage
51:59 

51:59 related tables.
52:02 

52:02 We will start with the foreign key
52:03 

52:03 constraint.
52:06 

52:06 So to properly introduce a foreign key
52:09 

52:09 constraint, we will use an example and
52:12 

52:12 start with a question.
52:14 

52:14 How do you create two related tables?
52:17 

52:17 For example, how do you create the
52:19 

52:19 product and review tables for the
52:21 

52:21 database of the sci-fi collective?
52:24 

52:24 Well, you can actually create the
52:26 

52:26 product table in the same way as if you
52:29 

52:29 are going to create a single table
52:31 

52:31 that's independent, that's not related
52:33 

52:33 to any other tables.
52:35 

52:35 Okay, you can rely on the help of the
52:38 

52:38 create a table command. Then that's
52:41 

52:41 going to be followed by the table name,
52:43 

52:43 then a bracket. In the bracket, you will
52:46 

52:46 wrap up all the columns line by line
52:49 

52:49 separated by commas. In each line, you
52:52 

52:52 will list the column name and the data
52:54 

52:54 types.
52:55 

52:55 You are also required to specify which
52:58 

52:58 column will be used as a primary key.
53:01 

53:01 But what about the reveal table? The
53:04 

53:04 review table can be created with the
53:07 

53:07 help of this command as you can see here
53:09 

53:09 on the screen. This is still a create a
53:12 

53:12 table command of course. However, there
53:15 

53:15 might be some clauses that you see for
53:17 

53:17 the first time. To explain this clearly,
53:20 

53:20 we need to discuss the general syntax of
53:23 

53:23 creating related tables first.
53:26 

53:26 So, what's the general syntax for
53:28 

53:28 creating two related tables or say two
53:31 

53:31 tables that have a relationship?
53:34 

53:34 As you know the shared column product ID
53:38 

53:38 is the primary key in the product table
53:40 

53:40 and is a foreign key in the review
53:42 

53:42 table. Given a pair of two related
53:44 

53:44 tables, we call a table like the product
53:47 

53:47 the parent table because it uses the
53:51 

53:51 shared column as a primary key. We call
53:54 

53:54 the table like the review the child
53:57 

53:57 table because it holds a foreign key. As
54:00 

54:00 you can see from the command that
54:02 

54:02 creates a product table, the syntax for
54:05 

54:05 creating a parent table is the same as
54:08 

54:08 the syntax for creating a single table
54:11 

54:11 that's not related to any other tables.
54:15 

54:15 To create a child table, you need to
54:18 

54:18 specify the foreign key that references
54:20 

54:20 the primary key in the parent table. You
54:23 

54:23 still need to define all the columns,
54:25 

54:25 data types, and as a primary key. Of
54:27 

54:27 course, the general syntax for creating
54:30 

54:30 a child table is presented here on the
54:33 

54:33 screen.
54:35 

54:35 Alternatively,
54:36 

54:36 you can create the two tables
54:38 

54:38 independently and add the foreign key
54:41 

54:41 constraint to the child table afterward
54:44 

54:44 like what you see here. In both options,
54:47 

54:47 you saw the constraint foreign key
54:49 

54:49 clause. What does it do exactly?
54:52 

54:52 In short, the clause creates a foreign
54:55 

54:55 key constraint which serves as a link
54:58 

54:58 between the two related tables. The
55:00 

55:00 constraint is twofold. On one hand, the
55:04 

55:04 constraint ensures that the foreign key
55:06 

55:06 column in the child table references
55:09 

55:09 only valid primary key values in the
55:11 

55:11 parent table. On the other hand, the
55:14 

55:14 constraint ensures that updating or
55:16 

55:16 deleting of rows in the parent table
55:19 

55:19 doesn't violate the consistency between
55:22 

55:22 the two related tables. We refer to
55:25 

55:25 these two aspects as referential
55:27 

55:27 integrity. So this long clause with
55:30 

55:30 three keywords constraint, foreign key,
55:33 

55:33 then references enforces referential
55:35 

55:35 integrity between two related tables.
55:38 

55:38 Referential integrity is an important
55:41 

55:41 aspect of data integrity. This clause is
55:44 

55:44 composed of two parts. The foreign key
55:47 

55:47 references statement creates a foreign
55:50 

55:50 key constraint that enforces the
55:53 

55:53 referential integrity.
55:55 

55:55 What about the constraint part? The
55:57 

55:57 constraint clause allows you to name
55:60 

56:00 this constraint.
56:02 

56:02 When you create a foreign key
56:04 

56:04 constraint, you don't necessarily need
56:06 

56:06 to name it, but naming it will make it
56:08 

56:08 easier to access whenever you need to
56:10 

56:10 modify such a constraint in the future.
56:13 

56:13 For example,
56:15 

56:15 in this example that you see here on the
56:17 

56:17 screen, we name the foreign key
56:19 

56:19 constraint in the review table as FK
56:23 

56:23 short for foreign key then product then
56:25 

56:25 review concatenated by underscore. As
56:29 

56:29 you can see foreign key then product
56:32 

56:32 table then review table. That's how the
56:34 

56:34 name is made.
56:36 

56:36 If you ever need to drop this
56:38 

56:38 constraint, you can access a constraint
56:41 

56:41 via this name like what you see here on
56:44 

56:44 the screen. If you don't name a
56:46 

56:46 constraint yourself, the database system
56:48 

56:48 that you use will name it automatically
56:50 

56:50 for you. The database system will use a
56:53 

56:53 default naming convention. Although the
56:56 

56:56 automatically picked the name can be
56:57 

56:57 retrieved, the name and the default
56:60 

56:60 naming convention vary from one system
57:02 

57:02 to the other. To avoid this hassle, we
57:05 

57:05 would recommend you always name the
57:07 

57:07 constraints. After all, dropping or
57:10 

57:10 disabling a constraint can be a common
57:13 

57:13 task whenever you need to modify your
57:15 

57:15 database design. Now you know how to
57:17 

57:17 create related tables and declare
57:19 

57:19 foreign key constraint. A natural
57:22 

57:22 question to ask next is whether the
57:24 

57:24 foreign key constraint have an impact on
57:27 

57:27 adding data into tables. Well, the short
57:30 

57:30 answer is yes. We mentioned this briefly
57:32 

57:32 when we talked about the referential
57:34 

57:34 integrity, but here we want to show it
57:37 

57:37 to you very explicitly using examples.
57:41 

57:41 First, adding data into the parent table
57:44 

57:44 is not impacted. It's the same as if you
57:46 

57:46 were to add data to a table that stands
57:49 

57:49 alone. For example, this command adds
57:52 

57:52 another row to the product table. As the
57:55 

57:55 product table is a parent table, there's
57:58 

57:58 no impact from the foreign key
57:60 

57:60 constraint at all. Second, adding data
58:03 

58:03 into child tables is impacted by the
58:06 

58:06 foreign key constraint. It's manifested
58:08 

58:08 in that every row added into the child
58:11 

58:11 table needs to have a product ID value
58:14 

58:14 that corresponds to an existing product
58:16 

58:16 ID value in the parent table. For
58:19 

58:19 example, if you ever try to add a row
58:22 

58:22 into the review table, but this row's
58:25 

58:25 product ID value doesn't correspond to
58:28 

58:28 anything in the product table like this.
58:31 

58:31 Whereas 30,000 doesn't exist as a
58:34 

58:34 product ID in the product table. You
58:37 

58:37 will receive complaints from SQL. If you
58:40 

58:40 think about it, such rows of data
58:43 

58:43 shouldn't exist in the database in the
58:45 

58:45 first place. If you are ever allowed to
58:48 

58:48 add such data into the review table, it
58:51 

58:51 only introduces troubles in the future
58:54 

58:54 such as inconsistency and is a trouble
58:57 

58:57 to figure out what non-existing products
58:59 

58:59 are referred to.
59:01 

59:01 That's how foreign key constraints
59:03 

59:03 protect the data integrity in your
59:05 

59:05 database. Deleting data from parent
59:08 

59:08 tables is also impacted. For example, if
59:12 

59:12 you try to delete the third row from the
59:14 

59:14 product table, I mean the row whose
59:17 

59:17 product ID value is three,
59:20 

59:20 then you are going to rely on the help
59:22 

59:22 from this query. Delete from product
59:25 

59:25 where product ID is equal to three. When
59:29 

59:29 you try to execute this query,
59:32 

59:32 you will get complaints from SQL because
59:35 

59:35 this product has been linked to multiple
59:38 

59:38 reviews in the review table. If you are
59:41 

59:41 allowed to take this action, you will
59:43 

59:43 end up with some orphan rows in the
59:45 

59:45 review table that references some
59:47 

59:47 product ID values that can never be
59:50 

59:50 found in the product table, which will
59:53 

59:53 introduce a lot of troubles in the
59:55 

59:55 future. That's how foreign key
59:56 

59:56 constraints protect the data integrity
59:59 

59:59 of your database. For almost the same
60:02 

60:02 reason, you can't update the primary key
60:04 

60:04 value that's referenced in a child
60:06 

60:06 table. When you try you will get errors
60:09 

60:09 from SQL.
60:12 

60:12 Beyond the data management for
60:15 

60:15 constraint also has an impact when it
60:18 

60:18 comes to dropping related tables or
60:20 

60:20 updating table structure given to
60:23 

60:23 related tables like product and review
60:26 

60:26 in the database for the online store
60:28 

60:28 sci-fi collective. If you try to drop
60:31 

60:31 the parent table or change the parent
60:33 

60:33 table structure such as renaming the
60:36 

60:36 primary key column or changing the
60:38 

60:38 primary key column from product ID to
60:41 

60:41 something else, you will certainly get
60:44 

60:44 complaints from SQL. If such changes can
60:47 

60:47 get us through easily without any
60:48 

60:48 constraints, they may lead to many
60:51 

60:51 issues in the future and harm the data
60:53 

60:53 integrity of your database. If you
60:56 

60:56 really want to perform such tasks, you
60:59 

60:59 will need to drop the foreign key
61:00 

61:00 constraint first. And in the eyes of the
61:03 

61:03 database system, that is an explicit
61:06 

61:06 agreement that you understand the
61:08 

61:08 consequence of doing such a thing. So
61:11 

61:11 then you will be allowed to perform such
61:13 

61:13 tasks.
61:15 

61:15 In this video, you'll learn about
61:17 

61:17 related tables and how to manage related
61:20 

61:20 tables and their data. You also picked
61:23 

61:23 up some new concepts such as foreign key
61:25 

61:25 constraints and data integrity. If you
61:28 

61:28 are finding this video helpful, give it
61:31 

61:31 a thumb up, subscribe, and drop your
61:33 

61:33 questions in the comments. I would love
61:36 

61:36 to hear what's working for you and what
61:38 

61:38 needs to change. If you want to go
61:40 

61:40 deeper into database design, this video
61:42 

61:42 is based on the book graing relational
61:44 

61:44 database design, which walks through
61:47 

61:47 these ideas step by step with real world
61:50 

61:50 examples. You'll find the link to the
61:53 

61:53 book in the first comment. If you are
61:55 

61:55 ever being confused by circle drawings,
61:58 

61:58 especially how drawings actually work
61:60 

61:60 and the difference between different
62:02 

62:02 types of drawings, you're not alone. In
62:05 

62:05 the next 15 minutes, we'll explain
62:07 

62:07 everything related to joins and break it
62:10 

62:10 all down with clear examples. In this
62:13 

62:13 video, you'll learn how to join data
62:15 

62:15 from related tables and dive deep into
62:18 

62:18 different types of joins and a
62:20 

62:20 comparison between where and join
62:22 

62:22 clauses. Let's get started with how to
62:26 

62:26 join data from related tables. First,
62:29 

62:29 from time to time, you may get a task
62:31 

62:31 that requires you to query data from
62:34 

62:34 related tables. For example, think about
62:38 

62:38 the online store named the sci-fi
62:39 

62:39 collective. There are two tables in its
62:43 

62:43 database. One represents the products
62:45 

62:45 that are being sold while the other
62:48 

62:48 represents the reviews of products.
62:51 

62:51 Since each review is for a specific
62:53 

62:53 product, the review table needs the help
62:56 

62:56 from product ID to remember which review
62:59 

62:59 is for which product. As a result, the
63:02 

63:02 two tables share a common column, the
63:05 

63:05 product ID. The product table is a
63:08 

63:08 parent table while the reveal table is a
63:10 

63:10 child table. In this relationship, if
63:13 

63:13 you need to see what the two tables look
63:16 

63:16 like with some data, here you go. If you
63:19 

63:19 ever want to understand how products are
63:21 

63:21 revealed, you will need to pull data
63:24 

63:24 from both tables. At least the name
63:27 

63:27 column from the product table and the
63:29 

63:29 review text column from the review
63:32 

63:32 table. Of course, as you do that, you
63:35 

63:35 need to map each review to its
63:37 

63:37 corresponding product. The query that
63:40 

63:40 answers your question looks like this.
63:43 

63:43 Select the name, review text from
63:46 

63:46 product, join review, and product. ID
63:51 

63:51 equals review dot product ID. This query
63:55 

63:55 takes advantage of the drawing on clause
63:58 

63:58 to join data from two tables. As you may
64:01 

64:01 have guessed, the drawing on clause here
64:04 

64:04 in this query maps each review to its
64:07 

64:07 corresponding product and makes sure
64:10 

64:10 that the result contains only the
64:12 

64:12 properly mapped products and their
64:15 

64:15 reviews. As you can see here in the
64:18 

64:18 result sample,
64:20 

64:20 although this query looks
64:21 

64:21 straightforward, we do need to explain a
64:24 

64:24 few things, answer a few questions so
64:27 

64:27 that you can have a thorough
64:28 

64:28 understanding of how drawing actually
64:30 

64:30 works. First, what's the general syntax
64:34 

64:34 for retrieving data from related tables?
64:36 

64:36 Second, what does the drawing on clause
64:39 

64:39 do? And a third what's the dot notation
64:43 

64:43 and how do we use it as in
64:45 

64:45 product.product
64:47 

64:47 id and review.p product ID. So the
64:50 

64:50 general syntax first the general syntax
64:53 

64:53 of joining data from related tables is
64:57 

64:57 presented here on the screen. We have
64:59 

64:59 some assumptions here which is that the
65:02 

65:02 two tables I mean table one and the
65:05 

65:05 table two are actually related. The two
65:08 

65:08 tables are listed here. From table one
65:11 

65:11 drawing table two. The condition of the
65:14 

65:14 drawing is listed here on table one dot
65:18 

65:18 column equals to table two dot column.
65:22 

65:22 This is a shared column between the two
65:25 

65:25 tables. In the select clause, it will
65:28 

65:28 list the target columns that you want to
65:31 

65:31 show up in the result. So that's a
65:33 

65:33 general syntax.
65:35 

65:35 Next, what does the drawing on clause do
65:38 

65:38 exactly?
65:39 

65:39 The answer to this question has two
65:41 

65:41 parts. First, the drawing clause
65:44 

65:44 computes the cartian product of the two
65:47 

65:47 tables. Second, the on clause filters
65:51 

65:51 the result based on the given condition.
65:54 

65:54 What's a cartition product? The
65:56 

65:56 cartition product between two tables is
65:59 

65:59 the set of all possible combination of
66:01 

66:01 rows from the two tables.
66:04 

66:04 Think about the product and review
66:06 

66:06 tables. To produce their cartition
66:09 

66:09 product, you will grab every row from
66:12 

66:12 the product table and then combine it
66:16 

66:16 with every row in the review table. For
66:18 

66:18 example, if the product and review
66:21 

66:21 tables each have only three rows, we
66:24 

66:24 will start from the first row in the
66:27 

66:27 product table and combine it with every
66:31 

66:31 row in the review table. It is a
66:33 

66:33 combined rows all going to the results.
66:36 

66:36 The same can be said for the second row
66:38 

66:38 in the product table. It will be
66:41 

66:41 combined with every row in the review
66:43 

66:43 table and they all go into the result.
66:46 

66:46 The third row in the product table is no
66:48 

66:48 exception. It will be combined with
66:51 

66:51 every row in the review table and they
66:53 

66:53 all go into the result. So in the end,
66:56 

66:56 the join clause by itself produces a
66:59 

66:59 cartition product of nine rows. Even if
67:03 

67:03 many of the combinations don't make much
67:05 

67:05 sense, we don't care here. All we care
67:08 

67:08 is to produce a result of all possible
67:10 

67:10 combinations of rows from the two
67:13 

67:13 tables. That's why the example cartition
67:16 

67:16 product has nine rows. That's three
67:19 

67:19 multiplied by three. Remember the
67:21 

67:21 product table has three rows and the
67:24 

67:24 reveal table has three rows. When you
67:26 

67:26 multiply three by three, you get an eye.
67:29 

67:29 Okay, that's the first part of the
67:31 

67:31 answer. So, what does the UNC clause do
67:34 

67:34 next? The UNC clause applies a filtering
67:37 

67:37 condition to the cartition product and
67:40 

67:40 only picks the rows in which product dot
67:43 

67:43 product ID is equal to review. ID. In
67:47 

67:47 short, the UNC clause picks the rows
67:51 

67:51 where mapping between a review and a
67:53 

67:53 product makes sense.
67:56 

67:56 In the end, the select clause only picks
67:59 

67:59 the two target columns names and review
68:02 

68:02 text and return them as a result. So on
68:06 

68:06 the screen, only the cells within the
68:09 

68:09 red highlighted rows and columns would
68:12 

68:12 be included in the result. Now you know
68:15 

68:15 what the drawing on clause does. Let's
68:19 

68:19 move to the next question. So what's a
68:21 

68:21 dot and how do we use the dot notation?
68:25 

68:25 As you see in the product.product ID and
68:28 

68:28 review.product ID, well the dot notation
68:32 

68:32 is a SQL syntax that's used to separate
68:36 

68:36 parts of a name. Product.product
68:39 

68:39 ID, for example, refers to the product
68:42 

68:42 ID column in the product table.
68:46 

68:46 For another example, product name can
68:50 

68:50 refer to the name column in the product
68:53 

68:53 table. Dot notation is especially handy
68:56 

68:56 when you query related tables because it
68:59 

68:59 helps you to be specific about the
69:02 

69:02 columns in case they have same name such
69:06 

69:06 as product.product ID and a
69:08 

69:08 review.product ID. This approach makes
69:12 

69:12 it clear which column and which table
69:14 

69:14 you are actually referring to so that it
69:18 

69:18 can prevent confusions. Beyond that, if
69:21 

69:21 two tables that you want to draw in have
69:23 

69:23 multiple columns that share a name, you
69:26 

69:26 may want to rename them in the select
69:28 

69:28 class to prevent confusions.
69:31 

69:31 For example, you have two related tables
69:34 

69:34 employee and a department here on the
69:36 

69:36 screen and you want to join them to get
69:39 

69:39 the names of the departments to which
69:42 

69:42 employees belong. However, both tables
69:46 

69:46 have a column named the name. So you
69:49 

69:49 need to use the dot notation to specify
69:51 

69:51 which name column to select. So the
69:54 

69:54 query that gets the dropdown properly
69:56 

69:56 will look like this. Select employee
69:60 

69:60 name department.name
70:03 

70:03 from employee join department on
70:06 

70:06 employee dot department id equal to
70:10 

70:10 department ID. The ID in the department
70:14 

70:14 table is a primary key and its
70:16 

70:16 corresponding foreign key in the
70:18 

70:18 employee table is named department ID.
70:21 

70:21 It's worth mentioning that the result
70:24 

70:24 may show two column names that look
70:26 

70:26 exactly the same like this. To prevent
70:30 

70:30 this confusion, you can rename the
70:32 

70:32 columns with an alias via the as clause
70:36 

70:36 like this. All it takes is to add an as
70:40 

70:40 clause that follows the actual column
70:43 

70:43 names so that you can specify how the
70:46 

70:46 column names should show up in the
70:48 

70:48 result. Okay, that's everything about
70:51 

70:51 how to join data from related tables.
70:54 

70:54 Let's move to discuss the different
70:56 

70:56 types of drawings. So this is the
70:58 

70:58 general syntax of how to use the drawing
71:01 

71:01 on clause to join data from two related
71:04 

71:04 tables. There are actually more than one
71:07 

71:07 type of drawings and the drawing keyword
71:10 

71:10 can be replaced by the following options
71:13 

71:13 including inner drawing, left drawing, a
71:16 

71:16 right drawing and a full outer drawing.
71:18 

71:18 Different types of drawings will give
71:21 

71:21 you different results. The inner drawing
71:23 

71:23 is the same as drawing and returns only
71:26 

71:26 the rows that have matching values in
71:29 

71:29 both tables.
71:31 

71:31 In contrast, left drawing returns all
71:34 

71:34 the rows from table one and the matching
71:37 

71:37 rows from table two. If a row in table
71:40 

71:40 one has no matching table two, the
71:43 

71:43 result row will include the table one
71:45 

71:45 record with no values for table two
71:48 

71:48 columns. For example, the left drawing
71:52 

71:52 between the product and the review table
71:54 

71:54 will lead to a result looking like this.
71:57 

71:57 Every match between the product and
71:60 

71:60 review tables will be included. Beyond
72:03 

72:03 that, the products that have never been
72:06 

72:06 reviewed will also be included. For
72:09 

72:09 example, the atomic nose hair trimmer
72:12 

72:12 was never revealed. It will still be
72:14 

72:14 included in the result, but its review
72:17 

72:17 text value will be null because it
72:20 

72:20 doesn't have a match from the review
72:22 

72:22 table and there's nothing to fill up
72:25 

72:25 that cell. Right? drawing returns all
72:27 

72:27 the rows from the table two and the
72:30 

72:30 matching rows from table one. If a row
72:32 

72:32 in table two has no matching table one,
72:35 

72:35 the result row will include the table
72:38 

72:38 two record with no values for table one
72:41 

72:41 columns. Full alter drawing returns all
72:44 

72:44 the rows from both tables including the
72:47 

72:47 non-matching rows. If a row in table one
72:50 

72:50 has a matching row in table two, the
72:53 

72:53 corresponding result row will simply
72:56 

72:56 include the data from both tables. If a
72:59 

72:59 row in table one has no matching table
73:01 

73:01 two, the result row will include the
73:04 

73:04 table one record with no values for
73:07 

73:07 table two column. Similarly, if a record
73:10 

73:10 in table two has no match in table one,
73:13 

73:13 the result row will simply be the table
73:16 

73:16 two record with no values for table one
73:18 

73:18 column. The relationships on the left
73:21 

73:21 table, the right table and the return
73:24 

73:24 results are summarized here on the
73:26 

73:26 screen. It's worth noting that the left
73:28 

73:28 drawing, right drawing and for drawing
73:31 

73:31 may lead to query results with no
73:34 

73:34 values. One side effect of getting no
73:37 

73:37 values in the result is that you need to
73:39 

73:39 handle them very carefully. No values
73:42 

73:42 can cause errors if you try to perform
73:44 

73:44 calculations or comparisons. Okay,
73:48 

73:48 that's everything about the different
73:49 

73:49 types of drawings. Let's move to talk
73:52 

73:52 about the comparison between where and
73:54 

73:54 the drawing clause. As curious as you
73:56 

73:56 are, you may be tempted to try to join
73:59 

73:59 in two tables using the wear clause. Can
74:03 

74:03 you do that? Yes, you can. You can
74:05 

74:05 certainly use a wear clause to join two
74:07 

74:07 tables as long as they are related. The
74:10 

74:10 two examples that use join clause and
74:14 

74:14 the wear clause to join tables are
74:15 

74:15 listed side by side on the screen. To
74:18 

74:18 use the wear clause to join data from
74:20 

74:20 related tables, you need to list the two
74:23 

74:23 tables one after the other in the from
74:26 

74:26 clause and separate them using commas.
74:29 

74:29 And the wear condition is used to do the
74:32 

74:32 filtering. Both of the two queries work
74:35 

74:35 and achieve what you want. However,
74:39 

74:39 whenever you need to query related
74:41 

74:41 tables, the drawing clause is generally
74:44 

74:44 preferred.
74:46 

74:46 There is a strong evidence that people
74:48 

74:48 make less mistakes using explicit
74:50 

74:50 drawing than the wear clause. So, it's
74:53 

74:53 less prone to errors and more
74:56 

74:56 straightforward to modify or debug.
74:58 

74:58 Beyond that, the join clause is
75:01 

75:01 typically much more optimized by
75:03 

75:03 database system engines than the wear
75:06 

75:06 clause. So using join clause does give
75:10 

75:10 your database a better performance than
75:12 

75:12 using a bunch of wear clause to join
75:14 

75:14 tables.
75:15 

75:15 In case if you wanted to run the queries
75:17 

75:17 that were covered in this video and
75:19 

75:19 replicate what you saw, you can find
75:22 

75:22 everything you need from this GitHub
75:24 

75:24 repository. The URL of this repository
75:27 

75:27 can be found in the description of this
75:29 

75:29 video.
75:31 

75:31 The easiest way to replicate what you
75:34 

75:34 saw from this video is to load data into
75:37 

75:37 a tool named SQL online.
75:40 

75:40 You can follow the readme files in each
75:42 

75:42 of the corresponding folders for example
75:45 

75:45 chapter 1 to see the detailed
75:47 

75:47 instruction. Of course, you can follow
75:50 

75:50 the instruction to install your
75:52 

75:52 preferred database system locally and
75:55 

75:55 then load the scripts, but we are going
75:57 

75:57 to simply use a tool that makes things
76:00 

76:00 easier for us to start. The tool is
76:03 

76:03 named SQL online. Essentially, that's
76:06 

76:06 SQL database system running on the
76:09 

76:09 cloud.
76:10 

76:10 Once you follow the instruction in the
76:12 

76:12 readme file to load the corresponding
76:15 

76:15 script from what we prepared for you,
76:19 

76:19 you will be able to run all the queries
76:21 

76:21 that were covered in this video and see
76:23 

76:23 the same results.
76:25 

76:25 In this video, you took a deep dive into
76:28 

76:28 the join clause, learn about how drawing
76:30 

76:30 actually works, the different types of
76:32 

76:32 drawings, and the comparison between the
76:35 

76:35 wear and the join clause. If you find
76:37 

76:37 this video helpful, give it a thumb up,
76:40 

76:40 subscribe, and drop your questions in
76:43 

76:43 the comments. I would love to hear
76:45 

76:45 what's working for you and what needs to
76:47 

76:47 change. If you want to go deeper into
76:49 

76:49 database design, this video is based on
76:52 

76:52 the book Groing Relational Database
76:54 

76:54 Design, which walks through these ideas
76:56 

76:56 step by step with real world examples.
76:60 

76:60 You'll find the link to the book in the
77:02 

77:02 first comment. Many people still try to
77:04 

77:04 learn SQL the old way. watching endless
77:07 

77:07 tutorials or trying to memorize syntax
77:10 

77:10 like it's 2010. But in 2025, that
77:13 

77:13 strategy will cost you time, money, and
77:16 

77:16 momentum. Today, tools like chat GTP can
77:19 

77:19 write most the SQL queries for you. So,
77:22 

77:22 your job isn't to memorize syntax.
77:25 

77:25 Your job is to know what to ask, why it
77:28 

77:28 works, and how to structure your data to
77:30 

77:30 get the results you need or gain
77:33 

77:33 insights that shape your database
77:34 

77:34 design. In the next 10 minutes, I'll
77:37 

77:37 show you a smarter and a faster way to
77:39 

77:39 learn SQL. One that leverages AI while
77:43 

77:43 helping you truly understand what's
77:44 

77:44 happening under the hood. First, what
77:48 

77:48 not to do. A lot of people begin their
77:51 

77:51 SQL journey with this pattern. Learn
77:53 

77:53 select, then wear, then drawing, then
77:56 

77:56 just keep practicing until it sticks.
77:59 

77:59 You'll watch tutorial after tutorial
78:01 

78:01 that walks through query after query
78:04 

78:04 showing the same surface level pattern
78:06 

78:06 but never really explaining what's going
78:08 

78:08 on underneath. They might teach you how
78:11 

78:11 to join two tables but not how the join
78:14 

78:14 works and what an cartian product is.
78:18 

78:18 They'll show you the syntax for where
78:20 

78:20 but skips over how the database engine
78:22 

78:22 actually processes that filter. This is
78:25 

78:25 like learning to recite a song in a
78:27 

78:27 foreign language without knowing the
78:29 

78:29 meanings of any words. Here's the
78:31 

78:31 problem with this approach. You don't
78:33 

78:33 build real understanding and you may get
78:36 

78:36 stuck the moment your task becomes even
78:38 

78:38 a little bit complex. And this approach
78:41 

78:41 of learning will set you up for directly
78:44 

78:44 competing with AI and you know that you
78:46 

78:46 will lose even now let alone in a few
78:49 

78:49 years.
78:50 

78:50 So what to do instead?
78:53 

78:53 We wanted to highlight at least two
78:55 

78:55 things that you should consider doing.
78:57 

78:57 First, you wanted to go beyond a simple
78:60 

78:60 SQL syntax to learn how to design smart
79:02 

79:02 questions that can be translated to SQL
79:05 

79:05 queries by generative AI.
79:08 

79:08 Different from most programming
79:09 

79:09 languages like Java or Python, SEL is
79:12 

79:12 declarative.
79:14 

79:14 By saying that, I mean SQL is more
79:16 

79:16 aligned with natural languages. When you
79:19 

79:19 deal with SEO, you only need to describe
79:22 

79:22 what results you want, not how to
79:24 

79:24 compute it step by step. This makes it
79:27 

79:27 exceptionally well suited for AI tools
79:30 

79:30 like chat GTP or copilot for various
79:32 

79:32 reasons such as that SQL has a limited
79:35 

79:35 and consistent structure and there's no
79:39 

79:39 control flow or state tracking SQL. AI
79:42 

79:42 can generate correct or even optimized
79:44 

79:44 queries with just a plain English
79:47 

79:47 prompt. Now, so in most cases, you don't
79:50 

79:50 need to write the syntax by hand. You
79:53 

79:53 can delegate that part. The more
79:55 

79:55 important skill is shifting your role.
79:58 

79:58 The new challenge isn't how do I write
79:60 

79:60 this query. It is instead what exactly
80:03 

80:03 am I trying to find out from the data.
80:06 

80:06 That sounds simple, but it's actually a
80:08 

80:08 higher order thinking skill. AI can't
80:11 

80:11 offer much help if you don't know what
80:13 

80:13 you are looking for. Learning SEL today
80:16 

80:16 means learning how to translate your
80:18 

80:18 real world questions into something a
80:20 

80:20 database can understand. To do that
80:23 

80:23 well, you need to know the language of
80:25 

80:25 data. You need to understand what a
80:27 

80:27 table is, how relationships between
80:29 

80:29 entities work, how joins work, and how
80:33 

80:33 your data is structured. Sometimes you
80:35 

80:35 even need some domain knowledge like
80:38 

80:38 what active users mean in a marketing
80:40 

80:40 context or what a transaction is in a
80:43 

80:43 financial data set. So yes, you are
80:45 

80:45 offloading the mechanics but you're
80:48 

80:48 upgrading the strategy. And here's the
80:50 

80:50 best part. Once you get good at
80:53 

80:53 designing smart questions in SQL, you'll
80:56 

80:56 get better at asking the right business
80:58 

80:58 questions, designing dashboard,
81:00 

81:00 exploring data, or even writing better
81:03 

81:03 prompts for other AI tools. You are not
81:05 

81:05 just a learning SQL. You are learning to
81:07 

81:07 think clearly about data and that's a
81:10 

81:10 competitive edge across different
81:12 

81:12 fields. Second, you should focus on why
81:15 

81:15 and how behind and beyond SQL queries.
81:18 

81:18 One of the most underrated ways to learn
81:20 

81:20 SEL, especially today, is to learn it in
81:23 

81:23 context. Think of learning SEL like
81:26 

81:26 learning a new language. Sure, you could
81:28 

81:28 memorize vocabulary lists and a grammar
81:31 

81:31 rules, but you will learn faster and
81:34 

81:34 retain more if you're actually using it
81:36 

81:36 in an authentic activity like reading or
81:39 

81:39 conversation. The same applies to SQL.
81:42 

81:42 Instead of keeping practicing isolated
81:44 

81:44 queries, you need to connect SQL to real
81:48 

81:48 tasks, tasks that matter to you. That's
81:51 

81:51 how you start uncovering the why and how
81:54 

81:54 behind each line of SQL that you write.
81:57 

81:57 If you are aiming to become a software
81:58 

81:58 engineer, a great entry point is to
82:01 

82:01 learn database design. Why? Because when
82:05 

82:05 you learn database design, you are
82:07 

82:07 forced to think about what data types to
82:09 

82:09 use, how tables relate to each other and
82:12 

82:12 how to structure the data for future
82:14 

82:14 queries and applications. Take a simple
82:17 

82:17 example. Suppose you decide between
82:20 

82:20 using date time and a time stamp for a
82:22 

82:22 column. At the first glance, they may
82:25 

82:25 seem interchangeable, but datetime just
82:29 

82:29 stores a date and a time as it is.
82:32 

82:32 Whereas time stamp is stored in
82:34 

82:34 coordinated universal time, short for
82:36 

82:36 UTC. A time stored in this format can be
82:39 

82:39 automatically adjusted for time zones
82:41 

82:41 when the data is read. If you're
82:44 

82:44 building a global app, this seemingly
82:46 

82:46 small choice can lead to big problems or
82:49 

82:49 save you from one. These kind of
82:51 

82:51 details, the ones that you don't see in
82:54 

82:54 most tutorials, are exactly what you
82:57 

82:57 better add SQL in the long run. If you
82:60 

83:00 are on the data analytics path, your
83:03 

83:03 best teacher is real data, an actual
83:05 

83:05 realistic and possibly messy data set
83:08 

83:08 from an actual relational database. Try
83:11 

83:11 using the data to build a dashboard,
83:13 

83:13 generating reports, or answering real
83:16 

83:16 business questions.
83:18 

83:18 Here's what I mean. Instead of learning
83:20 

83:20 group by in isolation, try calculating
83:23 

83:23 weekly retention from actual event data.
83:27 

83:27 Instead of just filtering rows, try
83:29 

83:29 preparing selected product data for a
83:32 

83:32 sales dashboard. Instead of writing a
83:34 

83:34 drawing just because your course told
83:37 

83:37 you so, try explaining user conversion
83:39 

83:39 rates across different signup flows.
83:42 

83:42 This way, you're not just asking how do
83:45 

83:45 I write this query. You're asking what
83:48 

83:48 does the metric mean? What shape does my
83:50 

83:50 data need to be in? Or how can I make
83:53 

83:53 this accurate and useful? The bottom
83:56 

83:56 line is this. You will learn SQL more
83:58 

83:58 deeply. If you stop treating it like the
84:01 

84:01 end goal, use it to design and implement
84:04 

84:04 databases. Use it to answer better
84:06 

84:06 questions. That's how you will get good
84:09 

84:09 and that's how you will stay ahead.
84:11 

84:11 Especially now AI can write nearly all
84:13 

84:13 the queries for you. Now you know what
84:16 

84:16 not to do and what to do to speed up
84:18 

84:18 your mastery of SQL. Let me introduce a
84:21 

84:21 learning road map that encapsulates
84:23 

84:23 these ideas.
84:25 

84:25 A smart learning road map still starts
84:28 

84:28 with mastery of the fundamentals. By
84:30 

84:30 saying that I mean you still need to
84:32 

84:32 learn how to write SQL like select from
84:35 

84:35 and aware clauses. But don't stop there
84:38 

84:38 and don't draw yourself with similar SQL
84:41 

84:41 practices. Instead what really separates
84:44 

84:44 effective learners from roles who get
84:47 

84:47 stuck is thus the effective learners
84:50 

84:50 understand the relational model that SQL
84:52 

84:52 is built on. For example, understanding
84:54 

84:54 what a table really is, not just a
84:57 

84:57 spreadsheet of data, but a set of tuples
84:59 

84:59 that follows a schema changes how you
85:02 

85:02 think about joins, keys, and
85:04 

85:04 constraints. For another example, the
85:07 

85:07 understanding of foreign keys shouldn't
85:09 

85:09 just be limited to their technical
85:11 

85:11 details. Foreign keys define
85:13 

85:13 relationships between entities and shape
85:15 

85:15 how you design your queries and your
85:17 

85:17 database. A lot of people know the
85:19 

85:19 syntax of joining clauses, but they kind
85:22 

85:22 of tell you how the difference in
85:23 

85:23 cardalities impact how you write the
85:25 

85:25 drawing queries. Another key idea is set
85:28 

85:28 based thinking. SQL operates on sets,
85:31 

85:31 not rows. That's why functions like
85:34 

85:34 count, group by, and distinct behave the
85:37 

85:37 way they do and why you can't always
85:39 

85:39 think procedurally. These concepts may
85:42 

85:42 sound academic, but they are exactly
85:44 

85:44 what gives you the confidence when
85:46 

85:46 reading, modifying or validating AI
85:49 

85:49 generated queries. So yes, learn your
85:51 

85:51 syntax, but if you want to learn SQL
85:54 

85:54 faster and retain it longer, focus on
85:56 

85:56 the core ideas behind it. Understanding
85:59 

85:59 the relational model is more important
86:01 

86:01 than memorizing syntax. And that
86:03 

86:03 foundation will pay off again and again
86:06 

86:06 as your projects grow more complex. If
86:09 

86:09 you want to get really good at a SQL,
86:11 

86:11 you need to go beyond just the writing
86:13 

86:13 queries. And what is that means depends
86:15 

86:15 on what you are learning SQL for. If you
86:17 

86:17 are on the path of software engineering,
86:20 

86:20 learn SQL in isolation is like learning
86:22 

86:22 vocabulary without knowing how sentences
86:24 

86:24 are structured. You need to understand
86:26 

86:26 how databases are designed because
86:29 

86:29 design decisions directly impact how
86:31 

86:31 queries work. For example, after
86:33 

86:33 learning various aspects of database
86:35 

86:35 security, you will know that password
86:38 

86:38 encryption only demands increasing the
86:40 

86:40 var size of the password column, but not
86:43 

86:43 necessarily the data type. Because one
86:45 

86:45 way hashed passwords require a bigger
86:48 

86:48 data storage, but can comfortably stay
86:50 

86:50 as characters. Such a change is
86:53 

86:53 critical, yet may seem so trivial and
86:56 

86:56 ridiculous in the eyes of roles who
86:58 

86:58 limit their learning to just the SQL
86:60 

86:60 itself. So if you're aiming to become a
87:02 

87:02 software engineer, go deeper into
87:04 

87:04 database design. This is where theories
87:07 

87:07 matter. Not in a purely academic way,
87:10 

87:10 but in a way that changes how you build
87:12 

87:12 software systems. If you are serious
87:15 

87:15 about this path, I have some resources
87:18 

87:18 to recommend. The book of database
87:21 

87:21 system concepts, a time-t tested
87:23 

87:23 academic classic which is in its seventh
87:26 

87:26 edition now. If you want an easier entry
87:29 

87:29 into this topic, cracking relational
87:31 

87:31 database design could be a choice. It's
87:33 

87:33 a modern example driving approach that
87:36 

87:36 works for self-arners and developers. As
87:39 

87:39 a disclaimer, I'm the lead author of
87:41 

87:41 this book. However, if you're learning
87:44 

87:44 SQL for data analytics, your learning
87:46 

87:46 journey should focus on how SQL fits
87:48 

87:48 into the larger data analytics pipeline.
87:51 

87:51 SQL is a tool that you use to ask
87:53 

87:53 questions from real data. But what makes
87:56 

87:56 those questions meaningful is the
87:58 

87:58 context of your analysis task.
88:01 

88:01 For example, instead of learning group
88:04 

88:04 by in the abstract, it will be more
88:07 

88:07 productive if you try to analyze user
88:09 

88:09 behavior from a website log or track
88:12 

88:12 sales performance over time or segment
88:15 

88:15 customers by purchase pattern. These
88:18 

88:18 aren't just exercise. They teach you why
88:21 

88:21 you need certain drawings or filters and
88:23 

88:23 how to structure queries for insights,
88:26 

88:26 not just for correctness. Taking on
88:28 

88:28 projects like building a dashboard or
88:30 

88:30 generating a weekly report for
88:32 

88:32 stakeholders gives you a reason to apply
88:35 

88:35 window function nested queries all in a
88:37 

88:37 real world context. In addition to that,
88:40 

88:40 you will be naturally exposed to related
88:43 

88:43 tools like Excel, PowerBI or Lucer
88:46 

88:46 Studio or even Python libraries like
88:49 

88:49 Pandas or SQL Alchemy or data cleaning
88:52 

88:52 and transformation steps that complement
88:55 

88:55 SQL. A great way to accelerate this kind
88:57 

88:57 of learning is to pick up data sets that
88:60 

88:60 you care about public data set, sport
89:03 

89:03 data set, financial data set or anything
89:06 

89:06 and challenge yourself to tell a story
89:08 

89:08 with it using SQL. So whether you are
89:11 

89:11 building an analysis pipeline or telling
89:13 

89:13 a datadriven story going beyond curies
89:16 

89:16 means understanding how SQL fits into
89:19 

89:19 the bigger picture of the problem you
89:21 

89:21 are solving. Once you have built the
89:23 

89:23 foundations and gone beyond just the
89:25 

89:25 curies, the best way to level up is by
89:28 

89:28 working on real projects. Projects that
89:31 

89:31 gave you a context, purpose and clear
89:34 

89:34 goals. They connect everything you have
89:36 

89:36 learned. If you are learning for
89:38 

89:38 software engineering, start building
89:40 

89:40 databases from scratch. Such activities
89:43 

89:43 will force you to think about data
89:44 

89:44 integrity, normalization, and how your
89:47 

89:47 schema decisions affect how SQL behaves.
89:50 

89:50 As I record this video, I realized that
89:53 

89:53 authentic problems on database designs
89:56 

89:56 are scattered and their qualities are
89:58 

89:58 uneven. So, I'm currently working on a
90:01 

90:01 public GitHub repo to aggregate quality
90:04 

90:04 problem sets on database design and
90:06 

90:06 implementation. The URL of this repo is
90:09 

90:09 here. You can find the link to this repo
90:12 

90:12 in the description of this video as
90:14 

90:14 well. Remember that you don't need a
90:17 

90:17 perfect idea or a big plan. The key is
90:20 

90:20 to start doing something small, messy
90:23 

90:23 and real. That's where the real learning
90:26 

90:26 happens. That's how you go from a
90:28 

90:28 student to an expert.
90:31 

90:31 If you are learning for data analytics,
90:33 

90:33 then your focus should be on curing,
90:36 

90:36 analyzing, and communicating insights.
90:38 

90:38 Pick a public data set from Kaggle,
90:41 

90:41 Google data set search or data.gov.
90:44 

90:44 Import it into a database system. Start
90:47 

90:47 with simple questions like what are the
90:49 

90:49 top five cities by population growth?
90:52 

90:52 Which product category has the highest
90:54 

90:54 return rate? Or how has revenue changed
90:58 

90:58 month over months? Then go deeper group
91:00 

91:00 by filter drawing with another table and
91:04 

91:04 try visualizing your results. Try to
91:06 

91:06 answer a real question as if you were
91:08 

91:08 building a report for a stakeholder. And
91:11 

91:11 again write up your analysis. Push your
91:14 

91:14 queries and charts to GitHub. Share your
91:16 

91:16 data analysis on Kaggle. This gives your
91:19 

91:19 learning a purpose and it builds a
91:22 

91:22 public track record of your growth. In
91:24 

91:24 this video, we explored how to learn SQL
91:27 

91:27 deeper and better. Such a discussion
91:29 

91:29 becomes unavoidable when generative AI
91:32 

91:32 tools can write accurate and even
91:34 

91:34 optimized the SQL queries given a proper
91:37 

91:37 prompt. If you find this video helpful,
91:40 

91:40 give it a thumb up, subscribe, and drop
91:43 

91:43 your questions in the comments. I would
91:45 

91:45 love to hear from what's working for you
91:47 

91:47 and what needs to change. Ever get
91:50 

91:50 charged the twice for the same thing or
91:52 

91:52 see your Uber driver's name change
91:54 

91:54 midright? It's not a bug, it's a bad
91:56 

91:56 database design. Behind every confusing,
91:59 

91:59 slow, or broken app, there's probably a
92:02 

92:02 database screaming for help. In this
92:05 

92:05 video, I'll show you the five goals
92:07 

92:07 every great database must achieve and
92:10 

92:10 why ignoring even one of them can cost
92:12 

92:12 your user money or worse. Let's start
92:15 

92:15 with data consistency and integrity.
92:19 

92:19 Data consistency means information in
92:21 

92:21 your database doesn't conflict with
92:23 

92:23 itself. Data integrity means information
92:26 

92:26 is accurate, complete, and follows the
92:29 

92:29 rules you have set. In combination,
92:31 

92:31 these two terms are about defining
92:33 

92:33 appropriate data types, constraints and
92:36 

92:36 relationships on all entities to ensure
92:38 

92:38 that data remains consistent across
92:40 

92:40 different tables, data redundancy is
92:43 

92:43 minimized and anomalies are prevented.
92:46 

92:46 For example, given two related entities,
92:49 

92:49 it's better that they are represented
92:51 

92:51 using different tables and they don't
92:54 

92:54 share more columns than the
92:56 

92:56 primary/foreign key. Otherwise, you will
92:59 

92:59 have redundancy issues as well as insert
93:02 

93:02 or update anomalies. Such issues can
93:05 

93:05 have real impact on real world. Think
93:08 

93:08 about when you order a laptop online.
93:10 

93:10 The confirmation page shows that the
93:12 

93:12 price is $1 short of $1,000, but your
93:15 

93:15 email receipt says it's actually $1,199.
93:21 

93:21 Issues with data consistency and
93:22 

93:22 integrity can instantly break trust and
93:25 

93:25 causes confusion. even in something as
93:28 

93:28 routine as online shopping.
93:31 

93:31 The second goal is maintainability and
93:33 

93:33 ease of use. Maintainability means your
93:37 

93:37 database is easy to update, fix or
93:39 

93:39 expand later without breaking
93:41 

93:41 everything. Ease of use means it's
93:44 

93:44 simple for developers and analysts to
93:46 

93:46 understand, write queries, and get the
93:49 

93:49 data they need from your database. In
93:52 

93:52 short, a real designed database should
93:54 

93:54 be intuitive to use and easy to maintain
93:57 

93:57 by the people who use it, including
93:59 

93:59 database administrators, analysts, and
94:02 

94:02 the developers of the web or mobile
94:04 

94:04 applications that are powered by the
94:06 

94:06 database. A lot of measures can be taken
94:09 

94:09 to increase the maintainability of a
94:11 

94:11 database. For example, following a
94:13 

94:13 constant naming convention is a smart
94:16 

94:16 thing to do when you design a database,
94:18 

94:18 but it can save a lot of time for people
94:20 

94:20 to use it or maintain the database in
94:22 

94:22 the future. Imagine that you are a data
94:25 

94:25 analyst or database admin and you have
94:28 

94:28 to maintain a database with this issue
94:31 

94:31 where ids are used inconsistently in the
94:34 

94:34 database. For example, in the product
94:37 

94:37 table, it's product
94:39 

94:39 ID. In the review table, you have a
94:42 

94:42 snake case that's review ID where I is
94:46 

94:46 capitalized. If developers who use a
94:49 

94:49 database have to spend time figuring out
94:53 

94:53 whether and how ID or identifiers are
94:56 

94:56 used as a primary key column, the
94:58 

94:58 database is hardly intuitive to use, let
95:01 

95:01 alone easy to maintain. The third goal
95:04 

95:04 is about performance and optimization.
95:07 

95:07 Performance means your database responds
95:09 

95:09 quickly even when handling a lot of
95:12 

95:12 data. Optimization means it's designed
95:14 

95:14 in a smart way so that it doesn't waste
95:17 

95:17 the time or resources when running
95:19 

95:19 queries. In short, a well optimized
95:22 

95:22 database should give you the right data
95:24 

95:24 fast whether it's one user or a million.
95:28 

95:28 You can take different measures to
95:29 

95:29 improve the performance and optimize
95:32 

95:32 your database. For example, indexing is
95:35 

95:35 a common technique to optimize the
95:37 

95:37 database performance. Think about the
95:39 

95:39 review table in a database supporting
95:41 

95:41 online store. The data in the review
95:44 

95:44 tablet often needs to be sorted by date
95:47 

95:47 because potential customers often want
95:50 

95:50 to see the latest review of a product
95:52 

95:52 that they are browsing. To speed this
95:55 

95:55 sorting process, you can index the date
95:58 

95:58 column in the review table. So, what's
96:00 

96:00 indexing?
96:02 

96:02 Think of the data in the review table as
96:04 

96:04 a library of books. Indexing is the
96:07 

96:07 process of creating an index card
96:09 

96:09 catalog that lists every book
96:11 

96:11 alphabetically along with its location.
96:14 

96:14 When you need to sort, you can use the
96:16 

96:16 index card catalog to locate every book
96:19 

96:19 and put it in the sorted position. This
96:22 

96:22 procedure will speed up the sorting task
96:25 

96:25 whenever it's needed. The next goal is
96:28 

96:28 about data security. Data security means
96:31 

96:31 protecting your data from unauthorized
96:33 

96:33 access leaks or temporary. Only the
96:36 

96:36 right people should see or change the
96:38 

96:38 data and no one else. It's about keeping
96:42 

96:42 sensitive information safe whether it's
96:44 

96:44 a user password, medical record or
96:46 

96:46 payment detail. You will need to take
96:49 

96:49 different measures to secure your
96:50 

96:50 database. For example, if you ever need
96:54 

96:54 to store payment method information in
96:56 

96:56 your database, you should store only
96:58 

96:58 encrypted information. Storing customers
97:01 

97:01 payment method information in plain text
97:03 

97:03 is a bad idea. If an evil hacker gains
97:06 

97:06 access to the database, they will know
97:09 

97:09 everybody's credit card number. And
97:12 

97:12 that's exactly what happened with Sony
97:14 

97:14 PlayStation in 2011.
97:17 

97:17 When the server of PlayStation was
97:19 

97:19 hacked, hackers stole personal
97:21 

97:21 information from 77 million user
97:23 

97:23 accounts. Sony admitted in testimony
97:26 

97:26 that part of the credit card data was
97:29 

97:29 stored unencrypted in many cases.
97:32 

97:32 Failing to secure the database has grave
97:34 

97:34 consequences. In this case, the
97:37 

97:37 estimated loss was over $170 million and
97:41 

97:41 as the entire PlayStation network had to
97:44 

97:44 stay offline for 23 days to strengthen
97:47 

97:47 its security measures.
97:50 

97:50 The last goal is about scalability and
97:53 

97:53 flexibility.
97:54 

97:54 So scalability means your database can
97:57 

97:57 grow. It can handle more users, more
97:59 

97:59 data, and more traffic without slowing
98:02 

98:02 down or breaking. Flexibility means that
98:05 

98:05 your database can adapt when your app or
98:08 

98:08 business changes like adding new
98:10 

98:10 features or storing new type of data. A
98:13 

98:13 good database design makes the easy to
98:16 

98:16 grow and change without starting over
98:19 

98:19 from scratch. You can take different
98:21 

98:21 measures to enhance the scalability and
98:24 

98:24 flexibility of your database design. For
98:26 

98:26 example, you can implement a cache
98:28 

98:28 mechanism for frequently accessed data
98:31 

98:31 such as a product information in the
98:34 

98:34 database of an online store. Caching
98:36 

98:36 involves storing frequently accessed
98:39 

98:39 data in fast access memory such as RAM,
98:43 

98:43 which can significantly improve database
98:45 

98:45 performance and responsiveness,
98:47 

98:47 particularly as the data size grows.
98:50 

98:50 There are many popular caching
98:52 

98:52 mechanisms
98:53 

98:53 such as radius and meme cacheed.
98:57 

98:57 In this video, you'll learn about the
98:59 

98:59 five goals database design should strive
99:02 

99:02 to achieve. If you find this video
99:04 

99:04 helpful, give it a thumb up, subscribe,
99:07 

99:07 and leave your questions in the
99:08 

99:08 comments. I would love to hear what's
99:10 

99:10 working for you and what needs to
99:12 

99:12 change. This video is based on the book
99:14 

99:14 graing relational database design which
99:17 

99:17 dives deeper into database design using
99:19 

99:19 real world examples to explain the
99:21 

99:21 concepts. You can find the link to the
99:24 

99:24 book in the first comment. Most database
99:27 

99:27 projects fail before the first line of
99:29 

99:29 code is ever written. Why? Because the
99:32 

99:32 design is rushed or worse skipped
99:34 

99:34 entirely and have seen teams spend weeks
99:37 

99:37 coding only to discover the foundation
99:40 

99:40 was broken from day one. Suddenly,
99:42 

99:42 queries are slow, features can't scale.
99:45 

99:45 Fixing it means tearing everything down.
99:48 

99:48 But it doesn't have to be that way. In
99:50 

99:50 this video, I'll walk you through the
99:52 

99:52 actual process professionals follow to
99:55 

99:55 design reliable, scalable, and
99:57 

99:57 maintainable databases before touching a
99:59 

99:59 keyboard. We'll break it down step by
100:02 

100:02 step from gathering the right
100:04 

100:04 requirements to translating them into a
100:06 

100:06 robust data model to implementing and
100:09 

100:09 validating your design with data. The
100:11 

100:11 first key step of database design is
100:14 

100:14 requirement gathering. Requirement
100:16 

100:16 gathering refers to gathering the
100:17 

100:17 information about the database in
100:20 

100:20 different ways such as talking to all
100:22 

100:22 the people who will be involved with or
100:24 

100:24 using the database in the future,
100:26 

100:26 studying existing databases if there are
100:29 

100:29 any and examining other relevant aspects
100:32 

100:32 of information management. To talk to
100:34 

100:34 all the people who will be involved with
100:37 

100:37 or using the database, you need to
100:39 

100:39 organize meetings, ask good questions,
100:42 

100:42 and have conversations with different
100:44 

100:44 groups. Let's use the database of the
100:46 

100:46 online store, The Sci-Fi Collective, as
100:48 

100:48 an example. The Sci-Fi Collective
100:50 

100:50 happens to be a typical business. It has
100:53 

100:53 two owners, one manager, several staff
100:56 

100:56 who are responsible for tasks such as
100:58 

100:58 data entry and customer service, and
101:01 

101:01 three others who work on software
101:03 

101:03 development. For each group, you will
101:06 

101:06 need to prepare tailored interview
101:08 

101:08 questions like these.
101:11 

101:11 If you are new to this process, you need
101:13 

101:13 to know that the generative AI tools can
101:16 

101:16 help a lot with this step. For example,
101:19 

101:19 you can ask for some example questions
101:21 

101:21 from chat GTP using a prompt like this.
101:25 

101:25 Beyond the interview questions, you need
101:27 

101:27 to prepare many other things beforehand
101:30 

101:30 such as whether to take notes or record
101:33 

101:33 the conversation,
101:35 

101:35 if to record the interviews, whether you
101:38 

101:38 need to gain permission from the
101:40 

101:40 organization, how you want to record the
101:42 

101:42 interview or even where to conduct the
101:45 

101:45 interviews.
101:46 

101:46 That's everything about the requirement
101:48 

101:48 gathering step. Now let's talk about the
101:51 

101:51 next step. Analysis and design. The
101:54 

101:54 analysis and design step involves many
101:56 

101:56 tasks. Let's highlight a few important
101:59 

101:59 ones such as identify the goals of the
102:02 

102:02 database. Identify subjects
102:04 

102:04 characteristics and relationships. After
102:07 

102:07 that you have data modeling and a
102:09 

102:09 normalization. Let's go over these key
102:12 

102:12 tasks one after another. So we're going
102:15 

102:15 to start with goal identification.
102:18 

102:18 Undoubtedly this is the first thing you
102:20 

102:20 should be doing based on the information
102:22 

102:22 that you gather from the last step. I
102:25 

102:25 mean requirement analysis. Every
102:27 

102:27 database is created for some specific
102:30 

102:30 purpose. Whether that purpose is to
102:32 

102:32 handle the daily transaction of a
102:34 

102:34 business or manage the information of an
102:37 

102:37 organization. You need to identify the
102:39 

102:39 goal of the database clearly because it
102:42 

102:42 will be used to help guide your other
102:45 

102:45 decisions down the road. So what would a
102:47 

102:47 goal look like? Let's use the same
102:50 

102:50 example of the sci-fi collective. If you
102:53 

102:53 do the hard work to interview the owner,
102:55 

102:55 manager, staff, and a software
102:57 

102:57 developers, you may summarize the goal
102:59 

102:59 of this database as this. The database
103:03 

103:03 of the Sci-Fi Collective is to maintain
103:05 

103:05 information about products such as their
103:07 

103:07 inventory and reviews and information
103:10 

103:10 about users such as their payment
103:12 

103:12 information and purchases as well as a
103:15 

103:15 transaction information linking users
103:16 

103:16 and products. In short, this a typical
103:19 

103:19 database for an online store.
103:23 

103:23 So the next task is to identify
103:25 

103:25 subjects, characteristics and
103:27 

103:27 relationships. To identify subjects and
103:31 

103:31 characteristics from the record of the
103:33 

103:33 interview, you can look for nouns in the
103:36 

103:36 responses to your questions. How do you
103:38 

103:38 differentiate subjects from
103:40 

103:40 characteristics? Typically, if you can
103:43 

103:43 build a sentence with two nouns in which
103:46 

103:46 one has or own the other, the one that
103:50 

103:50 is possessed is a characteristic and the
103:53 

103:53 other is a subject. What am I talking
103:56 

103:56 about here? Let's use an example to make
103:59 

103:59 this a little bit more clear.
104:02 

104:02 For example, you can put user and a
104:04 

104:04 password in the following sentence. A
104:07 

104:07 user has a password. In this sentence,
104:10 

104:10 the password is possessed by the user.
104:13 

104:13 So, the password is a characteristic of
104:16 

104:16 a user whereas the user is a subject.
104:20 

104:20 You need to perform similar deduction on
104:22 

104:22 interview conversations. For example,
104:25 

104:25 you might ask the following questions to
104:28 

104:28 an IT staff member working for the
104:30 

104:30 sci-fi collective.
104:33 

104:33 What are the main tasks or activities
104:35 

104:35 you perform that involve data storage or
104:37 

104:37 retrieval? The participant may give you
104:40 

104:40 a response like this.
104:44 

104:44 This response, as you can see, is very
104:47 

104:47 detailed. The participant answered this
104:49 

104:49 question very well with a lot of useful
104:52 

104:52 details.
104:53 

104:53 You can track all the nouns and see
104:55 

104:55 whether they can be used in sentences
104:58 

104:58 with verbs like has or to identify
105:02 

105:02 subjects and characteristics.
105:05 

105:05 However, in real world, you may need to
105:07 

105:07 ask a lot of follow-up questions to
105:10 

105:10 solicit such detailed information. And
105:13 

105:13 that's why experienced interviewers
105:16 

105:16 actually start identifying subjects and
105:18 

105:18 characteristics as soon as they start
105:21 

105:21 the interview so that they can track
105:24 

105:24 which answer is vague and whether
105:26 

105:26 follow-up questions are needed
105:28 

105:28 immediately which will save a lot of
105:31 

105:31 time down the road. If the transcript of
105:34 

105:34 the interview becomes very long, you may
105:36 

105:36 also seek the help from generative AI
105:39 

105:39 with a request like this on the left
105:41 

105:41 side of the screen that says, "Help me
105:44 

105:44 identify the subjects and their
105:46 

105:46 corresponding characteristics from the
105:48 

105:48 transcript of the interview." However,
105:51 

105:51 you can't blindly trust AI to do this
105:53 

105:53 job perfectly. You will need to double
105:56 

105:56 check and make possibly a lot of small
105:59 

105:59 or big revisions to the work of AI.
106:03 

106:03 If this task is carried out accordingly,
106:06 

106:06 you will at least be able to summarize
106:08 

106:08 two subjects and four characteristics
106:11 

106:11 from the snippet of the interview. The
106:13 

106:13 two subjects are user and other. User
106:17 

106:17 has two characteristics account and a
106:19 

106:19 payment method because you can put them
106:22 

106:22 in sentences where verbs are either has
106:25 

106:25 ors.
106:27 

106:27 Similarly, order also has two
106:29 

106:29 characteristics.
106:30 

106:30 product information and a total price.
106:33 

106:33 If you do all the hard work required by
106:35 

106:35 this task, you will be able to identify
106:38 

106:38 this many subjects and their
106:40 

106:40 characteristics for the database of the
106:42 

106:42 sci-fi collective. You will also be able
106:45 

106:45 to identify the following relationships
106:47 

106:47 represented as a bunch of sentences. By
106:50 

106:50 relationship, I mean the connection
106:52 

106:52 between two different subjects and
106:55 

106:55 characteristics don't really play a role
106:57 

106:57 here. For example, we know that a user
107:00 

107:00 and order are two different subjects and
107:03 

107:03 their relationship is summarized here in
107:06 

107:06 the first item. A user can make multiple
107:09 

107:09 orders. An order can be made by only one
107:12 

107:12 user. With this information, you will be
107:16 

107:16 able to actually engage with data
107:18 

107:18 modeling. So, data modeling aims to
107:20 

107:20 create a conceptual design that shows
107:23 

107:23 how the parts of a database fit together
107:26 

107:26 and relate to each other. The conceptual
107:28 

107:28 design is typically represented visually
107:31 

107:31 as an entity relationship diagram or
107:34 

107:34 short for ER diagram. For example, the
107:37 

107:37 ER diagram of the sci-fi collective
107:40 

107:40 database may look like this. I know that
107:43 

107:43 this diagram on the screen contains
107:45 

107:45 symbols and syntax that you don't
107:47 

107:47 understand yet. They will be covered in
107:49 

107:49 detail in the future videos.
107:52 

107:52 When a ER diagram is drafted, the next
107:55 

107:55 task is typically normalization. So
107:57 

107:57 what's normalization? Normalization is
108:00 

108:00 about breaking down a table representing
108:02 

108:02 more than one entities into smaller
108:05 

108:05 tables. For example,
108:08 

108:08 someone designed a table product review
108:11 

108:11 to hold data of both products and their
108:14 

108:14 reviews. You know that storing
108:16 

108:16 information about more than one entity
108:18 

108:18 in the same table can lead to redundancy
108:21 

108:21 and anomalies. You could fix this bad
108:24 

108:24 design by breaking it into two different
108:26 

108:26 tables, product and review. So this is a
108:30 

108:30 snapshot of normalization.
108:32 

108:32 Well, the details of normalization are
108:35 

108:35 more nuanced and this concept has a set
108:39 

108:39 of clear defined theorem that can guide
108:41 

108:41 your actions which we will cover in the
108:44 

108:44 future. Now you have some basic
108:47 

108:47 understanding of what needs to be done
108:49 

108:49 and the second key step. Let's move to
108:52 

108:52 the next and the last step.
108:55 

108:55 The implementation/ integration and
108:58 

108:58 testing step involves building and
108:60 

108:60 validating the database based on the
109:02 

109:02 blueprint you made in the design and
109:04 

109:04 analysis step. At this step, you will
109:08 

109:08 convert your ER model to SQL code that
109:11 

109:11 creates the corresponding database and a
109:13 

109:13 tables. After that, you will populate
109:16 

109:16 the database with sample data. To test
109:19 

109:19 the three aspects of the database,
109:21 

109:21 including functionality, you need to
109:24 

109:24 check whether the database performs the
109:26 

109:26 expected tasks correctly such as
109:29 

109:29 creating, updating, and deleting data
109:32 

109:32 entries.
109:33 

109:33 the performance. You need to check how
109:36 

109:36 well the database handles a large amount
109:38 

109:38 of data or heavy use.
109:41 

109:41 Then the last one is security. You need
109:44 

109:44 to verify that the database has
109:46 

109:46 appropriate security measures in place
109:49 

109:49 to protect insensitive information such
109:52 

109:52 as passwords or payment methods. Based
109:54 

109:54 on the testing result, you may need to
109:57 

109:57 revise your design or fix bugs that you
109:59 

109:59 have identified. In case if you ever
110:02 

110:02 wonder where the test data can come
110:04 

110:04 from, it can either come from your
110:07 

110:07 client, I mean the person or the people
110:10 

110:10 you are designing the database for, or
110:13 

110:13 you can easily simulate such data with a
110:16 

110:16 help from generative AI.
110:18 

110:18 For example, you may derive an ER
110:21 

110:21 diagram from the analysis and design
110:23 

110:23 step like this. You can ask generative
110:26 

110:26 AI to help you generate sample data for
110:29 

110:29 your targeted database system. You can
110:31 

110:31 even make sure that the sample data is
110:34 

110:34 already in a series of insert commands
110:36 

110:36 that's ready to use. This is a task that
110:39 

110:39 generative AI excels at. In this video,
110:42 

110:42 we went through the three key steps in
110:45 

110:45 designing a database including
110:47 

110:47 requirement gathering, analysis and
110:49 

110:49 design, and implementation/ integration
110:52 

110:52 and testing. If you find this video
110:54 

110:54 helpful, give it a thumb up, subscribe,
110:57 

110:57 and leave your questions in the
110:59 

110:59 comments. I would love to hear what's
111:01 

111:01 working for you and what needs to
111:03 

111:03 change. This video is based on the book
111:06 

111:06 Groing Relational Database Design, which
111:08 

111:08 dives deeper into database design using
111:11 

111:11 real world examples to explain the
111:13 

111:13 concepts. You'll find the link to the
111:16 

111:16 book in the first comment. Most people
111:18 

111:18 think database design starts with
111:20 

111:20 tables, but it doesn't. It starts with
111:22 

111:22 messy real world ideas like students,
111:25 

111:25 email or course. And here's where
111:28 

111:28 beginners go wrong. They kind of tell
111:30 

111:30 what should be a table, what should be a
111:32 

111:32 column, or what should be ignored
111:34 

111:34 entirely. In this video, I'll show you
111:37 

111:37 how to turn vague ideas into clean and
111:39 

111:39 structured database tables so that your
111:42 

111:42 design doesn't break later. We'll cover
111:45 

111:45 two topics starting with subjects and
111:47 

111:47 characteristics, then to entities and
111:50 

111:50 attributes. Let's get it started. The
111:53 

111:53 database design process starts with
111:55 

111:55 interviewing relevant people of the
111:57 

111:57 organization who's going to use the
111:59 

111:59 database like the stakeholder, software
112:01 

112:01 developers and technical staff. Beyond
112:04 

112:04 the interview, you are also going to
112:06 

112:06 gather sample data and study the old
112:08 

112:08 system if there is any. For example,
112:11 

112:11 think about your designing the database
112:13 

112:13 for an online store named the sci-fi
112:15 

112:15 collective that is specialized in
112:17 

112:17 selling only sci-fi stuff. You will need
112:20 

112:20 to design different interview questions
112:22 

112:22 to interview different groups of
112:24 

112:24 relevant people and record the interview
112:27 

112:27 as well. The recorded data allows you to
112:30 

112:30 conduct the necessary analysis that
112:32 

112:32 identifies key information, the subjects
112:35 

112:35 and characteristics.
112:37 

112:37 Why do we need to care about subjects
112:39 

112:39 and characteristics?
112:41 

112:41 Well, valid subjects will ultimately
112:44 

112:44 turn into tables in your database design
112:46 

112:46 and valid characteristics will turn into
112:49 

112:49 different columns in the corresponding
112:51 

112:51 tables. For example, think about a
112:54 

112:54 conversation with a software developer
112:56 

112:56 from the sci-fi collective as is
112:58 

112:58 presented here. You asked this question.
113:02 

113:02 What are the main tasks or activities
113:04 

113:04 you perform that involve data storage or
113:06 

113:06 retrieval? and get this response from
113:10 

113:10 the one that you interviewed. If you
113:12 

113:12 analyze this response, you will be able
113:15 

113:15 to identify these subjects and
113:17 

113:17 characteristics as is highlighted here
113:20 

113:20 on the screen. How is the analysis
113:22 

113:22 carried out? The analysis focuses on the
113:25 

113:25 nouns that can be used in sentences with
113:28 

113:28 a verb like has or
113:32 

113:32 like this. If you parse this response,
113:35 

113:35 you will get two subjects user and other
113:40 

113:40 as is shown here and a four
113:41 

113:41 characteristics like account and payment
113:44 

113:44 methods associated with the user and a
113:47 

113:47 product information and a total price
113:50 

113:50 associated with the order. This analysis
113:52 

113:52 step can be labor intensive but doesn't
113:55 

113:55 necessarily involve a whole lot of
113:57 

113:57 critical thinking and decision-m. So you
113:60 

113:60 may ask a generative AI tool to give you
114:02 

114:02 a hand with a request for chat GTP like
114:05 

114:05 thus I'm doing database design help me
114:08 

114:08 identify the subjects and their
114:10 

114:10 corresponding characteristics from the
114:12 

114:12 transcript of the interview and as in
114:15 

114:15 provide your transcript the context I'm
114:18 

114:18 doing database design is kind of
114:20 

114:20 important without it chat GTP may or may
114:23 

114:23 not know what you're doing and one
114:26 

114:26 important feature about generative AI
114:28 

114:28 tools is that they are terrible at
114:31 

114:31 asking clarifying questions or say they
114:33 

114:33 don't ask any clarifying questions at
114:36 

114:36 all. Of course, you still need to
114:38 

114:38 evaluate the work from AI and make a lot
114:41 

114:41 of revisions and eventually make your
114:43 

114:43 final call. If you do all the hard work
114:46 

114:46 required by this task for the sci-fi
114:48 

114:48 collective, you will be able to identify
114:51 

114:51 this many subjects and their
114:53 

114:53 characteristics as are presented here on
114:55 

114:55 the screen. With such information,
114:58 

114:58 you're ready to start your design. The
115:01 

115:01 next topic is entities and attributes.
115:04 

115:04 And this is the step where you convert
115:06 

115:06 the identified subjects and
115:08 

115:08 characteristics to entities and
115:10 

115:10 attributes. Wait a second. Did I mention
115:13 

115:13 that a subjects will be turned into
115:15 

115:15 tables and a characteristics will be
115:17 

115:17 turned into columns? Yes, I did. But I
115:21 

115:21 was not accurate enough. Between a
115:24 

115:24 subject and a table, there's a middleman
115:27 

115:27 called entity. An entity is a conceptual
115:29 

115:29 idea that you deal with during the
115:32 

115:32 design process, whereas a table is its
115:35 

115:35 corresponding implementation.
115:37 

115:37 The same thing can be said between an
115:39 

115:39 attribute and a column. So yes, a
115:41 

115:41 subject will ultimately be turned into a
115:44 

115:44 table, but it needs to be mapped to an
115:46 

115:46 entity first. characteristics will be
115:50 

115:50 mapped to attributes before they are
115:52 

115:52 implemented as columns. Now you are on
115:55 

115:55 the same page in terms of the
115:57 

115:57 terminology. Let's go over the key
115:59 

115:59 things in the mapping process.
116:02 

116:02 What's on the screen is an example of
116:05 

116:05 the mapping process. The user entity and
116:08 

116:08 its corresponding characteristics are
116:10 

116:10 what we identified for the database of
116:13 

116:13 the sci-fi collective. And when you map
116:15 

116:15 them to entities/attributes,
116:17 

116:17 it will look like this on the right side
116:20 

116:20 of the arrow. There are two key things
116:22 

116:22 that I wanted to cover. The first is the
116:25 

116:25 entity representation. The second is the
116:28 

116:28 naming convention. First, how is the
116:31 

116:31 entity represented in the ultimate
116:33 

116:33 design diagram.
116:35 

116:35 By the end of the design and analysis
116:37 

116:37 step, you will deliver a design diagram.
116:41 

116:41 The representation is relatively
116:43 

116:43 straightforward. We use a rectangular
116:46 

116:46 box to represent an entity and separate
116:50 

116:50 the entity name and its attribute names
116:53 

116:53 using a solid line. The attribute names
116:56 

116:56 will be listed one after another in
116:58 

116:58 different rows in this box. For example,
117:01 

117:01 the mapping between the subject user and
117:04 

117:04 its corresponding entity representation
117:06 

117:06 looks like this. The entity is
117:09 

117:09 represented as a rectangular box. As you
117:12 

117:12 can see here, the entity name user is
117:15 

117:15 separated from its attributes by a solid
117:18 

117:18 line. Okay, that's the first point. The
117:22 

117:22 second important thing about mapping
117:24 

117:24 subjects to entities is the naming
117:27 

117:27 convention. Of course, you're free to
117:30 

117:30 choose whatever suits you when it comes
117:32 

117:32 to name an entity and an attribute, but
117:35 

117:35 doing so arbitrarily will be bad for
117:37 

117:37 your design, let alone the
117:39 

117:39 implementation that comes afterward.
117:42 

117:42 Just think about the inconsistent usage
117:44 

117:44 of IDs and how much trouble that would
117:47 

117:47 introduce to the database admin software
117:49 

117:49 developers who need to build on top of
117:52 

117:52 the database and the analysts who need
117:54 

117:54 to extract the data from the database.
117:58 

117:58 As a result, you are strongly
117:60 

117:60 recommended to pick one of the naming
118:02 

118:02 conventions and stick to it strictly.
118:05 

118:05 The most popular one is the snake case.
118:08 

118:08 Camel case and Pascal comes as the
118:11 

118:11 second and the third popular options.
118:13 

118:13 Whichever you choose, you need to be
118:16 

118:16 consistent and stick to it. For example,
118:19 

118:19 in this video, we will stick to the
118:21 

118:21 snake case from now on. The feature of
118:24 

118:24 the snake case is that words are all
118:27 

118:27 lowercase letters and separated using
118:30 

118:30 underscores. When the snake case is
118:32 

118:32 used, the subject of users will be
118:35 

118:35 mapped to the user entity as you see
118:38 

118:38 here for the sci-fi collective database.
118:41 

118:41 Beyond the selection of the use of a
118:44 

118:44 consistent naming convention, you also
118:47 

118:47 want to pay attention to whether to use
118:49 

118:49 singular or plural in the naming.
118:52 

118:52 Singular names have their roots in
118:54 

118:54 objected oriented programming and the
118:57 

118:57 plural names are more natural as table
118:59 

118:59 names. I'm not going to get into the
119:02 

119:02 debate on which one is better. Whichever
119:05 

119:05 you choose, just to be consistent. The
119:07 

119:07 inconsistency on something as small as
119:10 

119:10 this could easily drive a dozen of
119:12 

119:12 software developers crazy. In this
119:14 

119:14 video, we will stick to singular names
119:16 

119:16 for entities. Next, the upper limit of
119:20 

119:20 name length. Many database systems have
119:23 

119:23 limits on column name length. Marand DB
119:26 

119:26 for example limits column names to 64
119:30 

119:30 characters. If you have such a column
119:32 

119:32 name, you should shorten it. Last, you
119:36 

119:36 should make sure that you are not using
119:38 

119:38 any of the reserved SEO keywords as
119:41 

119:41 names. Some of the comma reserved
119:43 

119:43 keywords include table, anning, data, or
119:46 

119:46 order. The list goes on and on. If you
119:50 

119:50 want a list of comma reserved keywords
119:52 

119:52 in SQL, generative AI can help a lot.
119:55 

119:55 However, if you want a list of reserved
119:58 

119:58 keywords of a specific database system
120:01 

120:01 like my SQL, you need to check its menu
120:04 

120:04 instead of relying on generative AI
120:07 

120:07 because the information from generative
120:09 

120:09 AI tools may not be fully up to date.
120:12 

120:12 Now you know that the reserved keywords
120:15 

120:15 need to be avoided and we happen to have
120:18 

120:18 such a subject named order. If we were
120:21 

120:21 to stick to this name, we will be using
120:24 

120:24 a comma reserved SQL keyword. This will
120:27 

120:27 lead to many problems such as syntax
120:29 

120:29 errors in SQL query execution and
120:32 

120:32 maintainability problems. As a result,
120:36 

120:36 we will rename this entity as purchase.
120:39 

120:39 So packing up all you need to know about
120:42 

120:42 how to map subjects to entities. The
120:45 

120:45 five subjects of the sci-fi collective
120:47 

120:47 database will be turned into five
120:50 

120:50 entities as is presented here on the
120:52 

120:52 screen.
120:54 

120:54 In this video, we went through how to
120:57 

120:57 convert messy real world ideas to
120:59 

120:59 entities and attributes. If you find
121:01 

121:01 this video helpful, consider giving it a
121:04 

121:04 thumb up, subscribe, and leave your
121:06 

121:06 questions in the comments. I would love
121:09 

121:09 to hear what's working for you and what
121:11 

121:11 needs to change. This video is based on
121:13 

121:13 the book Groing Relational Database
121:15 

121:15 Design which dives deeper into database
121:18 

121:18 design using real world examples to
121:20 

121:20 explain the concepts. You can find a
121:23 

121:23 link to the book in the first comment.
121:25 

121:25 Real companies have paid a huge price
121:27 

121:27 for not getting this right. In health
121:29 

121:29 care systems, there have been cases
121:32 

121:32 where patients records got accidentally
121:34 

121:34 merged simply because names and
121:36 

121:36 birthdays were used as keys. But people
121:39 

121:39 change their names, they get married,
121:41 

121:41 they get divorced, and suddenly two
121:44 

121:44 different patients become one record.
121:46 

121:46 The UK National Health Service spent
121:48 

121:48 hundreds of millions fixing this kind of
121:50 

121:50 data integrity disasters because the
121:53 

121:53 system couldn't reliably identify who
121:55 

121:55 was who. And it all comes down to one
121:57 

121:57 thing, how you choose your primary key.
121:59 

121:59 In this video, I'll show you exactly
122:01 

122:01 what a primary key is and how it's
122:04 

122:04 different from candidate keys and super
122:06 

122:06 keys and most importantly, how to pick
122:08 

122:08 the right one to avoid these kind of
122:10 

122:10 real world failures.
122:13 

122:13 Let's get started with the first topic
122:15 

122:15 on keys. Think about your designing a
122:18 

122:18 database for an online store named the
122:20 

122:20 sci-fi collective. based on the gathered
122:23 

122:23 requirements of the system and a sample
122:25 

122:25 data. It derived the following subjects
122:28 

122:28 and characteristics and also
122:31 

122:31 successfully turned them into a set of
122:33 

122:33 entities and attributes like what is
122:35 

122:35 shown here on the screen. As you may
122:38 

122:38 noticed, you used the snake case naming
122:40 

122:40 convention and derived five entities in
122:43 

122:43 total. Each entity has a set of
122:45 

122:45 associated attributes. Now your task is
122:48 

122:48 to identify the primary key for each
122:51 

122:51 entity. The first thing that you need to
122:54 

122:54 know is that each table or say entity
122:56 

122:56 needs a primary key. To expand on this
122:59 

122:59 and define primary key explicitly, it's
123:03 

123:03 one or more attributes that can be used
123:05 

123:05 to identify an individual rule. That
123:09 

123:09 said, the values of the primary key
123:11 

123:11 column are all unique. Otherwise, they
123:14 

123:14 can't be used to identify each
123:16 

123:16 individual rule anymore.
123:18 

123:18 Last, a table has only one primary key.
123:22 

123:22 And that's the full definition of
123:24 

123:24 primary keys. Based on the definition of
123:27 

123:27 primary keys, we can peek at the process
123:30 

123:30 of picking the primary key given a
123:32 

123:32 table. There are two steps involved.
123:35 

123:35 First, we will try to pick the best
123:37 

123:37 candidate key as the primary key. If we
123:41 

123:41 managed to do that then the job is done.
123:43 

123:43 If not I mean if there's no good
123:46 

123:46 candidate keys available we will create
123:49 

123:49 a new attribute to serve as a primary
123:52 

123:52 key. Here we mention something that
123:55 

123:55 requires further explanation. The
123:57 

123:57 candidate key. So what's a candidate key
124:00 

124:00 exactly? A candidate key is the smallest
124:03 

124:03 possible combination of attributes that
124:06 

124:06 can uniquely identify a row in a table.
124:10 

124:10 For example, let's look at the user
124:12 

124:12 entity from the sci-fi collective
124:14 

124:14 database. From the gathered
124:16 

124:16 requirements, you know that the
124:18 

124:18 username, email, and a phone number all
124:21 

124:21 need to be unique for each user. That
124:24 

124:24 said, they have all met the requirements
124:26 

124:26 of being candidate keys. Yes, they are
124:29 

124:29 all candidate keys of the user entity.
124:32 

124:32 What about the combination of the first
124:34 

124:34 name and a username? It can also
124:37 

124:37 uniquely identify each row in the user
124:39 

124:39 table because username is unique per
124:43 

124:43 user and is a combination of the first
124:45 

124:45 name and username can do the job too.
124:48 

124:48 Does the first name help or contribute
124:50 

124:50 to this task of identifying each row
124:53 

124:53 uniquely? Not at all. It's all because
124:56 

124:56 of the username. So, is this combo a
124:59 

124:59 candidate key? No, it's not because it's
125:03 

125:03 not the smallest possible combination of
125:06 

125:06 attributes that can do the job. I mean,
125:08 

125:08 you can remove the first name attribute.
125:11 

125:11 It can still do the same thing
125:12 

125:12 perfectly.
125:14 

125:14 As a result, it's not a candidate key.
125:17 

125:17 Well, the story doesn't really end here
125:19 

125:19 for this combo. We call combos of
125:22 

125:22 attributes like thus the super key. Same
125:25 

125:25 as candidate keys, super keys can also
125:28 

125:28 identify each row uniquely in a table.
125:30 

125:30 But shame on them, they are not the
125:33 

125:33 smallest combination, which means that
125:35 

125:35 you can also find a subset of attributes
125:38 

125:38 from them to do the same job. As a
125:41 

125:41 result, the relationship between super
125:43 

125:43 keys, candidate keys, and primary keys
125:46 

125:46 can be visualized as what you can see
125:48 

125:48 here on the screen. A table may have
125:51 

125:51 many super keys. Some of them are
125:54 

125:54 candidate keys and among those candidate
125:56 

125:56 keys, there's only one that you would
125:59 

125:59 pick as a primary key.
126:02 

126:02 That's everything about the definition
126:04 

126:04 of primary candidate and super keys.
126:06 

126:06 Let's get a practical about how to pick
126:08 

126:08 the right primary key. Next,
126:11 

126:11 remember we talked about this. There are
126:14 

126:14 two steps in picking the right primary
126:16 

126:16 key. First, we try to pick the best
126:19 

126:19 candidate key as a primary key. If we
126:22 

126:22 manage to do that, the job is done. If
126:25 

126:25 not, I mean if there's no good candidate
126:28 

126:28 key available, we will create a new
126:30 

126:30 attribute to serve as the primary key.
126:33 

126:33 Let's focus on the first step for now.
126:36 

126:36 Do you remember the user entity? We have
126:39 

126:39 identified three candidate keys for this
126:41 

126:41 entity including the username, email,
126:44 

126:44 and a phone number attributes. Once you
126:47 

126:47 identify all the candidate keys, you
126:50 

126:50 need to pick one as a primary key. To
126:53 

126:53 achieve this, you will use a set of
126:56 

126:56 standards to help with the decision
126:58 

126:58 making process. Here are some commonly
127:00 

127:00 used criteria. The most important one,
127:03 

127:03 of course, is whether an attribute
127:05 

127:05 contains only unique values and whether
127:08 

127:08 it can be null or not. The minimum
127:11 

127:11 requirement for the primary key is that
127:13 

127:13 it contains all unique values and its
127:16 

127:16 values cannot be null. Beyond that, the
127:20 

127:20 other criteria is check whether an
127:22 

127:22 attribute is stable or not. By that I
127:25 

127:25 mean whether its values change a lot or
127:27 

127:27 not. You also need to consider whether
127:30 

127:30 an attribute is simple, short and
127:33 

127:33 familiar to you. If an attribute
127:35 

127:35 contains values that are relatively
127:37 

127:37 short, it will help speed up the
127:40 

127:40 database. Last, sometimes you may care
127:43 

127:43 whether the primary key can prevent
127:46 

127:46 redundant rows or not. By that I mean
127:49 

127:49 some natural keys can inherently prevent
127:52 

127:52 meaningful duplications.
127:54 

127:54 And if preventing duplications is
127:56 

127:56 important to you, those attributes will
127:59 

127:59 be great choices for primary keys. If we
128:02 

128:02 apply these criteria to the three
128:05 

128:05 candidate keys in this example, we will
128:08 

128:08 have some results like this. Assuming
128:11 

128:11 that the sci-fi collective allows users
128:14 

128:14 to register an account without providing
128:16 

128:16 phone numbers, it will make phone number
128:19 

128:19 attribute allow no values. So it can't
128:22 

128:22 be the primary key between username and
128:25 

128:25 email. It can be a harder decision. If
128:29 

128:29 the system allows users to change
128:31 

128:31 username in every 14 days, emails would
128:34 

128:34 be a better choice. When a primary key
128:37 

128:37 is picked, you will need to indicate it
128:40 

128:40 clearly in the representation of the
128:43 

128:43 entity. For example, one way to do it is
128:46 

128:46 to place a K symbol by the primary key
128:49 

128:49 attribute like what you see here on the
128:52 

128:52 screen. In the user entity, we placed
128:55 

128:55 this case symbol at the very left side
128:57 

128:57 of the email attribute which is picked
128:60 

128:60 as a primary key.
129:02 

129:02 Let's look at another example. The
129:05 

129:05 product table in the sci-fi collective
129:07 

129:07 database. Based on the gathered
129:09 

129:09 requirements and information, we
129:12 

129:12 identified two candidate keys for this
129:14 

129:14 entity or say this table. The first is
129:17 

129:17 the product code. The second is a
129:20 

129:20 combination of product name and
129:22 

129:22 manufacturer. We call combo like thus
129:26 

129:26 the composite key. Both candidate keys
129:30 

129:30 can uniquely identify or say a product
129:33 

129:33 in this table.
129:35 

129:35 The concept of code comes from the
129:38 

129:38 universal product code. Each products
129:41 

129:41 being shipped and sold around the world
129:43 

129:43 has a unique product code. To pick a
129:47 

129:47 primary key between the two options, you
129:50 

129:50 will use the same criteria that you just
129:52 

129:52 saw in the last example. In this case,
129:55 

129:55 the product code is a clear winner over
129:58 

129:58 the combination of product name and
130:00 

130:00 manufacturer. The product code is
130:03 

130:03 simpler and shorter. So we can add a key
130:06 

130:06 symbol by the left side of the code
130:09 

130:09 attribute like this. So far we talked
130:12 

130:12 about how to pick the best candidate key
130:14 

130:14 as a primary key. But it doesn't always
130:17 

130:17 work. Sometimes none of the identified
130:19 

130:19 candidate keys would work or there are
130:22 

130:22 no candidate keys can be identified at
130:25 

130:25 all. Then we go to the second step which
130:27 

130:27 is to create a new attribute to serve as
130:30 

130:30 a primary key. Let's look at an example.
130:34 

130:34 The review entity from the cipher
130:36 

130:36 collective database. This entity
130:38 

130:38 represents product reviews and it has
130:42 

130:42 two identified attributes. The review
130:44 

130:44 text and the review time. Neither of the
130:47 

130:47 two attributes can serve as a candidate
130:49 

130:49 key. Not even the combination of the two
130:52 

130:52 attributes. Think about it. Two persons
130:55 

130:55 can submit the same worded reviews at
130:58 

130:58 the same time for the same or different
131:01 

131:01 products. None of them can identify a
131:04 

131:04 product review uniquely.
131:07 

131:07 When you apply the criteria for the
131:10 

131:10 primary key selection to them, it will
131:13 

131:13 become even more obvious.
131:15 

131:15 That said, you don't have a good
131:17 

131:17 candidate key available in the review
131:19 

131:19 table. What do you do in this case? When
131:23 

131:23 something like this happens, you will
131:25 

131:25 create a new attribute to serve as a
131:28 

131:28 primary key. For example, we can create
131:31 

131:31 an artificial attribute named the review
131:34 

131:34 ID to serve as a primary key in the
131:37 

131:37 review table. We call this type of
131:39 

131:39 artificial primary key the surrogate
131:42 

131:42 key. It is a non-miniful column that can
131:45 

131:45 uniquely identify rows in a table and
131:49 

131:49 its values are typically auto
131:51 

131:51 incrementing integers. The automatic
131:54 

131:54 increment is handled by the database
131:56 

131:56 system.
131:58 

131:58 As curious as you are, you may be
132:00 

132:00 thinking whether we can do the same
132:02 

132:02 thing to every table. I mean forcing
132:05 

132:05 every table to use a surrogate key as a
132:08 

132:08 primary key. This will save us the
132:10 

132:10 effort of syncing. After all, what's so
132:13 

132:13 different between using product code and
132:15 

132:15 a surrogate key product ID as a primary
132:18 

132:18 key for the product table? If you ever
132:21 

132:21 used generative AI tools to help you
132:24 

132:24 choose primary keys, you will know that
132:28 

132:28 such tools strongly prefer to use
132:30 

132:30 surrogated keys regardless of the
132:32 

132:32 situation. It's worth mentioning that
132:35 

132:35 surrogated keys may not be the best
132:37 

132:37 design choice. Natural keys have the
132:40 

132:40 power to reduce duplications inherently
132:43 

132:43 whereas surrogate keys cannot do that.
132:47 

132:47 In this example, as you can see here on
132:49 

132:49 the screen, product ID column by itself
132:53 

132:53 can identify each row or each product
132:56 

132:56 uniquely but can't prevent the same
132:58 

132:58 product information being entered
133:00 

133:00 repeatedly as different rows. To reduce
133:04 

133:04 duplications when segregated keys are
133:07 

133:07 used, you will have to use the help from
133:10 

133:10 a unique constraint which we will cover
133:13 

133:13 in the future. That said, in a situation
133:16 

133:16 like this, I mean for the product table,
133:19 

133:19 the product code as a natural key is
133:22 

133:22 certainly a better choice as a primary
133:24 

133:24 key than a surrogate key plus a
133:27 

133:27 constraint. That said, you should stick
133:29 

133:29 to the two given steps to pick the
133:32 

133:32 primary key for a table instead of just
133:34 

133:34 using surrogate keys everywhere.
133:37 

133:37 If you follow the two steps to pick the
133:40 

133:40 right primary keys for each table for
133:43 

133:43 the sci-fi collective database,
133:46 

133:46 they may turn out like this with two
133:48 

133:48 tables using natural keys and three
133:50 

133:50 tables using surrogate keys as their
133:53 

133:53 corresponding primary keys.
133:56 

133:56 In this video, we went through the
133:58 

133:58 definition of keys and how to pick the
133:60 

134:00 right primary key for a table. If you
134:02 

134:02 find this video helpful, give it a thumb
134:05 

134:05 up, subscribe, and leave your questions
134:07 

134:07 in the comments. I would love to hear
134:09 

134:09 what's working for you and what needs to
134:11 

134:11 change. This video is based on the book
134:14 

134:14 Groing Relational Database Design, which
134:16 

134:16 dives deeper into database design using
134:18 

134:18 real world examples to explain the
134:21 

134:21 concepts. You'll find a link to the book
134:24 

134:24 in the first comment.
134:25 

134:25 If you have ever used the var 255 as a
134:29 

134:29 data type for as many string columns as
134:31 

134:31 possible in your database design, you're
134:34 

134:34 not alone. But you might be designing a
134:36 

134:36 database that performs 10 times worse
134:38 

134:38 than it needs to be. Choosing a string
134:41 

134:41 data type is like picking the right box
134:43 

134:43 for shipping. Too small, your data gets
134:46 

134:46 cut off. Too big, you will waste the
134:49 

134:49 space and money. In this video, I'll
134:52 

134:52 show you how to choose the right string
134:54 

134:54 data type for string columns in database
134:56 

134:56 design. We will cover three most
134:58 

134:58 commonly used string data types in this
135:01 

135:01 video. Let's get started. So, data type
135:04 

135:04 selection. Data type selection is a task
135:07 

135:07 that you take on once you have
135:09 

135:09 identified all the entities, attributes,
135:11 

135:11 and the primary keys for each entity.
135:14 

135:14 Imagine that you're working towards
135:16 

135:16 designing a database for an online store
135:18 

135:18 named the sci-fi collective and you have
135:21 

135:21 identified all the entities attributes
135:24 

135:24 and the primary keys. Your next step
135:27 

135:27 will be to select the right data type
135:29 

135:29 for each column. To do this task right,
135:32 

135:32 you need to pick up some knowledge about
135:34 

135:34 data types first. And you will start
135:37 

135:37 with string data types. The most
135:40 

135:40 commonly used string data types are
135:42 

135:42 these three as listed here on the
135:44 

135:44 screen. Car, vcar, and a text. Car is
135:48 

135:48 short for character and vcar is short
135:51 

135:51 for variable character. As a data type,
135:54 

135:54 car is used to store strings of a fixed
135:57 

135:57 length. Vcar is used to store variable
136:00 

136:00 lens strings with a known length and a
136:03 

136:03 text is used to store highly variable
136:06 

136:06 strings without known length limit.
136:08 

136:08 Assuming that you are very familiar with
136:11 

136:11 string data types and know all the
136:13 

136:13 details beyond what is shown to you so
136:16 

136:16 far. The task of data type selection is
136:19 

136:19 basically to identify a string attribute
136:21 

136:21 and then pick the right string data type
136:24 

136:24 for it based on the requirements you
136:26 

136:26 gathered. Now let's look at each of the
136:29 

136:29 string data type more closely. Starting
136:32 

136:32 with car. Car stores strings of a fixed
136:36 

136:36 length. As you can see here from this
136:39 

136:39 example, if you have a column
136:41 

136:41 representing code of states in US, each
136:45 

136:45 code will be exactly two characters long
136:48 

136:48 and you will use card two to represent
136:51 

136:51 its size. The two that is wrapped up in
136:54 

136:54 the bracket indicates that this column
136:57 

136:57 will occupy exactly two bytes. So when
137:00 

137:00 should you consider car as a column data
137:03 

137:03 type? When the data of a column are all
137:06 

137:06 strings of fixed and a uniform length
137:10 

137:10 like country code, currency code,
137:13 

137:13 airport code or even language code. When
137:16 

137:16 you have a column that has this feature,
137:19 

137:19 you should think about assigning car as
137:22 

137:22 its data type. For example, this is a
137:26 

137:26 product table for the sci-fi collective
137:28 

137:28 database. The primary key code stores
137:32 

137:32 the universal product code of different
137:34 

137:34 products being sold by the store and
137:37 

137:37 each code has the exact length of 12
137:40 

137:40 characters. As a result, you can declare
137:43 

137:43 it data type as car 12 where 12
137:45 

137:45 indicates its fixed length. You will put
137:48 

137:48 this data type at the right side of the
137:52 

137:52 corresponding column. For another
137:54 

137:54 example, the payment method entity
137:56 

137:56 represents different payment methods in
137:58 

137:58 the cipher collective database. For all
138:01 

138:01 the credit and debit cards as payment
138:03 

138:03 methods, their card number have the same
138:06 

138:06 digits, which is 16. Beyond that, their
138:09 

138:09 expiration dates all have four digits.
138:12 

138:12 So, it's safe to declare their data
138:15 

138:15 types as car and assign car 16 to the
138:19 

138:19 card number attribute and a car 4 to the
138:22 

138:22 expir date attribute. As curious as you
138:25 

138:25 are, you may wonder if card numbers and
138:28 

138:28 expiration dates should be numbers
138:30 

138:30 instead of strings. A good rule of thumb
138:33 

138:33 is whether you need to perform
138:35 

138:35 calculations on those numbers. If not,
138:38 

138:38 they had a better stay as strings. If
138:40 

138:40 you declare zero data types as numbers,
138:43 

138:43 you may have to deal with many
138:44 

138:44 inconveniences like leading zeros.
138:47 

138:47 Another question you may have is why
138:50 

138:50 going with varcar as a variable
138:52 

138:52 character string is a bad choice here
138:54 

138:54 for attributes like card numbers or
138:56 

138:56 product code. Well, varcar 255
139:00 

139:00 certainly would work in this case for
139:03 

139:03 all the three attributes in both
139:05 

139:05 entities. However, if you do so, you
139:08 

139:08 risk wasting space and money. On
139:10 

139:10 average, car is 20% faster than VCAR.
139:14 

139:14 Plus, you will waste some extra space by
139:17 

139:17 using Vcar here, and that would cost you
139:20 

139:20 millions of dollars if the database is
139:23 

139:23 used in a highly scalable environment
139:25 

139:25 where millions of users are involved.
139:29 

139:29 The next string data type you will take
139:31 

139:31 a closer look is var. Marker attributes
139:34 

139:34 store variable length strings with a
139:37 

139:37 maximum length specified. As you can see
139:40 

139:40 from this example column, it's named
139:43 

139:43 names with some sample data shown up
139:46 

139:46 here on the screen. Marker 30 as the
139:48 

139:48 data type for this column specifies the
139:51 

139:51 maximum length of data in this attribute
139:54 

139:54 as 30. Each data point comes with a
139:57 

139:57 different length like a Bob has a length
140:00 

140:00 of three and Jigglypuff has a length of
140:04 

140:04 10. If it is actually running a database
140:07 

140:07 system, Bob will occupy four bytes
140:09 

140:09 because var needs to store the actual
140:12 

140:12 data length in addition to its actual
140:14 

140:14 length. The same can be said for
140:17 

140:17 Jigglypuff which will take 11 bytes. So
140:20 

140:20 when should you consider using marker as
140:23 

140:23 a data type for some columns?
140:26 

140:26 When you need to store variable lens
140:28 

140:28 text data, but the maximum length is
140:32 

140:32 reasonable and easy to estimate. For
140:34 

140:34 example, the user entity from the sci-fi
140:37 

140:37 collective database has quite a few
140:39 

140:39 attributes that can use the help from a
140:42 

140:42 var data types like email. Different
140:45 

140:45 email addresses come with different
140:47 

140:47 lengths, but the maximum length of an
140:49 

140:49 arbitrary email address is 320
140:53 

140:53 characters based on the RFC standards.
140:56 

140:56 If you don't know that, it's totally
140:58 

140:58 fine. This is where generative AI tools
141:02 

141:02 can offer a lot of help. The same can be
141:05 

141:05 said for comma first names. Its length
141:08 

141:08 may vary a lot, but it's almost certain
141:11 

141:11 to use less than 50 characters. If you
141:14 

141:14 don't know that, generative AI tools can
141:17 

141:17 offer a lot of help on figuring out the
141:20 

141:20 maximum length of var data types. So you
141:24 

141:24 can declare the data type of email
141:26 

141:26 attribute as var 320
141:29 

141:29 and the first name attribute as var 50.
141:32 

141:32 It's worth noting that an accurate max
141:34 

141:34 length of var can be more important than
141:37 

141:37 you think and you shouldn't assign it
141:39 

141:39 arbitrarily. If it is too short, it will
141:42 

141:42 cut a data off. If it is too big, it
141:45 

141:45 will make you waste the space and money.
141:47 

141:47 Dropbox once saved hundreds of terabyte
141:50 

141:50 space by simply reestimating the maximum
141:54 

141:54 length of columns in its database. A
141:57 

141:57 change as small as from varcar 124 to 80
142:02 

142:02 may seem very trivial, but it can have a
142:05 

142:05 big impact if you are in a highly
142:07 

142:07 scalable environment. A rule of thumb of
142:11 

142:11 using varcar is listed here. If the
142:14 

142:14 variable data length of a column is
142:16 

142:16 smaller than 1,000, you should think
142:19 

142:19 about varcar. If it goes beyond 1,000,
142:22 

142:22 you should consider the next string data
142:25 

142:25 type that we are going to talk about,
142:27 

142:27 which is text. Text attributes store
142:30 

142:30 strings of variable lengths and they
142:32 

142:32 almost have no maximum length like the
142:36 

142:36 comment column example that you can see
142:38 

142:38 here on the screen. A comment can be
142:40 

142:40 very short just three or two words but
142:43 

142:43 it can also be something that goes
142:45 

142:45 beyond 10 different paragraphs and as
142:47 

142:47 long as an article. When should you
142:49 

142:49 consider using text as a data type for
142:52 

142:52 some columns? When you have large blocks
142:56 

142:56 of texts and its maximum length is
142:59 

142:59 difficult to estimate. Think about the
143:02 

143:02 review entity from the sci-fi collective
143:04 

143:04 database. If you ever read some reviews
143:07 

143:07 on Amazon, they can be very verbose and
143:10 

143:10 long, but can also be very short, as
143:13 

143:13 short as a single word. Its maximum
143:15 

143:15 length cannot be easily estimated. So in
143:19 

143:19 this case, text is the best choice for
143:23 

143:23 its data type. A rule of thumb when it
143:26 

143:26 comes to text is when the data of a
143:28 

143:28 column may go beyond 1,000 characters
143:31 

143:31 and its maximum length is hard to know.
143:34 

143:34 In such a case, you should consider
143:36 

143:36 using text as the data type for that
143:38 

143:38 column. In this video, we covered the
143:41 

143:41 most commonly used string data types. If
143:44 

143:44 you find this video helpful, give it a
143:47 

143:47 thumb up. Subscribe and leave your
143:50 

143:50 questions in the comments. I would love
143:52 

143:52 to hear what's working for you and what
143:54 

143:54 needs to change. This video is based on
143:56 

143:56 the book graing relational database
143:58 

143:58 design which dives deeper into database
144:01 

144:01 design using real world examples to
144:04 

144:04 explain the concepts. You'll find the
144:06 

144:06 link to the book in the first comment.
144:09 

144:09 Ever stored prices using float in your
144:12 

144:12 database? You might be bleeding money
144:14 

144:14 literally. In 1983, the Vancouver Stock
144:17 

144:17 Exchange made what seemed to be a
144:19 

144:19 harmless decision. They used the
144:21 

144:21 floating point number to track their
144:23 

144:23 index value. Every time a stock was
144:25 

144:25 added or removed, a tiny running error
144:27 

144:27 would occur. Harmless, right? Except
144:30 

144:30 those tiny errors added up real fast.
144:33 

144:33 Within two years, their index had
144:35 

144:35 mysteriously dropped over 40%, investors
144:38 

144:38 panicked. The exchange had to reset the
144:41 

144:41 index to rebuild the trust. all because
144:43 

144:43 of a bad choice in how numbers are
144:45 

144:45 stored. In this video, we'll break down
144:48 

144:48 when to use int, decimal, or float in
144:50 

144:50 your database and why wrong choice can
144:52 

144:52 corrupt your data, hurt performance or
144:54 

144:54 cost real money. Let's get started.
144:57 

144:57 Imagine that you're working towards
144:58 

144:58 designing a database for an online store
145:00 

145:00 named the Sci-Fi Collective and have
145:03 

145:03 identified all their entities,
145:04 

145:04 attributes, and the primary keys. Your
145:07 

145:07 next step is to select the right data
145:09 

145:09 type for each column. To do this task
145:12 

145:12 right, you need to pick up some
145:14 

145:14 knowledge about data types first. And
145:16 

145:16 you will start with numeric data types
145:18 

145:18 in this video. The most commonly used
145:21 

145:21 numeric data types fall into three
145:23 

145:23 baskets. Whole numbers, floating point
145:25 

145:25 numbers, and a fixed point numbers.
145:28 

145:28 Let's look at the whole numbers first. A
145:30 

145:30 whole number is simply an integer. It
145:32 

145:32 may or may not allow negative values. If
145:35 

145:35 negative values are allowed, we call it
145:37 

145:37 a side integer. Otherwise, it is
145:39 

145:39 outside. There are four types of whole
145:42 

145:42 numbers that belong to different ranges
145:44 

145:44 like tiny int which has the smallest
145:46 

145:46 range or big int which has the biggest
145:48 

145:48 range. Each of them could be signed or
145:51 

145:51 unsigned. Int is simply the one type
145:54 

145:54 that's used most frequently and good for
145:56 

145:56 most cases when whole numbers are
145:58 

145:58 needed. I have listed the ranges of each
146:01 

146:01 type of whole numbers here side and
146:03 

146:03 outside. But don't ever try to memorize
146:05 

146:05 them because it's pointless. If you ever
146:08 

146:08 need to know the range, you could rely
146:10 

146:10 on the help from generative AI tools.
146:13 

146:13 This is a perfect use case of chat GTP
146:16 

146:16 considering how simple the question is.
146:18 

146:18 So when should you consider using whole
146:20 

146:20 numbers as a data type of some
146:21 

146:21 attributes when you have some actual
146:24 

146:24 values where possible computation or
146:26 

146:26 calculation would be performed on them
146:28 

146:28 and they can't be fractional. For
146:30 

146:30 example, here's an entity that
146:32 

146:32 represents product reviews from the
146:35 

146:35 sci-fi collective database. Its reveal
146:38 

146:38 id attribute is a surrogate key and its
146:41 

146:41 data are all auto incremented integers.
146:44 

146:44 So we can declare its data type as
146:46 

146:46 integer and place in by its right side
146:50 

146:50 in the reveal entity representation.
146:52 

146:52 For another example, the purchase table
146:55 

146:55 from the sci-fi collective database its
146:57 

146:57 purchase ID attribute is also a
146:59 

146:59 surrogate key that contains auto
147:01 

147:01 incrementing integers. Additionally, the
147:04 

147:04 product quantity attribute also only
147:06 

147:06 contains integers. As a result, you can
147:09 

147:09 declare the data types of both
147:11 

147:11 attributes as integer and place int by
147:14 

147:14 the right side of the two attributes.
147:16 

147:16 The use case of whole numbers is
147:18 

147:18 relatively straightforward. Let's move
147:20 

147:20 to floatingoint numbers. So, what are
147:23 

147:23 floatingoint numbers? There are decimals
147:26 

147:26 that can tolerate some level of
147:27 

147:27 approximation. Before we get to
147:29 

147:29 discussing what it means by that, we
147:32 

147:32 need to be sure that we are on the same
147:33 

147:33 page on terms that are used to describe
147:36 

147:36 accuracy. Precision and scale. Precision
147:40 

147:40 means the number of digits in a decimal.
147:43 

147:43 Scale means the number of digits after
147:45 

147:45 the decimal point. In combination, the
147:48 

147:48 two terms precision and scale are used
147:51 

147:51 to describe how accurate a decimal
147:53 

147:53 number is. Now, we are on the same page
147:55 

147:55 on terms. Let's get back to our
147:57 

147:57 discussion.
147:59 

147:59 So there are two commonly used floating
148:01 

148:01 number data types including float and a
148:03 

148:03 double. Float numbers all have a fixed
148:06 

148:06 data size as 32 bits or say four bytes.
148:10 

148:10 Doubles all have a fixed data size as 64
148:13 

148:13 bits or say 8 bytes. Because of the size
148:16 

148:16 difference they have different accuracy.
148:19 

148:19 float can guarantee about seven
148:21 

148:21 significant digits of precision while
148:24 

148:24 double can guarantee about 15 to 17
148:27 

148:27 significant digits of precision. So what
148:30 

148:30 would be a good use case to assign
148:32 

148:32 floatingoint numbers as a data type for
148:35 

148:35 an attribute?
148:37 

148:37 When you have an attribute that stores
148:38 

148:38 decimal data that can tolerate some
148:41 

148:41 level of inaccuracy such as sensor
148:43 

148:43 reading or scientific data. For example,
148:46 

148:46 think about when you are designing a
148:48 

148:48 database for a GPS tracker application
148:51 

148:51 and there's a table named the user
148:53 

148:53 location in the database. This table
148:55 

148:55 needs to store user's current latitude,
148:58 

148:58 longitude, and altitude to pinpoint
149:00 

149:00 user's location. Such data are decimals
149:04 

149:04 and all have about seven digits after
149:06 

149:06 the decimal point. Can such data
149:09 

149:09 tolerate some level of inaccuracies?
149:11 

149:11 Yes. Would the float be a good fit? No
149:15 

149:15 float can only guarantee the precision
149:17 

149:17 of about seven significant digits of
149:20 

149:20 precision. A piece of latitude data has
149:23 

149:23 over seven digits alone after its
149:25 

149:25 decimal point. If you make its data type
149:28 

149:28 as float, your database can have a
149:31 

149:31 location error of over 10 m making the
149:34 

149:34 application useless for navigation or
149:36 

149:36 tracking. To account for the required
149:38 

149:38 accuracy, you have to use double. Double
149:42 

149:42 can guarantee about 15 to 17 significant
149:45 

149:45 digits of precision and it's good enough
149:47 

149:47 for GPS data.
149:50 

149:50 Now you know when you should consider
149:52 

149:52 floatingoint numbers as data types and
149:54 

149:54 how to decide between float and double.
149:57 

149:57 Let's move to talk about when you
149:59 

149:59 shouldn't consider floatingoint numbers.
150:02 

150:02 Remember the Vancouver stock exchange?
150:05 

150:05 Their choice of using floatingoint
150:07 

150:07 numbers to track their index value can't
150:09 

150:09 guarantee the accuracy that's required
150:12 

150:12 which leads to the accumulation of
150:14 

150:14 errors and is a big disaster. As another
150:16 

150:16 example, if you use floatingoint numbers
150:19 

150:19 to handle currency exchange, it will
150:21 

150:21 lead to someone getting more or less
150:23 

150:23 than they should because of the running
150:26 

150:26 errors. So whenever your data demands
150:29 

150:29 full accuracy, you shouldn't consider
150:31 

150:31 floatingoint numbers at all. Such a job
150:34 

150:34 is for a different data type known as
150:37 

150:37 fixed point numbers. In short, fixed
150:40 

150:40 point numbers are exact decimals for
150:43 

150:43 precise values that require full
150:45 

150:45 accuracy. Remember the terms precision
150:48 

150:48 and scale. They are used to define fixed
150:51 

150:51 point number data types. The example
150:54 

150:54 that you see here on the screen, I mean
150:56 

150:56 decimal 6, comma 4 defines the data type
150:60 

150:60 for an attribute. Its precision is six
151:03 

151:03 and a scale is four and it guarantees
151:06 

151:06 exact accuracy for numbers with the same
151:09 

151:09 precision and a scale. To define the
151:12 

151:12 data type of an attribute as fixed point
151:15 

151:15 numbers or say decimal, you need to
151:18 

151:18 specify the precision and a scale. The
151:21 

151:21 keyword numeric and a decimal are
151:23 

151:23 basically the same and can be used
151:25 

151:25 interchangeably.
151:28 

151:28 So when do you need to use decimal as a
151:31 

151:31 data type for some columns?
151:33 

151:33 When you need to store numbers exactly
151:36 

151:36 as you define them down to the right
151:38 

151:38 numbers of decimal places. For example,
151:42 

151:42 the product table in the sci-fi
151:44 

151:44 collective database has two attributes
151:46 

151:46 that deal with money, the price and cost
151:49 

151:49 attributes. and they require full
151:52 

151:52 accuracy. If all products are sold at a
151:55 

151:55 price lower than $100,000, you can
151:58 

151:58 safely declare them as decimal whose
151:60 

152:00 precision is seven and a scale is two.
152:03 

152:03 Now you know when fixed pointed numbers
152:05 

152:05 should be considered. As curious as you
152:07 

152:07 are, you may wonder when double is a
152:10 

152:10 good fit as a data type, whether decimal
152:12 

152:12 could do the job as well. Because if you
152:15 

152:15 can use decimal, you can make it as
152:17 

152:17 simple. Everything would be decimal,
152:19 

152:19 right? Well, let's revisit the database
152:22 

152:22 supporting a GPS tracker application.
152:24 

152:24 Can we declare their data type all
152:27 

152:27 decimal 108 aiming for the attributes
152:30 

152:30 longitude, altitude, and latitude? Well,
152:33 

152:33 108 may have the exact accuracy that is
152:36 

152:36 needed. However, when double is good
152:39 

152:39 enough, you shouldn't really think about
152:41 

152:41 decimal. A double data type would mean
152:44 

152:44 that all the data in that corresponding
152:46 

152:46 column will have uniform length and
152:49 

152:49 would run much faster than data in a
152:51 

152:51 column whose data type is decimal. Such
152:54 

152:54 a decision has nothing to do with
152:55 

152:55 whether decimal would work or not. It
152:58 

152:58 will work but it won't be as efficient
153:00 

153:00 as double. In this video we covered the
153:04 

153:04 common numeric data types and discussed
153:06 

153:06 when they should be used as a data type
153:08 

153:08 for different attributes. If you find
153:10 

153:10 this video helpful, give it a thumbs up,
153:13 

153:13 subscribe, and leave your questions in
153:15 

153:15 the comments. I would love to hear
153:17 

153:17 what's working for you and what needs to
153:19 

153:19 change. This video is based on the book
153:21 

153:21 Groing Relational Database Design, which
153:23 

153:23 dives deeper into database design using
153:25 

153:25 real world examples to explain the
153:28 

153:28 concepts. You'll find the link to the
153:30 

153:30 book in the first comment. In 2017,
153:33 

153:33 United Airline canled a sex and delayed
153:36 

153:36 hundreds of flights. Not because of
153:38 

153:38 weather, not because of a strike. Their
153:41 

153:41 systems couldn't agree on what a time it
153:43 

153:43 was. One stored time stamps in
153:45 

153:45 coordinated universal time, another in
153:47 

153:47 local time, and a chaos followed. Poor
153:50 

153:50 handling of temporal data isn't a small
153:52 

153:52 bug. It can shut down airlines, break
153:54 

153:54 systems, and cost millions. In this
153:57 

153:57 video, I'll show you how to use
153:59 

153:59 different temporal data types the right
154:01 

154:01 way in your database design so your
154:03 

154:03 systems don't go off schedule. The most
154:06 

154:06 commonly used temporal data types
154:08 

154:08 include a date, time, date, time, and a
154:10 

154:10 time stamp. Date simply is a data type
154:13 

154:13 representing calendar dates. A data
154:16 

154:16 point of this type can simply be 2025
154:19 

154:19 during nice.
154:21 

154:21 Time as a data type represents the time
154:23 

154:23 of a day. A data sample of this type
154:26 

154:26 includes the hour, minute and a second.
154:29 

154:29 Date time as another data type can be
154:31 

154:31 thought as the combination of date and a
154:34 

154:34 time because it contains both the
154:36 

154:36 calendar date and the time of a day all
154:39 

154:39 in your local time. Time stamp is
154:41 

154:41 similar to date time but the date and a
154:44 

154:44 time are tied to a specific time zone
154:47 

154:47 like Pacific time zone or Easter time
154:50 

154:50 zone. That's an overview of all the
154:53 

154:53 different commonly used temporal data
154:55 

154:55 types. Now, let's look at each of them
154:57 

154:57 closely. Starting with date. As you just
155:01 

155:01 saw from the overview, date is a data
155:04 

155:04 type representing calendar dates. Just a
155:06 

155:06 year, month, and a date without the time
155:09 

155:09 of a day. When should you consider using
155:12 

155:12 date as a data type for a column? when
155:16 

155:16 you only need the calendar dates instead
155:18 

155:18 of the time of a day. For example, think
155:21 

155:21 about the database of a library that
155:24 

155:24 loans books to people. Typically, only
155:26 

155:26 the information of dates are tracked by
155:29 

155:29 such a database and the system it
155:31 

155:31 supports. So, date as a data type would
155:34 

155:34 be good enough for attributes such as
155:37 

155:37 checkout date and return date. That's
155:40 

155:40 date.
155:41 

155:41 The next commonly used temporal data
155:43 

155:43 type is time which represents a time of
155:46 

155:46 a day without calendar data information.
155:50 

155:50 When should you consider time as a data
155:52 

155:52 type for an attribute? When you only
155:54 

155:54 need the time of a day instead of the
155:57 

155:57 date, for example, think about the
155:59 

155:59 database that supports a course
156:01 

156:01 scheduling at a university. The start
156:04 

156:04 and end time of a class doesn't need to
156:07 

156:07 record the calendar date information.
156:09 

156:09 Such attributes are typically assigned a
156:11 

156:11 time data type. The date time as a data
156:15 

156:15 type combines both the calendar date and
156:17 

156:17 the time of a day and it's bound by your
156:20 

156:20 local time zone. When should you
156:23 

156:23 consider date time as a data type for a
156:25 

156:25 column when you need both date and a
156:28 

156:28 time but don't need to deal with the
156:30 

156:30 conversion of time between different
156:32 

156:32 time zones. For example, the database of
156:35 

156:35 hospitals typically use data time as a
156:37 

156:37 data type for appointment time like this
156:41 

156:41 because medical records shouldn't change
156:43 

156:43 based on the server time zone settings.
156:45 

156:45 More importantly, you might need to
156:48 

156:48 store very old appointments before 1970
156:51 

156:51 or for future appointments. In
156:54 

156:54 comparison, the range of datetime is
156:57 

156:57 much bigger than time stamp. Date time
156:59 

156:59 goes from year 1 to one short of 10,000.
157:03 

157:03 Whereas time stamp only goes from 1970
157:07 

157:07 to 2038.
157:09 

157:09 Judging by the range, you might think
157:11 

157:11 that time stamp is much less powerful,
157:14 

157:14 but its power doesn't lie in the range.
157:17 

157:17 The power of time stamp lies in
157:19 

157:19 automatic time zone handling. A data
157:22 

157:22 point of time stamp is tied to a
157:24 

157:24 specific time zone or the coordinated
157:27 

157:27 universal time. Timestamp data can be
157:30 

157:30 converted from one time zone to another,
157:32 

157:32 which is very useful when you need to
157:35 

157:35 support applications used by people
157:37 

157:37 across the world.
157:39 

157:39 Speaking of that, when should you
157:41 

157:41 consider time stamp as a data type for a
157:44 

157:44 column? When you need an accurate time
157:46 

157:46 and your application needs to work
157:48 

157:48 across different time zones or servers,
157:51 

157:51 then you need the help from timestamp.
157:54 

157:54 For example, think about the table
157:56 

157:56 storing the purchase records from a
157:58 

157:58 database that powers an online store.
158:01 

158:01 The store sells internationally, which
158:03 

158:03 means that its users come from different
158:05 

158:05 time zones. To make the purchase time of
158:08 

158:08 such users consistent, you can assign
158:11 

158:11 timestamp as the data type for the
158:13 

158:13 purchase time attribute. That's
158:15 

158:15 everything about time stamp data type.
158:18 

158:18 In this video, we covered the common
158:21 

158:21 temporal data types and discussed when
158:23 

158:23 they should be used in database design.
158:26 

158:26 If you find this video helpful, gave it
158:28 

158:28 a thumb up, subscribe, and leave your
158:31 

158:31 questions in the comments. I would love
158:33 

158:33 to hear what's working for you and what
158:35 

158:35 needs to change. This video is based on
158:37 

158:37 the book Groing Relational Database
158:39 

158:39 Design, which dives deeper into database
158:42 

158:42 design using real world examples to
158:44 

158:44 explain the concepts. You'll find the
158:47 

158:47 link to the book in the first comment.
158:49 

158:49 Have you ever stared at a bunch of
158:51 

158:51 entities and wonder even how to start
158:53 

158:53 connecting them so that you can have an
158:56 

158:56 ER diagram?
158:57 

158:57 In this video, I'll show you exactly how
159:00 

159:00 to start building relationships the
159:02 

159:02 right way, step by step, and clear up
159:05 

159:05 the confusion between different
159:07 

159:07 notations. We will start by providing an
159:09 

159:09 accurate definition for the ultimate
159:11 

159:11 goal of database design, a complete
159:14 

159:14 entity relationship diagram. So, an
159:17 

159:17 entity relationship diagram, short for
159:20 

159:20 ER diagram, is a visual tool that's used
159:23 

159:23 to design databases. It shows how
159:26 

159:26 different pieces of information relate
159:28 

159:28 to each other. What information?
159:31 

159:31 Like entities, attributes, and
159:33 

159:33 relationships.
159:35 

159:35 Entities are the things you want to
159:37 

159:37 store information about like customers,
159:40 

159:40 products, or orders.
159:42 

159:42 These appear as rectangular boxes in the
159:46 

159:46 diagram. Attributes are the specific
159:48 

159:48 details about each entity like a
159:50 

159:50 customer's name, email, and a phone
159:53 

159:53 number. Attributes are represented in
159:55 

159:55 the rectangular boxes of entities as
159:59 

159:59 well as their corresponding data types.
160:02 

160:02 Relationships show how entities connect
160:05 

160:05 to each other like customers place
160:08 

160:08 orders or orders contain products.
160:11 

160:11 Relationships appear either as diamonds
160:14 

160:14 or lines connecting entities depending
160:17 

160:17 on which notation you are using. A
160:20 

160:20 complete er diagram may look like this.
160:23 

160:23 Well, you can think of it as mapping out
160:26 

160:26 all the important pieces of information
160:28 

160:28 and connecting them. For example, in
160:32 

160:32 online stores database, it would have
160:35 

160:35 customers who place orders and those
160:37 

160:37 orders contain products. The ER diagram
160:41 

160:41 would show these connections visually
160:43 

160:43 before you actually build the database.
160:45 

160:45 The main benefit is that it helps you
160:48 

160:48 plan out your database structure clearly
160:51 

160:51 before you start building it. Making
160:53 

160:53 sure that you don't forget important
160:55 

160:55 connections or information you need to
160:57 

160:57 store. That's everything about the
161:00 

161:00 definition of er diagrams. Let's move to
161:04 

161:04 discuss the different types of
161:05 

161:05 notations. There are two very popular
161:08 

161:08 notations that are used to draw er
161:11 

161:11 diagrams including the chins notation
161:14 

161:14 and a crossfoot notation. So chins
161:17 

161:17 notation has its origin in a paper
161:19 

161:19 published in 1976 by Dr. Chen, hence the
161:23 

161:23 name chins notation. It's widely used by
161:26 

161:26 database text books and told across
161:29 

161:29 different higher education institutions
161:31 

161:31 across the world. For example, the
161:34 

161:34 textbook database system concepts adopts
161:37 

161:37 transnotation for all the ER diagrams
161:40 

161:40 used in this book.
161:43 

161:43 An example of an ER diagram developed
161:45 

161:45 using transotation looks like this. And
161:48 

161:48 the right side of the screen. It comes
161:51 

161:51 from the book database system concepts.
161:54 

161:54 Some notable features of chess notation
161:57 

161:57 include representing all the
161:59 

161:59 relationships between entities using
162:01 

162:01 diamond boxes and using underlines to
162:05 

162:05 denote primary keys. In comparison,
162:08 

162:08 crossoot notation has its root in a
162:11 

162:11 different paper published in the same
162:13 

162:13 year by Dr. Everest. Cross food notation
162:16 

162:16 is used more widely by the industry and
162:19 

162:19 practitioners and it's also used by some
162:23 

162:23 database design books like Grocking
162:26 

162:26 relational database design. I'm one of
162:28 

162:28 the authors of the book. An example of
162:32 

162:32 er diagram that uses crossoot notation
162:35 

162:35 looks like this and it comes from the
162:38 

162:38 book cropping relational database
162:40 

162:40 design.
162:42 

162:42 In this and all future videos of this
162:44 

162:44 series, we will stick to cross food
162:47 

162:47 notation mostly for the consideration of
162:50 

162:50 practicality. I told many students using
162:53 

162:53 trans notation in the past and the very
162:56 

162:56 valuable feedback that I got for many
162:58 

162:58 times is that the gap between using
163:01 

163:01 transotation for coursework and using
163:04 

163:04 crossoot notation during work is painful
163:07 

163:07 to conquer. Cross food notation does
163:10 

163:10 have some advantages in comparison such
163:13 

163:13 as cross food notation is more visually
163:16 

163:16 clear and efficient without the diameter
163:20 

163:20 box representing relationships where the
163:23 

163:23 foreign key goes is more
163:25 

163:25 self-explanatory.
163:26 

163:26 Beyond that most popular database design
163:29 

163:29 tools default to crossoot and a
163:32 

163:32 crossfoot notation is widely used in
163:35 

163:35 business and consulting environments.
163:39 

163:39 Okay, that's everything about the
163:41 

163:41 notation types. Let's move to explore a
163:44 

163:44 bit about how you start connecting
163:46 

163:46 related entities.
163:48 

163:48 Imagine that you are working towards
163:50 

163:50 designing a database for an online store
163:52 

163:52 named as a sci-fi collective and have
163:55 

163:55 identified all the entities attributes
163:57 

163:57 and as a primary keys. Beyond the
163:59 

163:59 entities attributes, you have also
164:02 

164:02 extracted a set of relationships from
164:05 

164:05 your requirement analysis. The next
164:08 

164:08 immediate task you will face is to
164:11 

164:11 connect the entities using lines based
164:13 

164:13 on the summarized relationships.
164:16 

164:16 This step doesn't involve attributes, at
164:19 

164:19 least not yet. So we can do something to
164:23 

164:23 simplify our representations of entities
164:26 

164:26 by hiding the attributes for now. So we
164:30 

164:30 only have six entities representing
164:32 

164:32 user, product, product review, purchase
164:36 

164:36 records of users and payment methods.
164:39 

164:39 Additionally,
164:41 

164:41 we can make the relationship summary
164:44 

164:44 even more succinct.
164:46 

164:46 The nouns in every sentence represents
164:49 

164:49 entities. If two nouns connected by some
164:53 

164:53 verbs show up in a one sentence, then
164:57 

164:57 the two entities are likely to be in a
164:59 

164:59 relationship. You may go through a few
165:02 

165:02 iterations of trial and error when
165:04 

165:04 mapping the summary to a diagram because
165:08 

165:08 of possible inaccuracies and
165:10 

165:10 misinterpretation.
165:12 

165:12 Based on the existing information, we
165:15 

165:15 can connect every two related entities
165:17 

165:17 like this. When you generate a draft
165:21 

165:21 diagram, you should test every
165:23 

165:23 relationship against the information you
165:25 

165:25 gathered and the sample data you
165:27 

165:27 collected. Also, take the draft diagram
165:31 

165:31 to the stakeholders and explain your
165:33 

165:33 design ration to them because it's
165:36 

165:36 likely that you made some mistakes or
165:39 

165:39 neglected something critical in your
165:41 

165:41 first iteration.
165:43 

165:43 For example, the software developers of
165:46 

165:46 the sci-fi collective will point out
165:48 

165:48 that an online purchase can't be
165:51 

165:51 performed without a payment method.
165:54 

165:54 Based on the new information, you need
165:56 

165:56 to answer this question shown up here on
165:59 

165:59 the screen first before visiting the
166:01 

166:01 draft diagram again. Without the payment
166:05 

166:05 method information, an online order
166:07 

166:07 can't be finished. It is the online
166:10 

166:10 store can't bill its customer. In other
166:13 

166:13 words, each purchase record needs to be
166:16 

166:16 mapped to a corresponding payment
166:18 

166:18 method. As a result, a relationship
166:22 

166:22 between payment method and the purchase
166:24 

166:24 makes sense. So, you can add a line
166:27 

166:27 between payment method and a purchase to
166:30 

166:30 make the two entities related to each
166:32 

166:32 other. This is how you start building
166:35 

166:35 relationship s entities.
166:37 

166:37 From here the next step is to figure out
166:40 

166:40 the cardality of each identified
166:43 

166:43 relationship. Cardinality describes the
166:46 

166:46 number of rows of one table that can be
166:50 

166:50 associated with a single row of another
166:52 

166:52 table. That's a big topic that we will
166:55 

166:55 cover in the next few videos.
166:58 

166:58 In this video, we defined ER diagram,
167:02 

167:02 covered two different types of diagram
167:04 

167:04 notations and get started on building ER
167:08 

167:08 diagrams by connecting entities based on
167:10 

167:10 the requirement analysis. If you find
167:13 

167:13 this video helpful, give it a thumbs up,
167:16 

167:16 subscribe, and leave your questions in
167:18 

167:18 the comments. I would love to hear
167:20 

167:20 what's working for you and what needs to
167:22 

167:22 change. This video is based on the book
167:25 

167:25 graing relational database design which
167:27 

167:27 dives deep into database design using
167:30 

167:30 real world examples to explain the
167:32 

167:32 concepts. You can find the link to the
167:35 

167:35 book in the first comment. Can one
167:38 

167:38 person have multiple passports or can
167:41 

167:41 one passport belongs to multiple people?
167:45 

167:45 The answer lies in the concept called
167:47 

167:47 cardality. In this video, we'll
167:50 

167:50 introduce you to this concept and cover
167:52 

167:52 the simplest kind of cardality which is
167:55 

167:55 a onetoone relationship.
167:59 

167:59 Cardinality is an important
168:01 

168:01 characteristic of a relationship between
168:03 

168:03 two entities describing the number of
168:06 

168:06 instances of one entity that can be
168:09 

168:09 associated with a single instance of
168:12 

168:12 another entity via the relationship. In
168:15 

168:15 case if you feel lost about the
168:17 

168:17 terminologies,
168:19 

168:19 the entity is essentially the same as a
168:21 

168:21 table. An entity is a conceptual idea of
168:24 

168:24 a real world object or concept. A table
168:28 

168:28 is the implementation of an entity in a
168:31 

168:31 database.
168:33 

168:33 An instance is essentially the same as a
168:36 

168:36 row. An instance is a conceptual idea.
168:39 

168:39 When being implemented, it will be a row
168:42 

168:42 in its corresponding table. To complete
168:45 

168:45 your ER diagram, you need to analyze the
168:48 

168:48 information you have collected from the
168:51 

168:51 requirement gathering phase. Identify
168:53 

168:53 the cardality of each relationship and
168:56 

168:56 update the draft diagram accordingly.
168:59 

168:59 For example, without cardalities, your
169:02 

169:02 ER diagram in its early stages may look
169:05 

169:05 like this.
169:08 

169:08 There are two things we need to explain
169:10 

169:10 on cardalities including direction and
169:13 

169:13 representation.
169:14 

169:14 Let's start with an example. Think about
169:18 

169:18 the database of a banking system. There
169:20 

169:20 are two entities representing users and
169:23 

169:23 accounts. So you have user and account
169:26 

169:26 as the two different entities and their
169:29 

169:29 relationship can be broken down to two
169:32 

169:32 directional relationships summarized
169:34 

169:34 using two sentences as is shown here on
169:37 

169:37 the screen. Sentence one says that a
169:40 

169:40 user has zero one or more accounts.
169:43 

169:43 Sentence two says that an account is
169:46 

169:46 associated with one and only one user.
169:50 

169:50 Sentence one and sentence two represent
169:53 

169:53 two different directional relationships
169:55 

169:55 between user and account. In sentence
169:59 

169:59 one, the direction flows from user to
170:02 

170:02 account. In sentence two, the direction
170:05 

170:05 flows from account to user. In short, a
170:09 

170:09 sentence that describes a directional
170:11 

170:11 relationship from one entity to another
170:14 

170:14 has a subject, a verb, and object. The
170:18 

170:18 direction always flows from the subject
170:21 

170:21 to the object. Now you know what
170:24 

170:24 direction means. Let's talk about the
170:27 

170:27 representation.
170:29 

170:29 Given a directional relationship from
170:31 

170:31 entity A to entity B, cardality
170:34 

170:34 describes how many instances of B which
170:37 

170:37 a single instance of A can and has to be
170:41 

170:41 associated with. Cardinality is
170:44 

170:44 represented by two graphical symbols on
170:46 

170:46 the relationship line between A and B.
170:49 

170:49 The symbols are used in pairs to
170:52 

170:52 represent the cardalities. The symbol on
170:54 

170:54 the inner side represents the minimum
170:58 

170:58 instance number of B that a single
171:00 

171:00 instance of A needs to be associated
171:03 

171:03 with. We call that the minimum
171:06 

171:06 cardality. The symbol on the outer side
171:09 

171:09 represents the maximum instance number
171:11 

171:11 of B that a single instance of A can be
171:15 

171:15 associated with. We call that the
171:17 

171:17 maximum cardality. Both symbols are
171:21 

171:21 placed closer to entity B. See the
171:24 

171:24 direction goes from A to B. Yet the two
171:28 

171:28 symbols need to be placed closer to B.
171:31 

171:31 That's something you want to pay
171:33 

171:33 attention to.
171:35 

171:35 Just so you know, you can swap the
171:38 

171:38 positions of entities A and B and that
171:41 

171:41 will lead to the positions of main and
171:44 

171:44 max cardalities swapped as well. As you
171:47 

171:47 can see, when the direction flows from A
171:50 

171:50 to B, the two symbols as a combination
171:53 

171:53 are always placed closer to B.
171:57 

171:57 Individually, the mean cardality is the
172:00 

172:00 symbol that's placed closer to A
172:03 

172:03 relatively while the maximum cardality
172:06 

172:06 is always placed closer to B relatively.
172:10 

172:10 Beyond that, you saw some symbols on the
172:12 

172:12 screen that requires explanation. The
172:15 

172:15 individual symbols like a bar or a
172:18 

172:18 crossoot all represent quantities.
172:22 

172:22 So quantity zero is represented by a
172:25 

172:25 circle. One is represented by a bar and
172:30 

172:30 many is represented by this symbol that
172:33 

172:33 looks like a crossfoot.
172:36 

172:36 In the example on the left side, the
172:38 

172:38 main cardality of the relationship from
172:41 

172:41 A to B is one and the max cardality is
172:45 

172:45 many.
172:47 

172:47 Now you know the basics about the
172:49 

172:49 cardality. We can revisit the example
172:53 

172:53 that you saw earlier. The relationship
172:55 

172:55 between user and our account entities in
172:58 

172:58 the database of a banking system. The
173:02 

173:02 relationship between the two entities is
173:04 

173:04 summarized as two different directional
173:06 

173:06 relationships.
173:08 

173:08 On one hand, a user has zero, one or
173:12 

173:12 many accounts. On the other hand, an
173:14 

173:14 account is associated with one and only
173:18 

173:18 one user.
173:20 

173:20 The first sentence defines the
173:22 

173:22 relationship from user to account. Since
173:26 

173:26 a user can be associated with as few as
173:29 

173:29 zero and as many as unlimited number of
173:32 

173:32 accounts, you can tell that the mean
173:34 

173:34 cardality is zero and the max cardality
173:37 

173:37 is many.
173:40 

173:40 The second sentence defines the
173:42 

173:42 relationship from account to user. Since
173:46 

173:46 an account is associated with as few as
173:49 

173:49 one and as many as one user, both the
173:52 

173:52 min and max cardalities from account to
173:55 

173:55 user are ones.
173:58 

173:58 In the end, you are supposed to merge
174:01 

174:01 the two directional relationships as one
174:04 

174:04 relationship between user and account
174:06 

174:06 entities. After they merge, the
174:09 

174:09 positions of the two min and max
174:11 

174:11 cardalities of the two directional
174:13 

174:13 relationships would be exactly the same
174:16 

174:16 as if you draw them separately.
174:19 

174:19 As you can see here from the
174:21 

174:21 representation on the screen, the max
174:23 

174:23 cardalities are more closer to the
174:26 

174:26 entities. In comparison, the main
174:29 

174:29 cardalities stay further from the two
174:32 

174:32 entities. That's everything about the
174:34 

174:34 two aspects of cardality. the direction
174:37 

174:37 and the representation. With such
174:40 

174:40 knowledge, we can put all the
174:42 

174:42 relationships you will see in the future
174:44 

174:44 into three baskets based on their
174:46 

174:46 cardalities, including onetoone
174:49 

174:49 relationships, one to many
174:50 

174:50 relationships, and many to many
174:52 

174:52 relationships. Such a classification is
174:55 

174:55 all based on the max cardalities.
174:59 

174:59 We will look closely at the onetoone
175:01 

175:01 relationships in this video. In a
175:04 

175:04 onetoone relationship, each instance or
175:07 

175:07 say each row in one entity is related up
175:11 

175:11 to one instance or say one row in the
175:14 

175:14 other entity.
175:17 

175:17 One to one I mean the two ones here
175:21 

175:21 refer primarily to the max cardalities
175:24 

175:24 of both directional relationships.
175:27 

175:27 The main cardalities could either be
175:29 

175:29 zero or one. Generally speaking,
175:32 

175:32 onetoone relationships are rare. But
175:35 

175:35 let's see at least one example to make
175:38 

175:38 sure that we are on the same page in
175:40 

175:40 terms of this relationship.
175:43 

175:43 Imagine that you are working towards
175:45 

175:45 designing a database for an online store
175:47 

175:47 named the sci-fi collective and have
175:50 

175:50 identified two entities user and a user
175:53 

175:53 address. The extracted relationship
175:56 

175:56 between the two entities can be
175:58 

175:58 summarized as following. A user may not
176:02 

176:02 have an address when they first register
176:04 

176:04 an account, but a user must have one and
176:08 

176:08 only one address before making a
176:10 

176:10 purchase.
176:12 

176:12 An address is associated with only a
176:15 

176:15 single user. Let's go from here and
176:18 

176:18 develop the two directional
176:20 

176:20 relationships one after another.
176:23 

176:23 So a user may have zero to one address.
176:27 

176:27 Based on this we can develop the
176:29 

176:29 directional relationship from user to
176:32 

176:32 user address. The main cardality is zero
176:36 

176:36 and max cardality is one. So you can add
176:40 

176:40 the two symbols accordingly. Since the
176:44 

176:44 direction goes from user to user
176:46 

176:46 address, both symbols should be placed
176:49 

176:49 closer to user address entity. In
176:52 

176:52 contrast, an address is only associated
176:55 

176:55 with a single user. which means that
176:58 

176:58 both the main NMX cardalities are one.
177:01 

177:01 The relationship goes from user address
177:04 

177:04 to user. So you will place a pair of two
177:07 

177:07 bars closer to user. After that you will
177:11 

177:11 merge the two directional relationships
177:13 

177:13 into one. This step is relatively easy
177:16 

177:16 to do because all the cardality symbols
177:19 

177:19 stay where they are in the directional
177:21 

177:21 relationship representation. They just
177:24 

177:24 share the same line between user and
177:26 

177:26 user address entities. In case if you
177:29 

177:29 think we are finished here, we aren't.
177:32 

177:32 There is one more step which is to
177:34 

177:34 restore the representations of entities.
177:38 

177:38 So far we have used a simplified
177:40 

177:40 representation of the user and user
177:43 

177:43 address entities because we didn't list
177:45 

177:45 their attributes. We need to do so in a
177:49 

177:49 fully developed ER diagram. As you do
177:52 

177:52 that, you also need to place a foreign
177:55 

177:55 key in one of the two entities. And
177:58 

177:58 that's the only way for the entities to
178:00 

178:00 know that they are in a relationship
178:03 

178:03 when they are implemented as tables.
178:07 

178:07 In a onetoone relationship, the foreign
178:09 

178:09 key is placed in the entity that's close
178:12 

178:12 to where the zero main cardality is. You
178:16 

178:16 may also see other sources call this
178:18 

178:18 entity the optional site and the entity
178:22 

178:22 close to the main cardality of one as
178:25 

178:25 the required site. The foreign key is
178:28 

178:28 always placed in the optional site. This
178:31 

178:31 choice supports the data entry order and
178:34 

178:34 makes sure that you won't end up with a
178:36 

178:36 null value for the foreign key. If you
178:39 

178:39 ever try to place a foreign key in the
178:41 

178:41 required site, its value will need to be
178:44 

178:44 null from time to time because a user is
178:47 

178:47 not necessarily associated with any
178:50 

178:50 addresses,
178:51 

178:51 especially those users who just register
178:54 

178:54 new accounts. In other words, when you
178:57 

178:57 follow the order of data entry to add a
178:59 

178:59 new user first, there's no existing
179:02 

179:02 address ID value that can be referenced
179:05 

179:05 at all. No values can create unnecessary
179:09 

179:09 problems for your database.
179:11 

179:11 As a result, you should always place the
179:14 

179:14 foreign key in the optional side.
179:17 

179:17 However, as curious as you are, you may
179:20 

179:20 wonder what if both mean cardalities of
179:22 

179:22 the two directional relationships are
179:25 

179:25 zeros or ones. And what should you do in
179:28 

179:28 those cases?
179:30 

179:30 If both mean cardalities are zeros, both
179:34 

179:34 sides are optional.
179:36 

179:36 And you can place the foreign key in
179:38 

179:38 either of the two entities like this,
179:41 

179:41 what you see here on the screen or this.
179:46 

179:46 It's worth noting that when you have
179:48 

179:48 such a relationship, the foreign key
179:50 

179:50 values have to be null from time to
179:52 

179:52 time. As you can see from this example,
179:56 

179:56 a user is not necessarily associated
179:58 

179:58 with an address and an address is not
180:02 

180:02 necessarily associated with a user. If
180:05 

180:05 you have such a relationship, you need
180:07 

180:07 to be prepared to deal with null values
180:10 

180:10 as when both mean cardalities are zero.
180:13 

180:13 What about when both mean cardalities
180:15 

180:15 are once? Well, two main cardalities as
180:19 

180:19 ones are good theoretically, but not in
180:22 

180:22 practice. If you ever have a onetoone
180:25 

180:25 relationship whose main cardalities are
180:28 

180:28 both ones, you need to relax one of them
180:31 

180:31 from one to zero for the sake of
180:33 

180:33 implementation. Let's use an example to
180:36 

180:36 demonstrate this point. Think about the
180:39 

180:39 relationship between two entities,
180:41 

180:41 department and a manager. A department
180:44 

180:44 has one and only one manager. Well, a
180:47 

180:47 manager works for one and only one
180:50 

180:50 department. This is a classical onetoone
180:52 

180:52 relationship. Both the max and min
180:55 

180:55 cardalities of both directional
180:57 

180:57 relationships are one. Thus
180:60 

180:60 representation is theoretically solid
181:02 

181:02 but impossible to implement. When it
181:05 

181:05 comes to data entry, which table should
181:08 

181:08 you start with? A department? A
181:11 

181:11 department needs to be associated with
181:13 

181:13 an existing manager or manager. A
181:16 

181:16 manager needs to be associated with an
181:19 

181:19 existing department.
181:20 

181:20 In other words, this design asks you to
181:24 

181:24 essentially place foreign keys in both
181:26 

181:26 sides or say both tables. Such a design
181:29 

181:29 is impossible for data entry. If you
181:32 

181:32 enter a row into the department table
181:35 

181:35 first, there won't be any manager ID
181:38 

181:38 values that you can reference. The
181:41 

181:41 foreign key constraint that you put in
181:43 

181:43 place will cause SQL to complain and
181:45 

181:45 reject the data entry. If you switch the
181:48 

181:48 order of data entry and try to enter a
181:51 

181:51 manager information first, you will face
181:54 

181:54 the same problem. To avoid such an
181:56 

181:56 issue, you can relax one of the two main
181:59 

181:59 cardalities from one to zero to make
182:02 

182:02 implementation possible. For example,
182:06 

182:06 you can modify the relationship between
182:08 

182:08 department and a manager like this. The
182:11 

182:11 first sentence will be revised from a
182:13 

182:13 department has one and only one manager
182:16 

182:16 to a department has zero or one manager.
182:21 

182:21 Based on the revision, you can relax the
182:24 

182:24 main cardality from department to
182:27 

182:27 manager from one to zero. After that,
182:32 

182:32 you have an optional side again and you
182:35 

182:35 can place a foreign key in the optional
182:37 

182:37 side. If the two tables are ever
182:39 

182:39 represented in a er diagram, the proper
182:43 

182:43 representation will look like this.
182:46 

182:46 That's everything about the onetoone
182:48 

182:48 relationship.
182:50 

182:50 In this video, we covered the basics of
182:53 

182:53 cardality and explored onetoone
182:55 

182:55 relationships. If you find this video
182:57 

182:57 helpful, give it a thumb up, subscribe,
182:60 

182:60 and leave your questions in the
183:02 

183:02 comments. I would love to hear what's
183:03 

183:03 working for you and what needs to
183:05 

183:05 change. This video is based on the book
183:08 

183:08 graining relational database design
183:10 

183:10 which dives deeper into database design
183:13 

183:13 using real world examples to explain the
183:15 

183:15 concepts. You can find the link to the
183:18 

183:18 book in the first comment. A customer
183:20 

183:20 may place many orders. A teacher may
183:23 

183:23 teach many students. But how do we
183:25 

183:25 represent these relationship in database
183:28 

183:28 design? It all starts with understanding
183:30 

183:30 one to many relationships. In this
183:32 

183:32 video, we'll introduce you to this
183:35 

183:35 concept and explores how to identify and
183:38 

183:38 represent one to many relationships in
183:40 

183:40 database design. Before we jump to the
183:42 

183:42 one to many relationships, I need to
183:45 

183:45 refresh your memory on cardality first.
183:48 

183:48 The understanding of cardality is
183:50 

183:50 important to understand one to many
183:52 

183:52 relationships. But if you are versed in
183:55 

183:55 this concept, you can skip this chapter
183:58 

183:58 and jump to the one to many
183:60 

183:60 relationships.
184:01 

184:01 Cardinality describes the number of
184:04 

184:04 instances of one entity that can be
184:06 

184:06 associated with a single instance of
184:09 

184:09 another entity via the relationship.
184:11 

184:11 There are two key points on cardality.
184:14 

184:14 First, a relationship between two
184:16 

184:16 entities can be broken down to two
184:19 

184:19 different directional relationships when
184:21 

184:21 you look at it from the lens of
184:23 

184:23 cardality. For example, think about the
184:27 

184:27 relationship between user and account in
184:30 

184:30 a database supporting a banking system.
184:33 

184:33 This relationship between the two
184:35 

184:35 entities are summarized in the two
184:38 

184:38 sentences on the screen. A user has
184:41 

184:41 zero, one or more accounts. An account
184:45 

184:45 is associated with one and only one
184:48 

184:48 user. The two sentences represent the
184:51 

184:51 two different directional relationships.
184:54 

184:54 Sentence one is about the relationship
184:57 

184:57 from user to account. Well, sentence two
185:01 

185:01 is about the relationship from account
185:04 

185:04 to user.
185:06 

185:06 Every relationship can be broken down
185:08 

185:08 into two different directional
185:11 

185:11 relationships like this. That's the
185:13 

185:13 first point. Second, a directional
185:16 

185:16 relationship is represented by two
185:18 

185:18 symbols of max and min cardalities.
185:23 

185:23 The three individual symbols represents
185:25 

185:25 three quantities including zero, one and
185:28 

185:28 many. The circle represents zero. The
185:31 

185:31 bar represents one and a symbol that
185:34 

185:34 looks like a crossfoot represents many.
185:37 

185:37 Each directional relationship is
185:40 

185:40 represented by two symbols as a pair.
185:42 

185:42 For example, the first sentence you saw
185:45 

185:45 earlier is on the directional
185:47 

185:47 relationship from user to account. The
185:51 

185:51 two symbols representing cardalities
185:54 

185:54 need to be placed near the account
185:57 

185:57 entity. A user can be associated with
185:60 

185:60 zero accounts. So the main cardality is
186:02 

186:02 zero. A user can be associated with as
186:06 

186:06 many accounts as possible. So the
186:08 

186:08 cardality is many. The second sentence
186:11 

186:11 is on the directional relationship from
186:14 

186:14 account to user and its cardalities are
186:17 

186:17 placed near user entity. Both of the min
186:20 

186:20 and max cardalities are one.
186:23 

186:23 In the end, you are supposed to merge
186:26 

186:26 the two directional relationships as one
186:29 

186:29 relationship between user and account
186:31 

186:31 entities. After they merge, the
186:34 

186:34 positions of the two main and max
186:36 

186:36 cardalities would be exactly the same as
186:39 

186:39 if you draw them individually.
186:43 

186:43 From the lens of cardality, we can put
186:46 

186:46 all the relationships into three
186:48 

186:48 baskets, including one to one
186:50 

186:50 relationships, one to many relationships
186:52 

186:52 and many to many relationships.
186:55 

186:55 We will look closely at one to many
186:57 

186:57 relationships in this video. In a one to
186:60 

186:60 many relationship, one directional
187:02 

187:02 relationship has the max cardality of
187:05 

187:05 one and the other has a max cardality as
187:08 

187:08 many.
187:10 

187:10 The one and many here primarily refer to
187:13 

187:13 the max cardalities of the two
187:15 

187:15 directional relationships.
187:18 

187:18 If both main cardalities are ones in a
187:21 

187:21 given one to many relationship, it will
187:24 

187:24 create problems for data entry when they
187:27 

187:27 are implemented as tables. As you can
187:30 

187:30 see from this example on the screen,
187:32 

187:32 when it comes to data entry, the two
187:35 

187:35 main cardalities as ones essentially
187:38 

187:38 means that when you try to add a row of
187:41 

187:41 data into table A, it has to be
187:44 

187:44 associated with an existing row in table
187:46 

187:46 B. And when you try to add a row into
187:50 

187:50 table B, it has to be associated with an
187:53 

187:53 existing row in table A. The association
187:57 

187:57 translates to placing the foreign key in
187:59 

187:59 both tables which will cause SQL to
188:02 

188:02 complain. Beyond that, you will also
188:04 

188:04 have a lot of null values in the two
188:07 

188:07 foreign key columns as the minimum
188:09 

188:09 association requirement can't be met in
188:12 

188:12 real world. To avoid such issues, it's
188:16 

188:16 typical to relax the main cardality of
188:18 

188:18 the many side from one to zero. The one
188:21 

188:21 to many relationships are the most
188:23 

188:23 common relationships that are modeled in
188:26 

188:26 database design. Let's look at a few
188:28 

188:28 examples.
188:30 

188:30 Imagine that you're working towards
188:31 

188:31 designing a database for an online store
188:34 

188:34 named as a sci-fi collective and have
188:37 

188:37 identified two entities user and a
188:39 

188:39 review. The extracted relationships
188:42 

188:42 between the two entities can be
188:44 

188:44 summarized as following. A user can
188:47 

188:47 write zero to many reviews. A review can
188:51 

188:51 be written by one and only one user.
188:54 

188:54 Let's go from here and develop the two
188:56 

188:56 directional relationships one after
188:58 

188:58 another.
188:60 

189:00 A user can write zero to many reviews.
189:04 

189:04 Based on this, we can develop the
189:06 

189:06 directional relationship from user to
189:08 

189:08 review. The main cardality is zero and
189:11 

189:11 the max cardality is many. So you can
189:14 

189:14 add the two symbols accordingly. Since
189:17 

189:17 the direction goes from user to review,
189:20 

189:20 both symbols should be placed closer to
189:22 

189:22 review.
189:25 

189:25 In contrast, a review is only associated
189:29 

189:29 with a single user, which means that
189:31 

189:31 both the main and max cardalities are
189:33 

189:33 one. This relationship goes from review
189:36 

189:36 to user. So you will place the two bars
189:39 

189:39 as a pair closer to user. After that,
189:43 

189:43 you will merge the two directional
189:45 

189:45 relationships into one. This step is
189:49 

189:49 relatively easy to do because all the
189:51 

189:51 cardality symbols stay where they are in
189:54 

189:54 the directional relationship
189:56 

189:56 representation. They just share the same
189:58 

189:58 line between user and user address
190:01 

190:01 entity. There is still one more step
190:04 

190:04 which is to explicitly list all the
190:07 

190:07 attributes. As you do that, you need to
190:10 

190:10 place a foreign key in one of the two
190:12 

190:12 entities. And in one to many
190:15 

190:15 relationships, the foreign key is always
190:18 

190:18 placed in the entity that's close to the
190:21 

190:21 max cardality of many. In this example,
190:26 

190:26 that's the review entity.
190:28 

190:28 So the full representation of this
190:31 

190:31 relationship is like what you see here
190:33 

190:33 on the screen. The foreign key is in the
190:36 

190:36 many side. Email as a foreign key is
190:40 

190:40 placed in the review entity. Let's look
190:42 

190:42 at another example. For the same
190:44 

190:44 database, you have identified and
190:46 

190:46 developed two entities, user and a
190:49 

190:49 purchase.
190:51 

190:51 Their relationship is summarized as what
190:53 

190:53 you see here on the screen. A user can
190:56 

190:56 make multiple purchases. A purchase can
190:58 

190:58 be made by only one user. A user may
191:02 

191:02 make zero to many purchases. Based on
191:05 

191:05 this we can develop the directional
191:07 

191:07 relationship from user to purchase. The
191:10 

191:10 main cardality is zero and the max
191:13 

191:13 cardality is many. So you can add the
191:16 

191:16 two symbols accordingly. Since the
191:18 

191:18 direction goes from user to purchase,
191:21 

191:21 both symbols should be placed closer to
191:23 

191:23 purchase. In contrast, a purchase is
191:27 

191:27 only associated with a single user,
191:30 

191:30 which means that both the min and max
191:32 

191:32 cardalities are one. This relationship
191:35 

191:35 goes from purchase to user. So you will
191:38 

191:38 place two bars as a pair closer to user.
191:41 

191:41 After that you will merge the two
191:43 

191:43 directional relationships into one like
191:46 

191:46 this.
191:49 

191:49 Translating this relationship to their
191:51 

191:51 full representation.
191:53 

191:53 You need to make all the attributes of
191:56 

191:56 both entities explicit. Beyond that, we
191:59 

191:59 also need to add a foreign key to the
192:01 

192:01 many site which is the purchase entity
192:04 

192:04 like this.
192:06 

192:06 Let's see second example. Let's look at
192:10 

192:10 another from the same database. The
192:12 

192:12 entities payment method and purchase.
192:17 

192:17 Their relationships are as follows. A
192:20 

192:20 payment method can be associated with
192:22 

192:22 many purchases. A purchase is associated
192:25 

192:25 with one payment method. It's simply
192:28 

192:28 another classical one to many
192:29 

192:29 relationship. If you follow the same
192:31 

192:31 steps to establish the two individual
192:34 

192:34 directional relationships and merge them
192:36 

192:36 as one, you will have something like
192:38 

192:38 this. The next step is to make their
192:42 

192:42 attributes explicit and add the foreign
192:45 

192:45 key to the many side entity. However,
192:48 

192:48 before you start, you should know that
192:50 

192:50 we have already established two
192:52 

192:52 relationships on the three entities. I
192:55 

192:55 mean between user purchase and a payment
192:58 

192:58 method. We have already added two
193:01 

193:01 foreign keys into the payment method and
193:04 

193:04 a purchase entities. This is based on
193:06 

193:06 the work we did in the first two
193:08 

193:08 examples.
193:10 

193:10 Once you establish the one to many
193:12 

193:12 relationship between payment method and
193:14 

193:14 the purchase entities, purchase will
193:17 

193:17 have two foreign keys added to it. Email
193:20 

193:20 and payment ID. That's not the issue.
193:24 

193:24 But you may notice that the three
193:26 

193:26 entities form like a circular
193:28 

193:28 relationship, right? That's the issue
193:32 

193:32 and it can definitely lead to some
193:34 

193:34 problems. This is something that we will
193:37 

193:37 address in the future when we cover
193:39 

193:39 normalization.
193:41 

193:41 Regardless of that, you did a great job
193:44 

193:44 in picking up everything about one to
193:46 

193:46 many relationships so far. In this
193:48 

193:48 video, we learned everything about the
193:50 

193:50 one to many relationships in database
193:53 

193:53 design. If you find this video helpful,
193:56 

193:56 give it a thumb up, subscribe, and leave
193:59 

193:59 your questions in the comments. I would
194:01 

194:01 love to hear what's working for you and
194:03 

194:03 what needs to change. This video is
194:06 

194:06 based on the book graing relational
194:08 

194:08 database design, which dives deeper into
194:10 

194:10 database design using real world
194:12 

194:12 examples to explain the concepts. You
194:15 

194:15 can find the link to the book in the
194:17 

194:17 first comment. When it comes to database
194:20 

194:20 design, many beginners try to connect
194:23 

194:23 two tables directly in a many to many
194:25 

194:25 relationship
194:27 

194:27 and as in wonder why SQL queries don't
194:30 

194:30 work. The secret you need a third table.
194:34 

194:34 In this video, we'll show you how to
194:36 

194:36 represent many to many relationships
194:38 

194:38 properly. Before we jump to the many to
194:41 

194:41 many relationships, I will need to
194:43 

194:43 refresh your memory on cardality. First,
194:46 

194:46 the understanding of cardality is
194:48 

194:48 important to understand the many to many
194:50 

194:50 relationships. But if you are wellversed
194:53 

194:53 in this concept, you can skip this
194:55 

194:55 chapter and jump to the many to many
194:57 

194:57 relationship. Cardinality describes the
195:00 

195:00 number of instances of one entity that
195:04 

195:04 can be associated with a single instance
195:06 

195:06 of another entity via the relationship.
195:09 

195:09 There are two key points on cardality.
195:13 

195:13 First, a relationship between two
195:15 

195:15 entities can be broken down into two
195:18 

195:18 different directional relationships when
195:21 

195:21 you look at it from the lens of
195:23 

195:23 cardality. For example,
195:26 

195:26 think about the relationship between
195:28 

195:28 user and account in a database
195:31 

195:31 supporting a bank system. The
195:33 

195:33 relationship between the two entities
195:36 

195:36 are summarized in the two sentences as
195:38 

195:38 you can see on the screen. First, a user
195:42 

195:42 has zero, one or more accounts. Second,
195:47 

195:47 an account is associated with one and
195:50 

195:50 only one user. The two sentences
195:52 

195:52 represent the two directional
195:54 

195:54 relationships.
195:57 

195:57 Sentence one is about the relationship
195:59 

195:59 from user to account while sentence two
196:03 

196:03 is about the relationship from account
196:05 

196:05 to user. Every relationship can be
196:08 

196:08 broken down into two different
196:10 

196:10 directional relationships like this.
196:13 

196:13 That's the first point.
196:15 

196:15 Second, a direct relationship is
196:18 

196:18 represented by two symbols of max and
196:21 

196:21 main cardalities.
196:23 

196:23 The three individual symbols represents
196:26 

196:26 three quantities include zero, one and
196:29 

196:29 many. The circle represents zero. The
196:33 

196:33 bar represents one and the symbol that
196:36 

196:36 looks like a crossoot represents many.
196:39 

196:39 Each directional relationship is
196:42 

196:42 represented by two symbols. For example,
196:45 

196:45 the first sentence you saw earlier is on
196:48 

196:48 the directional relationship from user
196:50 

196:50 to account.
196:53 

196:53 The two symbols representing cardalities
196:56 

196:56 need to be placed in your account. A
196:59 

196:59 user can be associated with zero
197:01 

197:01 accounts. So the main cardality is zero.
197:04 

197:04 A user can be associated with many
197:07 

197:07 accounts as possible. So the max
197:09 

197:09 cardality is many.
197:12 

197:12 The second sentence is on the
197:14 

197:14 directional relationship from account to
197:16 

197:16 user and its cardalities are placed near
197:20 

197:20 user entity. Both of the main and max
197:23 

197:23 cardalities are one. In the end, you are
197:27 

197:27 supposed to merge the two directional
197:29 

197:29 relationship as one relationship between
197:32 

197:32 user and account entities. After they
197:35 

197:35 merge, the positions of the two main and
197:38 

197:38 max cardalities would be the same as if
197:41 

197:41 you draw them separately. From the lens
197:44 

197:44 of cardality, we can put all
197:46 

197:46 relationships into three baskets,
197:49 

197:49 including the one to one relationship,
197:51 

197:51 one to many relationship, and a many to
197:54 

197:54 many relationship.
197:56 

197:56 We will look closely at the many to many
197:58 

197:58 relationships next.
198:01 

198:01 In a many to many relationship, the max
198:04 

198:04 cardalities of both directional
198:06 

198:06 relationships are many.
198:09 

198:09 The many here primarily refers to the
198:11 

198:11 max cardalities of the two directional
198:14 

198:14 relationships. In other words, the name
198:17 

198:17 many to many doesn't concern itself with
198:20 

198:20 the main cardalities. It can be either
198:23 

198:23 zero or one. However, we don't represent
198:27 

198:27 a many to many relationship like what
198:30 

198:30 you see here on the screen.
198:32 

198:32 Then how are many to many relationships
198:35 

198:35 represented?
198:37 

198:37 Think about a real life many to many
198:39 

198:39 relationship between authors and books.
198:42 

198:42 An author writes one to many books.
198:45 

198:45 Well, not a zero to many because a
198:48 

198:48 person who hasn't written a book yet is
198:50 

198:50 not an author. On the other hand, a book
198:53 

198:53 can be written by one to many authors.
198:56 

198:56 This is a classical many to many
198:58 

198:58 relationship.
199:00 

199:00 However, we don't represent such a
199:03 

199:03 relationship like what you see here on
199:05 

199:05 the screen. When such a representation
199:08 

199:08 is implemented, you will introduce many
199:10 

199:10 problems. As a starter, such a design
199:13 

199:13 will force each table to have a foreign
199:15 

199:15 key of its counterpart like this.
199:20 

199:20 After that, because an author is
199:22 

199:22 associated with potentially many books,
199:25 

199:25 you are going to store multiple foreign
199:27 

199:27 key values in a single book ID field,
199:30 

199:30 which violates something we call
199:32 

199:32 normalization rules. Normalization is
199:35 

199:35 something that's very important for you
199:37 

199:37 to design robust database and we will
199:40 

199:40 cover that in future videos. Or the
199:43 

199:43 alternative is that you will end up with
199:46 

199:46 many redundant rows. Well, what we
199:50 

199:50 showed you here is not possible. You
199:52 

199:52 can't have several identical primary key
199:55 

199:55 values in either table. So basically
199:58 

199:58 this representation that you see here is
200:00 

200:00 impossible to implement without any of
200:03 

200:03 the issues.
200:05 

200:05 Then how is the many to many
200:07 

200:07 relationship represented?
200:10 

200:10 Let's stick to the same author and a
200:12 

200:12 book example and give them both a few
200:15 

200:15 attributes like this. The author entity
200:18 

200:18 or say the author table has three
200:21 

200:21 attributes author ID, name and address.
200:24 

200:24 The book entity has three attributes
200:27 

200:27 book ID, title and a cover. We are going
200:31 

200:31 to skip the data types here for
200:33 

200:33 simplicity to represent this
200:36 

200:36 relationship properly. You need to go
200:38 

200:38 one step further by making a junction
200:41 

200:41 table that contains only the primary
200:44 

200:44 keys of the two involved entities author
200:46 

200:46 and a book. Also, you must convert the
200:50 

200:50 many to many relationship to two one to
200:53 

200:53 many relationships between the junction
200:55 

200:55 table and both entities. As you see here
200:59 

200:59 on the screen, the junction table author
201:02 

201:02 book contains only the primary keys from
201:05 

201:05 the author and the book entities. In
201:08 

201:08 this junction table, the book ID and
201:11 

201:11 author ID attributes together serve as a
201:14 

201:14 composite primary key. It's worth noting
201:17 

201:17 that the crow foot always points to the
201:21 

201:21 junction table.
201:23 

201:23 the author and the book entities become
201:26 

201:26 indirectly related via the junction
201:28 

201:28 table. That's how a many to many
201:31 

201:31 relationship is represented.
201:34 

201:34 Now you have two one to many
201:37 

201:37 relationships whose main cardalities are
201:40 

201:40 all ones. This is theoretically okay but
201:44 

201:44 not compatible with the real world
201:46 

201:46 especially data entry. Think about it.
201:50 

201:50 When it comes to entering data into the
201:52 

201:52 database, the tables, it will start with
201:55 

201:55 either the book table or the author
201:57 

201:57 table, then move to the junction table.
202:00 

202:00 But the main cardality of one requires
202:03 

202:03 that there needs to be at least one row
202:06 

202:06 in the author book table, I mean the
202:08 

202:08 junction table to be referenced when a
202:11 

202:11 row of author is added. To address this
202:13 

202:13 issue, you need to relax the main
202:16 

202:16 cardality of the many side from one to
202:18 

202:18 zero. After that, you will no longer
202:21 

202:21 face problems in data entry when you
202:23 

202:23 implement such a design to make sure
202:26 

202:26 that we're on the same page in terms of
202:28 

202:28 the many to many relationship. Let's
202:30 

202:30 look at another example. Imagine that
202:33 

202:33 you are working towards designing a
202:35 

202:35 database for an online store named the
202:37 

202:37 sci-fi collective
202:39 

202:39 and have identified two entities product
202:42 

202:42 and a purchase. The product table
202:45 

202:45 represents the products being sold by
202:48 

202:48 the sci-fi collective. Purchase table
202:50 

202:50 represents the order records of users.
202:54 

202:54 The extracted relationships between the
202:57 

202:57 two entities can be summarized as
202:59 

202:59 following. A purchase can have more than
203:02 

203:02 one product. A product can show up in
203:05 

203:05 multiple purchase records. This is
203:07 

203:07 clearly a many to many relationship
203:10 

203:10 because the max cardalities of both
203:12 

203:12 directional relationships is many. Note
203:16 

203:16 that a product doesn't have to show up
203:18 

203:18 in an order. So the mean cardality from
203:21 

203:21 product to purchase is zero. What you
203:24 

203:24 see here on the screen is more like an
203:27 

203:27 intermediate step before we reach the
203:30 

203:30 proper representation.
203:32 

203:32 To do this right, we needed the help
203:34 

203:34 from a junction table and we are going
203:37 

203:37 to turn this into two one to many
203:39 

203:39 relationships between an entity and the
203:43 

203:43 junction table. The idea would be like
203:45 

203:45 this. Note that the crossoot always
203:49 

203:49 points to the junction table. Beyond
203:51 

203:51 that, whatever you have, I mean both the
203:56 

203:56 min and max cardalities of the original
203:59 

203:59 two directional relationships will be
204:02 

204:02 copied and applied to the relationship
204:04 

204:04 flowing from the entity to the junction
204:07 

204:07 table. When you make all the attributes
204:10 

204:10 explicit, they would look like this. The
204:14 

204:14 last step is to relax them in cardality
204:16 

204:16 from purchase to the junction table. So
204:20 

204:20 you won't face problems when it comes to
204:22 

204:22 implementation and a data entry. In this
204:25 

204:25 video, we learned everything about the
204:28 

204:28 many to many relationship.
204:30 

204:30 If you find this video helpful, give it
204:33 

204:33 a thumb up, subscribe, and leave your
204:35 

204:35 questions in the comments. I would love
204:38 

204:38 to hear what's working for you and what
204:40 

204:40 needs to change. This video is based on
204:43 

204:43 the book Groing Relational Database
204:45 

204:45 Design which dives deeper into database
204:47 

204:47 design using real world examples to
204:50 

204:50 explain the concepts. You can find the
204:52 

204:52 link to the book in the first comment.
204:55 

204:55 When it comes to database design, do you
204:58 

204:58 really need a weak entity or you're just
205:01 

205:01 mislabeling a regular one? Weak entities
205:04 

205:04 are often introduced when you think an
205:06 

205:06 entity depends on another like a
205:09 

205:09 dependent relying on employee. But
205:12 

205:12 dependency alone isn't enough. In fact,
205:15 

205:15 many so-called weak entities should have
205:18 

205:18 been regular entities instead. In this
205:20 

205:20 video, I'll walk you through exactly
205:23 

205:23 what makes an entity weak, when to use
205:25 

205:25 them, and just as importantly, when not
205:28 

205:28 to. By the end, you'll know how to model
205:32 

205:32 relationships in your ER diagram without
205:34 

205:34 misusing weak entities and how to
205:37 

205:37 recognize the rare cases when they
205:39 

205:39 actually make sense.
205:42 

205:42 Let's start by defining strong and weak
205:44 

205:44 entities first. So, what are strong and
205:47 

205:47 weak entities? Does this have anything
205:50 

205:50 to do with cardality? No. Strong and
205:53 

205:53 weak entities are another characteristic
205:56 

205:56 of a relationship between two entities.
205:59 

205:59 And that's beyond cardality. This
206:02 

206:02 characteristic, I mean strong and weak
206:04 

206:04 entities, may come handy for refining er
206:07 

206:07 diagrams. That said, it is up to you to
206:11 

206:11 designate two related entities as strong
206:14 

206:14 and a weak entities when you think it's
206:16 

206:16 appropriate. What's a strong entity?
206:19 

206:19 Then a strong entity is a entity that
206:22 

206:22 has its own unique identifier. Usually
206:24 

206:24 that's just going to be a primary key.
206:27 

206:27 It doesn't depend on any other entities
206:30 

206:30 to exist or to be identified. For
206:32 

206:32 example, the purchase entity that
206:35 

206:35 represents orders in a database of an
206:38 

206:38 online store, it has its own primary
206:41 

206:41 key, the purchase ID to identify each
206:44 

206:44 other or each row individually in the
206:47 

206:47 table. That sounds like just another
206:50 

206:50 regular entity, isn't it? Yes, it is.
206:54 

206:54 There's nothing special about a strong
206:56 

206:56 entity. If you look at it by itself,
206:59 

206:59 what's special is a weak entity that
207:01 

207:01 relies on the strong entity. So what's a
207:04 

207:04 weak entity? A weak entity doesn't have
207:07 

207:07 a unique identifier by itself. It
207:10 

207:10 depends on a related strong entity so
207:12 

207:12 that it can uniquely identify each row
207:15 

207:15 in its corresponding table.
207:18 

207:18 How you may wonder?
207:20 

207:20 Well, its primary key is typically a
207:23 

207:23 composite key and it's composed of a
207:27 

207:27 foreign key from the strong entity plus
207:30 

207:30 one or more attributes of its own. Of
207:33 

207:33 course, it doesn't have to be a
207:35 

207:35 composite key. In that case, the primary
207:38 

207:38 key will simply be the foreign key from
207:41 

207:41 the strong entity. For example, we
207:43 

207:43 looked at the purchase table
207:45 

207:45 representing all the records in a
207:47 

207:47 database of an online store. It's a
207:49 

207:49 strong entity or say a regular entity.
207:52 

207:52 Its corresponding weak entity is a
207:55 

207:55 purchase item entity representing the
207:58 

207:58 information of an individual product
207:60 

208:00 that's involved in an order. As an
208:03 

208:03 entity, purchase item lacks readily
208:06 

208:06 available keys that can uniquely
208:09 

208:09 identify each. So, it relies on the help
208:12 

208:12 from the foreign key to do the job. The
208:15 

208:15 foreign key is a purchase ID referencing
208:18 

208:18 the primary key from the purchase
208:20 

208:20 entity. It's both a foreign key as well
208:23 

208:23 as a primary key of the purchase item
208:25 

208:25 entity.
208:27 

208:27 Now you know what a strong and weak
208:29 

208:29 entity may look like.
208:32 

208:32 Let's move to the key question. When
208:34 

208:34 should or shouldn't you consider using a
208:38 

208:38 weak entity in your database design?
208:42 

208:42 when if you check out some text books or
208:45 

208:45 ask such a question to your generative
208:47 

208:47 AI tool you can find some specific
208:51 

208:51 conditions such as one entity can't
208:54 

208:54 uniquely identify rows by its own
208:56 

208:56 attributes alone or it depends on
208:59 

208:59 related entity for identification yada
209:01 

209:01 yada but as long as you understand how
209:04 

209:04 strong and weak entities are defined
209:06 

209:06 such conditions don't provide much help
209:09 

209:09 because they focus on describing the
209:11 

209:11 character characteristic of weak
209:13 

209:13 entities. But the real challenge is to
209:16 

209:16 identify a weak entity. My experience in
209:20 

209:20 database design and software development
209:22 

209:22 told me a rather practical rule of
209:24 

209:24 thumb, which is when designating a weak
209:28 

209:28 entity makes your database design
209:30 

209:30 simpler and less prone to error, then
209:33 

209:33 that would be a great use case. Yes, as
209:36 

209:36 long as you know how strong and weak
209:38 

209:38 entities are defined, it's up to you to
209:41 

209:41 designate some regular entities as a
209:44 

209:44 weak entity. There are no such thing as
209:46 

209:46 a natural weak entity because nearly all
209:49 

209:49 the entities you design or encounter are
209:52 

209:52 simply regular entities or say strong
209:54 

209:54 entities by default. For example, think
209:57 

209:57 about two entities from a database
209:59 

209:59 supporting movie theater. One represents
210:02 

210:02 a movie and the other represents
210:05 

210:05 tickets. They are in a one to many
210:07 

210:07 relationship and both are regular
210:09 

210:09 entities. Ticket uses a composite key as
210:13 

210:13 its primary key composed of seat number,
210:16 

210:16 room number and time and its movie ID
210:20 

210:20 attribute in the ticket entity is a
210:23 

210:23 foreign key referencing the primary key
210:25 

210:25 from the movie table. Does the existence
210:29 

210:29 of the ticket rely on the movie? Yes or
210:32 

210:32 no? Or you are not sure. If I ask you
210:37 

210:37 twice, you may feel less certain about
210:39 

210:39 it because without movies, there
210:41 

210:41 shouldn't be any tickets being sold by
210:43 

210:43 the movie theater. If you really buy
210:46 

210:46 that, you can convert ticket from a
210:49 

210:49 regular entity or say a strong entity to
210:52 

210:52 a weak one like this. All it takes is to
210:56 

210:56 add movie ID as a part of the composite
210:59 

210:59 key in ticket and the proper
211:01 

211:01 representation of a strong weak entity
211:04 

211:04 relationship needs to use a dotted line
211:07 

211:07 as you can see here on the screen. So
211:10 

211:10 movie ID is a foreign key indeed but
211:13 

211:13 it's also a part of the composite key
211:16 

211:16 that serves as a primary key in the
211:18 

211:18 ticket entity. Do you feel that this
211:21 

211:21 representation is more complex?
211:23 

211:23 Certainly yes. Unnecessarily complex
211:27 

211:27 database design is terrible because it
211:30 

211:30 leads to more complex queries and may
211:33 

211:33 require more space and a data structure
211:36 

211:36 and it's going to be less efficient
211:39 

211:39 because it's not going to be something
211:41 

211:41 that can be easily understood by human
211:43 

211:43 beings either. Unnecessarily complex
211:47 

211:47 design is something we should try to
211:49 

211:49 avoid and the decision is yours whether
211:52 

211:52 to designate a regular entity as a weak
211:55 

211:55 one. In this case, if I were you, I will
211:58 

211:58 stick to the version when both movie and
212:01 

212:01 a ticket are strong and regular
212:03 

212:03 entities. Why?
212:06 

212:06 It's simpler and easy to understand,
212:08 

212:08 friendly to both database systems and
212:11 

212:11 other human beings that you need to work
212:13 

212:13 with on the same database. Does ticket
212:16 

212:16 really rely on movie? That's a good
212:19 

212:19 philosophical question. But in database
212:21 

212:21 design, the answer doesn't need to be a
212:24 

212:24 yes. That said, we also learned
212:27 

212:27 something else that we didn't intend to
212:30 

212:30 cover, which is that a weak entity can
212:33 

212:33 always be converted to a strong entity
212:36 

212:36 if you wanted to, just like what you saw
212:39 

212:39 from the movie and ticket example. All
212:42 

212:42 it takes is to stop using the foreign
212:44 

212:44 key as a part of the primary key in the
212:47 

212:47 weak entity and convert the line that
212:50 

212:50 connects the two entities to be a solid
212:53 

212:53 one. Now, let's reinforce this idea by
212:57 

212:57 going through a few more nuanced
212:59 

212:59 examples.
213:01 

213:01 Imagine that you're working on designing
213:03 

213:03 a database for an online store named as
213:06 

213:06 a sci-fi collective and you have
213:09 

213:09 identified all the entities, attributes,
213:12 

213:12 relationships. Beyond that, you have
213:14 

213:14 also identified the cardalities of each
213:17 

213:17 relationship. Now it's a good time to
213:20 

213:20 think about if there are any regular
213:22 

213:22 entities that you want to convert to
213:24 

213:24 weak entities. For simplicity and
213:27 

213:27 readability, I will hide out the
213:29 

213:29 attributes.
213:31 

213:31 So you have seven tables or seven
213:34 

213:34 entities that's easier to read.
213:37 

213:37 User represents user of the platform or
213:40 

213:40 say the online store. Each user has an
213:44 

213:44 address and possibly many payment
213:46 

213:46 methods. Product represents products
213:49 

213:49 being sold by the online store. Review
213:53 

213:53 represents product reviews from users.
213:56 

213:56 Purchase represents all the orders put
213:60 

213:60 by the users. Product purchase is a
214:03 

214:03 junction table between purchase and a
214:06 

214:06 product because the two entities I mean
214:09 

214:09 purchase and a product are in a many to
214:12 

214:12 many relationship. If you just think
214:15 

214:15 about whether one entity relies on
214:17 

214:17 another in real world, you could
214:20 

214:20 potentially identify this many entities
214:23 

214:23 as your candidates for weak entities.
214:26 

214:26 But should you convert them? The answer
214:29 

214:29 is likely no for most of them. For
214:31 

214:31 example, the review table sits between
214:35 

214:35 user and a product and has two foreign
214:38 

214:38 keys, one from user, the other from
214:41 

214:41 product. This design is solid. If you
214:44 

214:44 are stubborn on the reliance of review
214:47 

214:47 on user and product entities, you can
214:50 

214:50 potentially change review to a weak
214:53 

214:53 entity which will asks you to add email
214:56 

214:56 and code as a part of its jumbo
214:59 

214:59 composite key and you need to change the
215:02 

215:02 lines connecting the three entities to
215:05 

215:05 dotted lines. This change doesn't help
215:08 

215:08 anyone and it complicates things.
215:12 

215:12 It makes the design harder to understand
215:15 

215:15 and makes the database less efficient
215:17 

215:17 because a bigger primary key requires a
215:20 

215:20 bigger storage and a data structure to
215:22 

215:22 work. It can also slow queries because
215:25 

215:25 they need to be indexed. If you don't
215:28 

215:28 know what indexing is, don't worry. We
215:31 

215:31 will cover it in future videos. So, this
215:34 

215:34 is a bad example anyway. The original
215:37 

215:37 design is much better. You may wonder if
215:40 

215:40 there are any cases in which converting
215:42 

215:42 a regular entity to a weak entity helps
215:46 

215:46 make a database design simpler and less
215:49 

215:49 prone to errors. The answer is yes, but
215:52 

215:52 it doesn't occur often. Let's look at a
215:55 

215:55 relationship between user and user
215:58 

215:58 address of the same online store, the
216:01 

216:01 Sci-Fi Collective.
216:03 

216:03 This is the original design between the
216:05 

216:05 user and user address entities. Okay, a
216:09 

216:09 user can have up to one address. The
216:12 

216:12 user address entity uses a surrogate key
216:15 

216:15 as its primary key and has a foreign key
216:19 

216:19 which references the primary key of
216:21 

216:21 user. Now you can ask yourself the
216:24 

216:24 philosophical question. Does a user
216:27 

216:27 address rely on the user for existence?
216:30 

216:30 The answer can certainly be yes. If you
216:33 

216:33 agree and convert the user address
216:35 

216:35 entity to a weak entity, it will look
216:38 

216:38 like this.
216:40 

216:40 Now the user address entity no longer
216:43 

216:43 needs the help from the surrogate key.
216:46 

216:46 All it takes to identify each address is
216:49 

216:49 the email of an user. Remember they are
216:53 

216:53 in a onetoone relationship. So email as
216:56 

216:56 an attribute can identify each address
216:59 

216:59 uniquely.
217:01 

217:01 It's both the foreign key and as a
217:03 

217:03 primary key of the user address entity.
217:06 

217:06 When you put the two designs side by
217:09 

217:09 side, you can tell that the strong and
217:12 

217:12 weak entity in this case is certainly
217:15 

217:15 better because it eliminates the usage
217:18 

217:18 of a surrogate key and it doesn't add
217:20 

217:20 any burdens on understanding such a
217:23 

217:23 relationship or database system that's
217:26 

217:26 going to host such a database. In this
217:28 

217:28 case, I would argue that the strong and
217:31 

217:31 weak entity design makes sense. In this
217:34 

217:34 video, we covered what makes an entity
217:37 

217:37 weak, when to use such entities, and
217:40 

217:40 when not to. If you find this video
217:43 

217:43 helpful, give it a thumb up, subscribe,
217:45 

217:45 and leave your questions in the
217:47 

217:47 comments. I would love to hear what's
217:49 

217:49 working for you and what needs to
217:51 

217:51 change. This video is based on the book
217:54 

217:54 Crocking relational database design
217:56 

217:56 which dives deep into database design
217:59 

217:59 using real world examples to explain the
218:02 

218:02 concepts. You can find the link to the
218:04 

218:04 book in the first comment. Have you ever
218:07 

218:07 seen a table where one cell holds
218:09 

218:09 multiple values like a list of phone
218:11 

218:11 numbers or product IDs or worse a table
218:16 

218:16 with no clear way to identify a single
218:18 

218:18 row? These are not small issues. They
218:21 

218:21 can break the logic of your queries and
218:24 

218:24 ruin your database design. In this
218:26 

218:26 video, I'll introduce you to the concept
218:29 

218:29 of normalization and the simplest of all
218:31 

218:31 the normal forms, the first normal form.
218:34 

218:34 By the end of this video, you'll know
218:36 

218:36 what normalization is and how to use the
218:39 

218:39 first normal form to help you normalize
218:41 

218:41 your database designs. So, what's
218:43 

218:43 normalization? Normalization is like
218:46 

218:46 organizing a library. You are creating a
218:49 

218:49 logical system where each book or say
218:52 

218:52 piece of data has one proper place
218:54 

218:54 making it easier to find and maintain
218:57 

218:57 without duplication. It's a process of
218:59 

218:59 organizing a database in a way that
219:02 

219:02 minimizes redundancy and dependencies
219:05 

219:05 where maximizing data integrity and
219:08 

219:08 consistency. In other words, we break
219:10 

219:10 the database into smaller, more
219:12 

219:12 manageable tables where each table
219:15 

219:15 represents a single entity or concept.
219:18 

219:18 The primary goal of normalization is to
219:20 

219:20 strengthen data integrity. When do you
219:23 

219:23 use or say start the process of
219:25 

219:25 normalization in the database design
219:28 

219:28 process? When you draft your first
219:30 

219:30 version of the ER diagram, I mean when
219:33 

219:33 you successfully identify all the
219:35 

219:35 entities, attributes and relationships
219:37 

219:37 based on the requirement analysis, you
219:40 

219:40 will have something to normalize. Doing
219:42 

219:42 so will help eliminate redundancy,
219:45 

219:45 ensures consistency, avoid insert,
219:48 

219:48 update or delete anomalies. Okay, enough
219:52 

219:52 talking about normalization. How do we
219:54 

219:54 actually do it? You can use normal forms
219:57 

219:57 to guide the normalization process.
219:59 

219:59 Normal forms are the rules that can
220:01 

220:01 guide you break down a big messy table
220:04 

220:04 into smaller more manageable tables.
220:07 

220:07 There are multiple normal forms
220:09 

220:09 including the first normal form, second
220:11 

220:11 normal form, third normal form, and boys
220:14 

220:14 normal form. It can even go up to four
220:17 

220:17 and a five. Each normal form can address
220:20 

220:20 a specific type of problems. The normal
220:23 

220:23 forms build on each other with
220:25 

220:25 increasingly strict requirements. They
220:28 

220:28 form a nested containment relationship
220:31 

220:31 like Russia nesting dolls. To achieve
220:34 

220:34 any normal form, you must firstly
220:36 

220:36 satisfy all the lower levels. The first
220:39 

220:39 normal form is the largest outer doll
220:42 

220:42 and serves as a basic foundation. The
220:45 

220:45 second normal form is the next doll
220:47 

220:47 inside. It's smaller and a more refined
220:50 

220:50 fitting perfectly within the first
220:53 

220:53 normal form but also comes with extra
220:56 

220:56 requirements in comparison. The third
220:58 

220:58 normal form is a doll nested inside the
221:01 

221:01 second normal form which is even more
221:04 

221:04 constrained and organized. It goes on
221:07 

221:07 and on and you get the idea. Just as you
221:10 

221:10 cannot have a inner doll without all the
221:13 

221:13 outer dolls containing it, you cannot
221:16 

221:16 have a database in the third normal form
221:19 

221:19 without it being automatically
221:20 

221:20 satisfying the requirements from the
221:23 

221:23 second normal form and the first normal
221:25 

221:25 form. In this video, we will focus on
221:28 

221:28 the largest alter doll, the simplest
221:31 

221:31 normal form. A table or an entity is in
221:35 

221:35 the first normal form if it meets these
221:38 

221:38 criteria. First, in this table, each
221:41 

221:41 column needs to contain values of the
221:44 

221:44 same data type. Second, each row needs
221:47 

221:47 to be unique. Third, each cell in the
221:51 

221:51 table needs to contain exactly one
221:53 

221:53 value. Let's go over these criteria one
221:56 

221:56 after another to make sure that we are
221:59 

221:59 on the same page. The first criteria is
222:02 

222:02 very clear. Each column needs to have a
222:05 

222:05 specific and only one data type. Imagine
222:08 

222:08 that you are tasked to design a database
222:11 

222:11 for an online store named the Sci-Fi
222:13 

222:13 Collective.
222:15 

222:15 As long as you go through every step
222:17 

222:17 that is required, such as identifying
222:19 

222:19 entities, attributes, their data types,
222:21 

222:21 and all the relationships, it is very
222:24 

222:24 hard for your tables not to meet this
222:27 

222:27 criteria. Remember a complete er diagram
222:30 

222:30 requires every column to have a data
222:33 

222:33 type which means that each column will
222:35 

222:35 contains values of the same data type
222:38 

222:38 that you designate.
222:40 

222:40 Secondly, a table that is in first
222:43 

222:43 normal form needs to make sure that each
222:46 

222:46 row is unique. In other words, the table
222:49 

222:49 needs to have a primary key that can
222:51 

222:51 uniquely identify each row in the table.
222:54 

222:54 Similar as the first criteria that asked
222:57 

222:57 for data types for each column, as long
223:00 

223:00 as you did every step to put together
223:03 

223:03 your ER diagram, each table should have
223:06 

223:06 a primary key, which may come in various
223:09 

223:09 forms such as a natural key like a email
223:13 

223:13 or product code or a composite key or
223:17 

223:17 even a surrogate key. If you need to
223:20 

223:20 refresh your memory on how to pick the
223:22 

223:22 optimal primary key for a table, you can
223:25 

223:25 refer to the linked video in the
223:27 

223:27 description. The third criteria of the
223:30 

223:30 first normal form is worth some good
223:32 

223:32 explanation. It is says that every cell
223:35 

223:35 of the table contains exactly one value.
223:39 

223:39 Or you may see this being phrased as
223:42 

223:42 there are no multivalued columns. What
223:45 

223:45 does the one value mean? or say what
223:47 

223:47 does the multialue columns mean? Let's
223:50 

223:50 look at an example that violates this
223:52 

223:52 criteria like this one. This table is
223:56 

223:56 supposed to represent what courses
223:59 

223:59 students take in a database supporting a
224:02 

224:02 course management system. The
224:04 

224:04 troublesome column that violates this
224:06 

224:06 criteria is the course column. As you
224:10 

224:10 can see here on the screen, each cell in
224:12 

224:12 this column holds the values of multiple
224:16 

224:16 different courses and they are separated
224:19 

224:19 using commas. For a table to be in the
224:21 

224:21 first normal form, each of its cell can
224:24 

224:24 contain only one value or say that there
224:29 

224:29 shouldn't be any multialued columns like
224:31 

224:31 this course column. To fix such an
224:34 

224:34 issue, you need to separate this course
224:37 

224:37 table into two different and smaller
224:40 

224:40 tables. Specifically, you need to split
224:43 

224:43 the multialued column into a new table.
224:46 

224:46 And in this new table, you will make a
224:49 

224:49 separated row for each value that was
224:52 

224:52 packed in a single cell in the old
224:55 

224:55 representation.
224:57 

224:57 So in this case you will have two
224:59 

224:59 tables. One representing students and
225:02 

225:02 the other representing the courses they
225:04 

225:04 take. In the course table since you
225:07 

225:07 split each multia entry to its own row,
225:12 

225:12 it will require a composite key composed
225:15 

225:15 of both columns to uniquely identify
225:18 

225:18 each row. In summary, a new table that
225:21 

225:21 splits each multialued cell into its own
225:25 

225:25 row is a fix for this type of first
225:28 

225:28 normal form of violation.
225:30 

225:30 However, we are not entirely done here.
225:34 

225:34 A question that I got asked a lot by
225:36 

225:36 students on this particular point looks
225:39 

225:39 like this. Who decides whether something
225:42 

225:42 is multialued? Many people feel that
225:45 

225:45 there's no clear standard when it comes
225:47 

225:47 to decide whether a value should be
225:49 

225:49 further split into multiple pieces which
225:52 

225:52 would directly impact the determination
225:54 

225:54 of multivalued columns. For example,
225:58 

225:58 imagine that you have a customer table
225:60 

226:00 in a database supporting an online
226:02 

226:02 store. The table has four columns.
226:05 

226:05 Customer ID, first name, last name, and
226:08 

226:08 email address. Some sample data in this
226:10 

226:10 table may look like this.
226:13 

226:13 Now let's focus on the email column. As
226:16 

226:16 you know, each email address is composed
226:19 

226:19 of two parts. The username and as a
226:21 

226:21 domain name. Should you consider an
226:23 

226:23 email address as multiple values and a
226:27 

226:27 further splitting them into two
226:29 

226:29 different columns, one representing the
226:31 

226:31 username part while the other
226:33 

226:33 representing the domain name part like
226:35 

226:35 what you see here on the screen. Of
226:38 

226:38 course, to avoid redundancy and
226:40 

226:40 anomalies, the next step is to move the
226:43 

226:43 two columns into a new table, right? But
226:46 

226:46 let's hold on there and think again
226:48 

226:48 about this. Should you do this or not?
226:51 

226:51 Here's a rule of thumb that works very
226:54 

226:54 well. Whether something is multialued or
226:56 

226:56 not shouldn't really be up to your whim,
226:59 

226:59 but up to how users see it. Before you
227:02 

227:02 make such a move, you should ask
227:04 

227:04 yourself whether users care about
227:06 

227:06 splitting email addresses into multiple
227:09 

227:09 pieces. By users, I mean the users of
227:11 

227:11 the application supported by the
227:13 

227:13 database, which is the customers of the
227:16 

227:16 online store in this case. The answer is
227:18 

227:18 likely no. Which means that an email
227:21 

227:21 address shouldn't be considered
227:23 

227:23 multalued and a move to split the email
227:26 

227:26 address column into two in a new table
227:29 

227:29 won't be necessary. And that's how you
227:32 

227:32 should think when it comes to decide
227:34 

227:34 whether a column is multivalued or a
227:37 

227:37 table cell contains multiple values. Ask
227:40 

227:40 yourself how the users of the
227:42 

227:42 application supported by the database
227:44 

227:44 feel about it. To make sure that we are
227:47 

227:47 on the same page. Let's look at another
227:49 

227:49 example. What you see here on the screen
227:52 

227:52 is a receipt example from an online
227:55 

227:55 store named the Sci-Fi Collective. Same
227:57 

227:57 as our receipts, it contains the
227:60 

227:60 detailed information of each product you
228:02 

228:02 ordered, their unit price and
228:05 

228:05 quantities. Of course, there's also a
228:07 

228:07 total price. If someone who designs
228:10 

228:10 their database decides to translate the
228:13 

228:13 information from a receipt directly to a
228:16 

228:16 table, they will have a table like this.
228:20 

228:20 then name it as purchase in which each
228:23 

228:23 row of the table will represent an order
228:26 

228:26 from a customer. The logic may sound
228:29 

228:29 straightforward but the issues will
228:32 

228:32 appear as soon as you start populating
228:35 

228:35 this table with data. As you can see
228:37 

228:37 here on the screen in the product ID
228:39 

228:39 column, each cell will contain multiple
228:42 

228:42 ID values of different products. The
228:45 

228:45 same thing can be said for the product
228:47 

228:47 name column.
228:49 

228:49 Ask yourself, would the users of the
228:52 

228:52 applications supported by the database
228:54 

228:54 see multiple product names as multiple
228:57 

228:57 pieces of different values? If the
228:60 

229:00 answer is affirmative, that means that
229:03 

229:03 you have a violation of the multialue
229:05 

229:05 criteria of the first normal form. To
229:08 

229:08 fix this issue, you need to move the two
229:11 

229:11 problematic columns into a new table and
229:15 

229:15 split each entry into multiple rows so
229:18 

229:18 that each cell in this new table will
229:21 

229:21 contain a single value. In this split,
229:24 

229:24 the new table will represent the
229:27 

229:27 itemized product information that is
229:30 

229:30 involved in a single order and the table
229:32 

229:32 will contain the information such as the
229:35 

229:35 product name, unit price and quantities.
229:38 

229:38 That's how you use the first normal form
229:41 

229:41 to guide your normalization of database
229:44 

229:44 designs. In this video, we covered the
229:47 

229:47 concept of normalization and the
229:49 

229:49 simplest normal forms, the first normal
229:52 

229:52 form, as well as how to use it to help
229:55 

229:55 you normalize database designs. If you
229:58 

229:58 find this video helpful, give it a thumb
230:00 

230:00 up, subscribe, and leave your questions
230:03 

230:03 in the comments. I would love to hear
230:05 

230:05 what's working for you and what needs to
230:07 

230:07 change. This video is based on the book
230:10 

230:10 Groing Relational Database Design which
230:12 

230:12 dives deeper into database design using
230:15 

230:15 real world examples to explain the
230:17 

230:17 concepts. You can find the link to the
230:20 

230:20 book in the first comment. Your table
230:22 

230:22 looks fine. It has a primary key. Each
230:26 

230:26 column has a data type and each cell has
230:29 

230:29 a single value. But something still
230:32 

230:32 feels off. Some columns only depend on
230:35 

230:35 part of the primary key. This issue will
230:38 

230:38 lead to all kinds of insert anomaly,
230:41 

230:41 update anomaly and delete anomaly.
230:45 

230:45 How do you fix such a design? It needed
230:47 

230:47 help from something known as the second
230:50 

230:50 normal form.
230:52 

230:52 In this video, I'll walk you through a
230:54 

230:54 simplified and a full version of the
230:56 

230:56 second normal form and explore how to
230:60 

230:60 use the second normal form to guide the
231:02 

231:02 process of normalizing database design.
231:05 

231:05 By the end, you'll have a good
231:07 

231:07 understanding of the second normal form
231:09 

231:09 and how to use it for normalization.
231:11 

231:11 Maybe you'll start with a simplified
231:13 

231:13 version of the second normal form first.
231:16 

231:16 A few words on normal forms first in
231:19 

231:19 case if you need to refresh your memory.
231:22 

231:22 So, what are normal forms? Normal forms
231:24 

231:24 are the rules that can guide you break
231:27 

231:27 down a messy big table into smaller,
231:30 

231:30 more manageable tables. There are
231:32 

231:32 multiple normal forms including the
231:34 

231:34 first normal form, second normal form
231:36 

231:36 and the third normal form and it can
231:39 

231:39 goes on. Each normal form addresses
231:42 

231:42 specific types of problems. The normal
231:45 

231:45 forms build on each other with
231:47 

231:47 increasingly strict requirements. They
231:49 

231:49 form a nested containment relationship.
231:52 

231:52 To achieve any normal form, you must
231:54 

231:54 firstly satisfy all the lower levels.
231:57 

231:57 For example, for a table to satisfy the
232:00 

232:00 requirement of the second normal form,
232:03 

232:03 it has to satisfy all the requirements
232:05 

232:05 of the first normal form in the first
232:07 

232:07 place. The first normal form checks the
232:10 

232:10 fundamental stuff such as whether a
232:12 

232:12 table has a primary key or multival
232:15 

232:15 columns and whether each column has a
232:18 

232:18 data type. If you need to learn more
232:20 

232:20 about the first normal form, you can
232:22 

232:22 find the link of a video dedicated to
232:25 

232:25 the first normal form in the
232:27 

232:27 description. In this video, we will
232:30 

232:30 focus on what problems the second normal
232:32 

232:32 form aims to address. Remember, we're on
232:36 

232:36 our journey to understand a simplified
232:38 

232:38 version of the second normal form. For
232:41 

232:41 now, if a table is in the second normal
232:44 

232:44 form, then every nonkey column should
232:47 

232:47 depend on the whole primary key and not
232:50 

232:50 just a part of it. There are two terms
232:53 

232:53 that require some clarification. First,
232:56 

232:56 by non-key column, I mean the columns
232:59 

232:59 that are not the primary key or a part
233:02 

233:02 of the primary key in case the primary
233:05 

233:05 key is a composite key that has more
233:07 

233:07 than one columns. Second depends on here
233:11 

233:11 refers to functional dependency. To
233:14 

233:14 fully understand it, we need to
233:16 

233:16 introduce you the concept of functional
233:18 

233:18 dependency. What's functional
233:20 

233:20 dependency? It's about one piece of data
233:23 

233:23 determining another. For example,
233:26 

233:26 suppose you have a function called power
233:28 

233:28 that takes an input and returns its
233:31 

233:31 square value. Given any value of x, the
233:34 

233:34 output of power x is completely
233:37 

233:37 determined by x. You can also say that
233:40 

233:40 the result of applying the power
233:42 

233:42 function to a number functionally
233:45 

233:45 depends on that number. When the same
233:48 

233:48 concept is applied to columns in the
233:50 

233:50 same table, you will have something like
233:53 

233:53 this. If you know the value of column A,
233:56 

233:56 you can figure out the value of column B
233:59 

233:59 every time with no exceptions.
234:02 

234:02 For example, think about a table
234:05 

234:05 representing employees and the table has
234:08 

234:08 only two columns employee ID and
234:11 

234:11 employee name. Employee ID is the
234:14 

234:14 primary key of this table. Given a value
234:17 

234:17 of the employee ID, you can always
234:20 

234:20 figure out the corresponding employee
234:22 

234:22 name.
234:24 

234:24 We can say that employee ID functionally
234:28 

234:28 determines the employee name in this
234:30 

234:30 table and express this dependency
234:33 

234:33 relationship using an arrow that goes
234:35 

234:35 from employee ID to employee name.
234:39 

234:39 You may also see the same relationship
234:42 

234:42 expressed as employee name functionally
234:45 

234:45 depends on employee ID. Okay, that's
234:49 

234:49 everything about functional dependency
234:51 

234:51 and we are going to use it to depen our
234:54 

234:54 understanding of the second normal form.
234:56 

234:56 And let's go back to the second normal
234:59 

234:59 form. Now
235:01 

235:01 beyond asking the table to be in the
235:03 

235:03 first normal form, the simplified
235:05 

235:05 version only has one criteria which is
235:09 

235:09 about nonkey columns functionally
235:12 

235:12 depending on the primary key. And it's
235:15 

235:15 worth emphasizing that it has to be the
235:18 

235:18 whole primary key, not a part of it.
235:21 

235:21 Let's focus on this criteria and explore
235:25 

235:25 what a violation of this criteria may
235:27 

235:27 look like and how to fix it. For
235:30 

235:30 example,
235:32 

235:32 think about an enrollment table
235:34 

235:34 representing which course a student
235:36 

235:36 enrolls in a database supporting a
235:39 

235:39 course management system. The enrollment
235:41 

235:41 table uses a composite key as its
235:44 

235:44 primary key and it is composed of two
235:48 

235:48 columns student ID and a course ID.
235:51 

235:51 Beyond as a primary key the enrollment
235:54 

235:54 table has three more columns student
235:56 

235:56 name course name and a grade. When the
235:60 

235:60 table is populated with data it may look
236:03 

236:03 like this. On the first look, the design
236:06 

236:06 of this table is fine until you find out
236:09 

236:09 that the table contains three functional
236:12 

236:12 dependencies and two of them only uses a
236:16 

236:16 part of the primary key. The first
236:18 

236:18 functional dependency is where student
236:21 

236:21 ID functionally determines student name.
236:24 

236:24 As you can imagine, the course ID
236:26 

236:26 doesn't help with that at all. The
236:29 

236:29 second functional dependency is where
236:31 

236:31 the course ID functionally determines a
236:33 

236:33 course name. Similarly, student ID won't
236:37 

236:37 help much with determining a course
236:39 

236:39 name. We typically refer to these two
236:42 

236:42 functional dependencies as partial
236:45 

236:45 dependency because the determinant in
236:48 

236:48 the two relationships is simply a part
236:51 

236:51 of the primary key.
236:54 

236:54 Ali's the last functional dependency
236:56 

236:56 uses the primary key as a determinant.
236:59 

236:59 The primary key student ID plus course
237:02 

237:02 ID in combination determines a grade.
237:06 

237:06 How do you fix a violation of the second
237:09 

237:09 normal form like this?
237:11 

237:11 In general, you need to move any columns
237:14 

237:14 that depend on only a part of the
237:16 

237:16 primary key to a new table along with
237:18 

237:18 the part of the key. the part of the key
237:21 

237:21 will become the new table's primary key.
237:24 

237:24 So in this case, you need to move
237:26 

237:26 student name plus student ID to a new
237:29 

237:29 table and we can call this new table
237:32 

237:32 student and its primary key is student
237:35 

237:35 ID. The same thing can be said for the
237:38 

237:38 course name plus course ID. We can call
237:41 

237:41 this new table course and its primary
237:43 

237:43 key is course ID. What would happen to
237:46 

237:46 the old enrollment table?
237:49 

237:49 In general, you will leave behind only
237:51 

237:51 the columns that truly depend on the
237:53 

237:53 entire original composite primary key.
237:56 

237:56 For the columns that meet both
237:58 

237:58 conditions, they will be kept in both
238:01 

238:01 the newly created table and the old
238:04 

238:04 table such as the student ID column and
238:07 

238:07 the course ID column. That's how you fix
238:10 

238:10 a violation of the second normal form.
238:13 

238:13 Now you have a good understanding of the
238:16 

238:16 simplified version of the second normal
238:18 

238:18 form. It's time for us to learn about
238:21 

238:21 the full version or say the real thing.
238:24 

238:24 In the simplified version, we laser
238:27 

238:27 focused on the relationship between a
238:29 

238:29 nonkey column and a part of the primary
238:32 

238:32 key. But the full version is actually a
238:36 

238:36 bit broader. Beyond the primary key, it
238:39 

238:39 also cares about any candidate keys in
238:42 

238:42 the same table. The full version of the
238:45 

238:45 second normal form states that no nonkey
238:48 

238:48 columns depend on a proper subset of any
238:52 

238:52 candidate key. In other words, in a
238:54 

238:54 table that doesn't violate the full
238:57 

238:57 version of the second normal form, you
238:59 

238:59 shouldn't be able to find any nonkey
239:01 

239:01 column that partially depends on the
239:04 

239:04 primary key or any other candidate keys.
239:07 

239:07 To make sure that we're on the same
239:09 

239:09 page, I wanted to explain what it means
239:12 

239:12 by a proper subset.
239:15 

239:15 It means a smaller group that's
239:17 

239:17 completely contained within a large
239:20 

239:20 group and it can never be as large as
239:23 

239:23 the large group. Let's look at an
239:25 

239:25 example. When this concept is being
239:28 

239:28 applied to keys,
239:30 

239:30 think about the enrollment table in the
239:32 

239:32 database of a course management system.
239:36 

239:36 It's using a composite key as its
239:38 

239:38 primary key. So three columns in
239:40 

239:40 combination including student ID, course
239:43 

239:43 code and term code they make the primary
239:47 

239:47 key. Okay. A proper subset of this key
239:51 

239:51 can be composed of any columns from the
239:54 

239:54 three columns but the total number of
239:56 

239:56 the columns of this set has to be
239:59 

239:59 smaller than three. For example, the
240:02 

240:02 combination of course code and a term
240:05 

240:05 code makes a proper subset of the
240:07 

240:07 primary key or student ID and a term
240:10 

240:10 code. Whatever you do in order to make a
240:13 

240:13 proper subset, it can't be the same as
240:16 

240:16 the original primary key. Okay. So by a
240:20 

240:20 proper subset of any candidate keys, the
240:23 

240:23 full version of the second normal form
240:26 

240:26 is still talking about partial
240:28 

240:28 dependency but in a more accurate way.
240:32 

240:32 Now you know the definition of the full
240:34 

240:34 version of the second normal form. Let's
240:37 

240:37 compare the simplified version to the
240:39 

240:39 full version so that you can have a
240:42 

240:42 robust understanding of the second and
240:44 

240:44 normal form and appreciate the fine
240:46 

240:46 grain difference between the two
240:47 

240:47 versions. Let's look at this employee
240:50 

240:50 project table which represents a
240:53 

240:53 project's employees are engaged with in
240:56 

240:56 a company. The table uses a composite
240:59 

240:59 key as its primary key which is composed
241:02 

241:02 of two columns. Employee ID and project
241:05 

241:05 ID. Beyond that, it has three more
241:08 

241:08 columns. Employee social security number
241:10 

241:10 or short for employee SSN, project name
241:14 

241:14 and a credit score.
241:16 

241:16 In addition to the primary key, the
241:18 

241:18 table has another candidate key, the
241:21 

241:21 combination of employee SSN and a
241:24 

241:24 project ID. The table has two functional
241:27 

241:27 dependencies. The first one is about the
241:30 

241:30 primary key functionally determining
241:32 

241:32 three columns including employee SSN,
241:36 

241:36 project name and a project budget. The
241:39 

241:39 second functional dependency is about
241:41 

241:41 employee SSN functionally determining
241:44 

241:44 the column of credit score. In case if
241:48 

241:48 you think the design of this table is
241:50 

241:50 ridiculous, it is indeed. However, a
241:54 

241:54 reasonable and authentic design that can
241:56 

241:56 differentiate the simplified and the
241:58 

241:58 full version of the second normal form
242:01 

242:01 doesn't really exist. So bear with me on
242:04 

242:04 this. Okay, let's continue. Does this
242:07 

242:07 table violate the criteria of the
242:10 

242:10 simplified version of the second normal
242:12 

242:12 form?
242:14 

242:14 Then you need to ask yourself, do you
242:16 

242:16 have a partial dependency on the primary
242:18 

242:18 key? Nope. So it's perfectly valid in
242:23 

242:23 the eyes of the simplified version of
242:25 

242:25 the second normal form.
242:27 

242:27 However, does the table violate the
242:29 

242:29 criteria of the full version of the
242:31 

242:31 second normal form? Employee SSN is a
242:35 

242:35 part of a candidate key. Yet it
242:38 

242:38 functionally determines another column
242:41 

242:41 which is a credit score. That is that
242:44 

242:44 you do have a violation and the table is
242:48 

242:48 not in the second normal form. Okay. How
242:51 

242:51 do you fix the issue? In the same way as
242:54 

242:54 you would fix a violation of the
242:56 

242:56 simplified version of the second normal
242:58 

242:58 form. It will move the columns that
243:01 

243:01 depend on only a part of a candidate key
243:04 

243:04 to a new table and keep only those
243:07 

243:07 columns that fully depend on a key in
243:10 

243:10 the original table. In this case, you
243:13 

243:13 will move employee SSN and credit score
243:16 

243:16 to a new table and employee SSN the
243:19 

243:19 determinant will become the new primary
243:22 

243:22 key in this table. Beyond that, you will
243:25 

243:25 also keep the columns in the original
243:27 

243:27 table. I mean the columns that are not
243:31 

243:31 partially dependent on any candidate
243:33 

243:33 keys. In our case, there's one column
243:36 

243:36 employee SSN that meets the requirement
243:40 

243:40 of both conditions. So it's going to be
243:42 

243:42 kept in both tables. Okay. In real
243:46 

243:46 world, most of violation of the second
243:49 

243:49 normal form is on the simplified
243:51 

243:51 version. The violation that happens to a
243:54 

243:54 candidate key that doesn't serve as a
243:56 

243:56 primary key is very rare. But being able
243:59 

243:59 to understand their fine grained
244:01 

244:01 difference is still important
244:02 

244:02 theoretically.
244:04 

244:04 If you get it now, you can pat yourself
244:06 

244:06 on the shoulder. Good job.
244:09 

244:09 In this video, we covered a simplified
244:12 

244:12 and a full version of the second normal
244:14 

244:14 form and how to use them to guide your
244:17 

244:17 normalization of database design. I
244:19 

244:19 would love to hear what's working for
244:21 

244:21 you and what needs to change. This video
244:24 

244:24 is based on the book Groing Relational
244:26 

244:26 Database Design which dives deeper into
244:28 

244:28 database design using real world
244:31 

244:31 examples to explain the concepts. You
244:34 

244:34 can find the link to the book in the
244:36 

244:36 first comment. Imagine this. You updated
244:39 

244:39 a student's major name in one row in a
244:42 

244:42 table but forgot to change it in other
244:44 

244:44 three. Now your data is inconsistent and
244:48 

244:48 worse your reports are wrong. This
244:50 

244:50 happens when your table has transitive
244:53 

244:53 dependencies. In this video you'll learn
244:56 

244:56 how the third normal form can be used to
244:59 

244:59 address issues like this in your
245:01 

245:01 database design and why it matters in
245:03 

245:03 real world situations. Let's get
245:05 

245:05 started. A few words on normal forms
245:08 

245:08 first in case if you need to refresh
245:10 

245:10 your memory. So what are normal forms?
245:13 

245:13 Normal forms are the rules that can
245:16 

245:16 guide you break down messy big tables
245:18 

245:18 into smaller and more organized tables.
245:21 

245:21 There are multiple normal forms
245:23 

245:23 including the first normal form, second
245:26 

245:26 normal form and a third one and it can
245:28 

245:28 goes on and on. Each normal form
245:31 

245:31 addresses specific types of problems and
245:35 

245:35 normal forms build on each other with
245:37 

245:37 increasingly strict requirements. They
245:40 

245:40 form a nested containment relationship.
245:43 

245:43 To achieve any normal form, you must
245:46 

245:46 first satisfy all the lower levels. For
245:48 

245:48 example, for a table to satisfy the
245:51 

245:51 requirements of the third normal form,
245:54 

245:54 it has to satisfy all the requirements
245:57 

245:57 of the first normal form and the second
245:59 

245:59 normal form in the first place. The
246:02 

246:02 first normal form checks the fundamental
246:05 

246:05 stuff such as whether a table has a
246:07 

246:07 primary key or multivalued columns and
246:10 

246:10 whether each column has a data type. The
246:13 

246:13 second normal form is about dealing with
246:15 

246:15 partial dependencies of n and keys. If
246:19 

246:19 you need to learn more about the first
246:21 

246:21 normal form or the second normal form,
246:24 

246:24 you can find the links of videos
246:26 

246:26 dedicated to the two normal forms in the
246:28 

246:28 description. In this video, we will
246:31 

246:31 focus on the problems that the third
246:33 

246:33 normal form aims to address. Remember,
246:37 

246:37 we are on our journey to understand a
246:39 

246:39 simplified version of the third normal
246:41 

246:41 form. For now, based on the definition
246:43 

246:43 of the simplified version, if a table is
246:46 

246:46 in the third normal form, then no nonkey
246:50 

246:50 columns in this table can be
246:52 

246:52 transitively dependent on the primary
246:55 

246:55 key. There are two terms here that
246:57 

246:57 require some clarification. First, by
247:00 

247:00 nonkey column, I mean the columns that
247:03 

247:03 are not the primary key or a part of the
247:06 

247:06 primary key in case if the primary key
247:08 

247:08 is a composite one that has more than
247:11 

247:11 one columns.
247:13 

247:13 Second, transitively dependent. Before
247:16 

247:16 we get to this term, let's make sure
247:18 

247:18 that we are on the same page in terms of
247:21 

247:21 what a dependency means. First, in the
247:24 

247:24 context of columns of a table,
247:26 

247:26 functional dependency means that if you
247:28 

247:28 know the value of column A, you can
247:31 

247:31 always figure out the value of column B
247:34 

247:34 every time without exception. In such a
247:36 

247:36 case, you can say that column A
247:39 

247:39 functionally determines column B or you
247:42 

247:42 can also say that column B functionally
247:45 

247:45 depends on column A. For example, think
247:48 

247:48 about a table representing employees.
247:51 

247:51 And the table has only two columns,
247:53 

247:53 employee ID and employee name. Employee
247:57 

247:57 ID is the primary key of the table.
247:59 

247:59 Given a value of the employee ID, you
248:02 

248:02 can always figure out the corresponding
248:04 

248:04 employee name. We can see that employee
248:07 

248:07 ID functionally determines employee name
248:11 

248:11 and express this dependency relationship
248:13 

248:13 using an arrow that goes from employee
248:16 

248:16 ID to employee name. You may also see
248:19 

248:19 the same relationship expressed as
248:21 

248:21 employee name functionally depends on
248:24 

248:24 employee ID. Okay.
248:27 

248:27 Now to transitive dependency. It means a
248:31 

248:31 nonkey column is functionally dependent
248:33 

248:33 on another nonkey column which in turn
248:37 

248:37 is functionally dependent on a key in
248:40 

248:40 the table. Now you know what transitive
248:43 

248:43 dependency is. Let's go back to the
248:46 

248:46 issues the third normal form aims to
248:49 

248:49 address. I mean the simplified version
248:51 

248:51 of the third normal form. If a table
248:54 

248:54 contains transitive dependency, it means
248:57 

248:57 trouble. It means that the table
248:59 

248:59 violates a certain normal form and needs
249:01 

249:01 to be fixed. For example, think about a
249:05 

249:05 table representing employees of a
249:07 

249:07 company. It has four columns and uses a
249:11 

249:11 surrogate key employee ID as the primary
249:14 

249:14 key. The table contains two functional
249:17 

249:17 dependencies. The first is about the
249:20 

249:20 primary key functionally determining two
249:23 

249:23 other columns including employee name
249:26 

249:26 and the department ID. The second
249:29 

249:29 functional dependency is about
249:31 

249:31 department ID functionally determining
249:34 

249:34 department name. Based on such
249:36 

249:36 information, you can tell that there's a
249:39 

249:39 transitive dependency which department
249:42 

249:42 name is functionally determined by
249:44 

249:44 department ID which in turn is
249:46 

249:46 determined by employee ID. Before we
249:50 

249:50 talk about how to fix such an issue, I
249:52 

249:52 want you to know what issues transitive
249:55 

249:55 dependencies may cause. Think about when
249:58 

249:58 the table is populated with data. There
250:01 

250:01 would be a lot of redundancies since
250:03 

250:03 many employee will belong to the same
250:06 

250:06 department which means that the same
250:08 

250:08 department ID values and department name
250:11 

250:11 values are repeated in many different
250:14 

250:14 rows. Beyond that, if you ever try to
250:17 

250:17 update a department name, you will need
250:20 

250:20 to make sure to update every row that
250:23 

250:23 contains the old value. If you ever
250:25 

250:25 forget to do that, there will be data
250:28 

250:28 inconsistency.
250:29 

250:29 In short, it will cause data redundancy
250:32 

250:32 and all kinds of updated delete
250:35 

250:35 anomalies.
250:36 

250:36 Now you know what the problem is. Let's
250:38 

250:38 talk about how to fix the issue. You
250:41 

250:41 need to move the columns in the
250:43 

250:43 transitive dependencies to a different
250:45 

250:45 table and the determinant will become
250:48 

250:48 the primary key in the new table. In
250:51 

250:51 this example, that's a department table.
250:54 

250:54 Additionally, you will keep all the
250:56 

250:56 columns that are directly dependent on
250:59 

250:59 the primary key in the old table. So the
251:02 

251:02 employee table will have three columns
251:05 

251:05 after the update. Notice that the
251:07 

251:07 department ID will show up in both
251:10 

251:10 tables in our fix. That's because it
251:13 

251:13 perfectly meets the two conditions.
251:16 

251:16 I mean it's directly dependent on the
251:18 

251:18 primary key in the employee table as
251:21 

251:21 well as it was involved in the
251:24 

251:24 transitive dependency. That's why it
251:27 

251:27 ended up showing in both of the two
251:29 

251:29 tables after we fix this design flaw.
251:32 

251:32 Beyond a bad design inherent to a single
251:35 

251:35 table, a violation of the third normal
251:37 

251:37 form can also be caused by three tables
251:40 

251:40 in a circular relationship.
251:43 

251:43 For example, think about three tables
251:45 

251:45 from a database supporting an online
251:47 

251:47 store. So the tables are user, payment
251:51 

251:51 methods, and a purchase. The user table
251:54 

251:54 represents users of the store. Payment
251:57 

251:57 methods represent users different
251:59 

251:59 payment methods that are used for
252:01 

252:01 ordering stuff online and a purchase
252:03 

252:03 table represents the order records from
252:06 

252:06 users. between each pair of the two
252:09 

252:09 tables. It's a one to many relationship.
252:12 

252:12 However, that also leads to putting two
252:15 

252:15 foreign keys, payment ID and email, into
252:18 

252:18 the purchase table. Payment ID is the
252:21 

252:21 primary key in the payment method table.
252:24 

252:24 Email is the primary key in the user
252:27 

252:27 table. However, we know that the primary
252:30 

252:30 key in a table functionally determines
252:32 

252:32 all other nonkey columns. From the
252:35 

252:35 payment method table, we can learn that
252:38 

252:38 payment ID determines email. From the
252:42 

252:42 purchase table, we have something
252:44 

252:44 similar, which is the purchase ID
252:47 

252:47 functionally determines payment ID. If
252:50 

252:50 we put these two relationships together
252:53 

252:53 and focus your attention on the purchase
252:56 

252:56 table, we will have a transitive
252:58 

252:58 dependency like this in which purchase
253:02 

253:02 ID functionally determines payment ID
253:05 

253:05 and a payment ID also determines email.
253:09 

253:09 So by definition that's a transitive
253:12 

253:12 dependency even though the purchase ID
253:16 

253:16 can also directly determine email in the
253:18 

253:18 same table. How do you fix such an issue
253:22 

253:22 when three tables present themselves in
253:25 

253:25 such a circular relationship? All you
253:28 

253:28 need to do is to remove one of the three
253:30 

253:30 relationships so that the table that
253:33 

253:33 suffers from transitive dependency
253:35 

253:35 issues will no longer need to. In this
253:37 

253:37 case, you can remove the relationship
253:40 

253:40 between user and purchase. In other
253:42 

253:42 words, you need to remove the foreign
253:45 

253:45 key email from the purchase table as
253:47 

253:47 well. The fixed relationship will look
253:50 

253:50 like this. With such a fix, you may
253:52 

253:52 wonder how the purchase table can be
253:54 

253:54 connected to user if there's ever such a
253:57 

253:57 need. They can still go through the
253:60 

253:60 payment method table. Then everything
254:02 

254:02 about the simplified version of the
254:05 

254:05 third normal form. Let's move to look at
254:08 

254:08 the full version and explores the fine
254:10 

254:10 grain difference between the simplified
254:13 

254:13 version and the full version next.
254:16 

254:16 In the simplified version, we laser
254:19 

254:19 focused on the transitive dependency on
254:21 

254:21 the primary key. But as the full version
254:24 

254:24 is actually a bit broader beyond the
254:27 

254:27 primary key, it also cares about any
254:30 

254:30 candidate keys that are involved in
254:32 

254:32 transitive dependencies.
254:34 

254:34 The full version of the third normal
254:36 

254:36 form states that no nonkey columns are
254:40 

254:40 transitively dependent on any candidate
254:42 

254:42 key. In other words, in a table that
254:46 

254:46 doesn't violate the full version of the
254:48 

254:48 third normal form, you shouldn't be able
254:51 

254:51 to find any nonkey column that is
254:54 

254:54 transitively dependent on any keys.
254:57 

254:57 Now you know the definition of the full
254:60 

254:60 version of the third normal form. Let's
255:03 

255:03 compare the simplified version to this
255:05 

255:05 full version so that you can have a
255:08 

255:08 robust understanding of the third normal
255:10 

255:10 form and appreciate the fine grain
255:12 

255:12 difference between the two versions.
255:15 

255:15 Let's look at the enrollment project
255:17 

255:17 table which represents student
255:20 

255:20 enrollment at a university. The table
255:23 

255:23 uses email address as a primary key.
255:27 

255:27 Beyond that, it has three more columns.
255:30 

255:30 Student ID, name, and GPA. In addition
255:34 

255:34 to the primary key, the table has
255:36 

255:36 another candidate key, the student ID.
255:40 

255:40 The table has three functional
255:42 

255:42 dependencies. The first one is about the
255:46 

255:46 primary key functionally determining two
255:48 

255:48 columns including student ID and GPA.
255:53 

255:53 The second is about student ID as a
255:56 

255:56 candidate key functionally determining
255:59 

255:59 name and the GPA. The third is about the
256:03 

256:03 name column functionally determining the
256:06 

256:06 GPA column. In case if you think the
256:09 

256:09 design of the table is ridiculous, it is
256:12 

256:12 indeed ridiculous. However, a reasonable
256:15 

256:15 and authentic design that can help
256:17 

256:17 differentiate the simplified and full
256:20 

256:20 versions of the third normal form almost
256:22 

256:22 doesn't exist. So bear with me on this
256:25 

256:25 and let's continue. Does this table
256:28 

256:28 violate the criteria of the simplified
256:31 

256:31 version of the third normal form? You
256:33 

256:33 can ask yourself,
256:35 

256:35 do you see any transitive dependency on
256:38 

256:38 the primary key? Nope.
256:41 

256:41 That said, it's perfectly valid in the
256:44 

256:44 eyes of the simplified version of the
256:46 

256:46 third normal form. However, does the
256:50 

256:50 table violate the criteria of the full
256:52 

256:52 version of the third normal form?
256:54 

256:54 Student ID as a candidate key
256:57 

256:57 functionally determines name and name in
256:60 

257:00 turn functionally determines GPA. That
257:03 

257:03 said, you do have a transitive
257:05 

257:05 dependency on a candidate key.
257:09 

257:09 So the table is not in the third normal
257:11 

257:11 form. How do you fix this issue?
257:15 

257:15 In the same way as you fix a violation
257:17 

257:17 to the simplified version of the third
257:20 

257:20 normal form, you need to move the
257:22 

257:22 columns in the transitive dependencies
257:25 

257:25 to a different table and the determinant
257:28 

257:28 will become the primary key in the new
257:30 

257:30 table. In this example, that's the name
257:33 

257:33 GPA table. Additionally, you will keep
257:36 

257:36 all the columns that are directly
257:39 

257:39 dependent on the primary key and any
257:41 

257:41 other candidate keys in the old table.
257:44 

257:44 So the enrollment table will have three
257:47 

257:47 columns after the fix. Notice that the
257:50 

257:50 name column will show up in both tables.
257:53 

257:53 That's because it perfectly fits both
257:56 

257:56 conditions. I mean it's directly
257:58 

257:58 dependent on the primary key as well as
258:01 

258:01 was involved in the transitive
258:03 

258:03 dependency. That's why it shows up in
258:06 

258:06 both tables after we fix this design
258:08 

258:08 flaw. I know that the name GPA table
258:12 

258:12 looks really ridiculous in real world.
258:14 

258:14 You would never put such two columns as
258:16 

258:16 one table. But let's just focus on the
258:20 

258:20 things at the theoretical level. After
258:22 

258:22 all, an example that can do this job, I
258:25 

258:25 mean, differentiating the two versions
258:27 

258:27 is very elusive.
258:30 

258:30 In this video, we covered a simplified
258:33 

258:33 and a full version of the third normal
258:35 

258:35 form and how to use them to guide your
258:37 

258:37 normalization of database designs. I
258:40 

258:40 would love to hear what's working for
258:42 

258:42 you and what needs to change. This video
258:44 

258:44 is based on the book graining relational
258:47 

258:47 database design which dives deeper into
258:50 

258:50 database design using real world
258:52 

258:52 examples to explain the concepts. You
258:55 

258:55 can find the link to the book in the
258:57 

258:57 first comment. Imagine that you have
258:60 

258:60 followed all the rules designed a
259:02 

259:02 database that meets all the requirements
259:04 

259:04 of the third normal form but it still
259:07 

259:07 breaks.
259:09 

259:09 That's exactly what happened to Raymond
259:11 

259:11 boys in the 1970s which led to the
259:15 

259:15 development of the boys called normal
259:17 

259:17 form short for BCNF.
259:20 

259:20 In this video we are going to introduce
259:23 

259:23 you to a new normal form the boy called
259:25 

259:25 normal form. Yet this video isn't just
259:28 

259:28 about BNF. It's also about how one of
259:32 

259:32 the most elegant series theories in
259:34 

259:34 databases turned out to have a subtle
259:37 

259:37 flaw and how a young researcher caught
259:39 

259:39 it. Before we get to the drama, let's
259:42 

259:42 quickly recall what the third normal
259:44 

259:44 form is trying to prevent. For a table
259:47 

259:47 to be in the third normal form, it needs
259:50 

259:50 to be in the first and the second normal
259:52 

259:52 forms first. What is a third normal form
259:56 

259:56 specifically requires is that no nonkey
259:59 

259:59 columns can be transitively dependent on
260:03 

260:03 any candidate keys. For example, a
260:07 

260:07 violation of the third normal form may
260:09 

260:09 look like this. when a nonkey column
260:13 

260:13 functionally depends on another nonkey
260:15 

260:15 column and ultimately on some key like
260:18 

260:18 this. To fix a table that violates the
260:22 

260:22 requirements of the third normal form,
260:25 

260:25 you need to move the columns in the
260:27 

260:27 transitive dependencies to a different
260:29 

260:29 table and the determinant will become
260:32 

260:32 the primary key in the new table.
260:34 

260:34 Additionally, it will keep all the
260:37 

260:37 columns that are directly dependent on
260:40 

260:40 the primary key in the old table. There
260:43 

260:43 would be typically a column that meets
260:45 

260:45 both of the two conditions and it got to
260:48 

260:48 be placed in both tables like what you
260:51 

260:51 see here in the example on the screen.
260:54 

260:54 The third normal form was developed by
260:56 

260:56 Adagard Cot in 1970s. His work laid the
261:00 

261:00 foundation for nearly all modern
261:02 

261:02 database systems for which he won a
261:05 

261:05 touring award. The development of the
261:07 

261:07 third normal form was detailed in his
261:10 

261:10 paper titled a rational model of data
261:13 

261:13 for large shared data banks published in
261:16 

261:16 the 1970.
261:18 

261:18 After the paper was published for four
261:21 

261:21 years, no one examined or challenged
261:24 

261:24 whether the third normal form definition
261:26 

261:26 was sufficiently rigorous. In 1974, a
261:31 

261:31 young scientist named Raymond Boyce was
261:33 

261:33 working through a database design
261:35 

261:35 example and noticed that something odd.
261:39 

261:39 When two candidate keys shared a column,
261:42 

261:42 certain normal form doesn't always
261:44 

261:44 protect you for update or delete
261:46 

261:46 anomalies. it is supposed to address.
261:50 

261:50 For example, think about a table named
261:52 

261:52 the teacher course. It's a messed up
261:55 

261:55 design of course and probably won't make
261:58 

261:58 much sense to the real world, but it is
262:01 

262:01 still useful enough theoretically. The
262:04 

262:04 table has two candidate keys and both
262:06 

262:06 are composite keys. The first one uses
262:09 

262:09 teacher ID and a course ID while the
262:12 

262:12 second one uses course ID and the room.
262:16 

262:16 The functional dependency in the table
262:18 

262:18 is between two columns. Course ID
262:21 

262:21 depends on room. This design is
262:24 

262:24 perfectly valid in the eyes of the third
262:26 

262:26 normal form as all the columns belong to
262:30 

262:30 some candidate keys. So there's no
262:33 

262:33 nonkey columns.
262:35 

262:35 That's no transitive dependencies.
262:39 

262:39 However, if the table is populated and
262:42 

262:42 when you ever try to perform an update
262:45 

262:45 such as updating the room where CS 101
262:48 

262:48 is told, you need to remember to find
262:51 

262:51 all the rows where course ID is CS101
262:56 

262:56 and then update all their rooms.
262:58 

262:58 Otherwise, you will have an update
263:01 

263:01 anomaly. Remember the functional
263:04 

263:04 dependency between course ID and the
263:06 

263:06 room asked you to do so. So the third
263:10 

263:10 normal form is not strict enough
263:13 

263:13 especially the focus on nonkey columns.
263:17 

263:17 Theoretically speaking you can have some
263:20 

263:20 rogue columns that are part of a key and
263:23 

263:23 it still gives you the trouble of
263:24 

263:24 transitive dependencies.
263:27 

263:27 Actually it doesn't have to be a
263:29 

263:29 transitive dependency either.
263:32 

263:32 As you saw in this example, when a part
263:35 

263:35 of a key is functionally dependent on a
263:37 

263:37 part of another key, it is already bad
263:40 

263:40 enough to give you an update or delete
263:43 

263:43 anomaly. To address such an issue,
263:46 

263:46 Edgard and Raymond worked together to
263:49 

263:49 develop the voice called normal form. It
263:52 

263:52 requires a table to be in the third
263:54 

263:54 normal form first. Beyond that, it also
263:57 

263:57 requires that every determinant in a
263:60 

263:60 table to be a super key but nothing
264:03 

264:03 else. The new normal form successfully
264:06 

264:06 fixed the loophole discovered by
264:08 

264:08 Raymond.
264:09 

264:09 If a table is not in BCNF,
264:12 

264:12 it would fix its design by splitting it
264:15 

264:15 to two more smaller tables based on the
264:18 

264:18 violating functional dependency. In the
264:21 

264:21 example that we visited, it's the course
264:24 

264:24 ID determining room that violates the
264:27 

264:27 BCNF. So, we split them to two different
264:31 

264:31 tables
264:32 

264:32 and make the determinant the primary key
264:35 

264:35 in the new table. The original table
264:38 

264:38 should only keep those columns that are
264:40 

264:40 determined by super keys. Since course
264:44 

264:44 ID meets both conditions, it's going to
264:46 

264:46 be kept in both tables.
264:50 

264:50 That said, I need to confess that BCNF
264:54 

264:54 is rarely used in real world because
264:56 

264:56 it's very hard to find a table that
264:59 

264:59 violates only BCNF but not the third
265:02 

265:02 normal form. However, BCNF is still
265:05 

265:05 valuable to database series and the
265:08 

265:08 foundation of the field. The best part
265:11 

265:11 of this story is not just about BCNF or
265:15 

265:15 the loophole of the third normal form.
265:17 

265:17 It's about a people. Edgard initially
265:20 

265:20 resisted the idea that his definition of
265:22 

265:22 the third normal form was incomplete but
265:26 

265:26 eventually embraced the correction. And
265:28 

265:28 when the two successfully developed a
265:31 

265:31 new normal form that fixed the issue,
265:34 

265:34 Raymond's name came first in its name.
265:37 

265:37 BCNF may be rare in practice, but it
265:40 

265:40 represents something important. The
265:43 

265:43 pursuit of completeness even when the
265:45 

265:45 flaw is rare.
265:47 

265:47 In this video, we started from the tiny
265:50 

265:50 loophole of the third normal form and
265:52 

265:52 explored how it was discovered and fixed
265:56 

265:56 and ultimately led to the bo called
265:58 

265:58 normal form. I would love to hear what's
266:01 

266:01 working for you and what needs to
266:03 

266:03 change. This video is based on the book
266:06 

266:06 graing relational database design which
266:09 

266:09 dives deeper into database design using
266:11 

266:11 real world examples to explain the
266:14 

266:14 concepts. You can find the link to the
266:16 

266:16 book in the first comment. Think primary
266:19 

266:19 key and the unique constraints are
266:21 

266:21 basically the same thing. They are not
266:24 

266:24 and is a misunderstanding can quietly
266:26 

266:26 break your database design. Sure, both
266:30 

266:30 enforce uniqueness, but only one of them
266:33 

266:33 does something extra, something
266:34 

266:34 critical. and choosing the wrong one
266:37 

266:37 might mean your app would allow
266:39 

266:39 duplicated emails, break foreign key
266:42 

266:42 references, or silently fail when you
266:44 

266:44 try to insert new rows. In this video,
266:47 

266:47 we'll cover two types of constraints,
266:49 

266:49 the primary key and the unique
266:51 

266:51 constraints, as well as break down the
266:54 

266:54 real differences between the two with
266:56 

266:56 clear examples and recommended
266:58 

266:58 practices. By the end of this video,
267:01 

267:01 you'll know how and when to apply both
267:03 

267:03 of the two constraints when it comes to
267:06 

267:06 implementing your database design. Let's
267:09 

267:09 get started by defining constraints.
267:11 

267:11 First, what are constraints? Constraints
267:14 

267:14 are SQL rules that are being applied to
267:17 

267:17 columns in a table to ensure data
267:20 

267:20 accuracy and reliability.
267:22 

267:22 When you need to maintain the data
267:24 

267:24 integrity of your database or need to
267:27 

267:27 enforce some business rules, constraints
267:30 

267:30 will be your friend. A good way to think
267:33 

267:33 about constraints is to think about
267:35 

267:35 traffic rules for a city. Just as
267:37 

267:37 traffic lights, stop signs, and speed
267:40 

267:40 limits ensure cars move safely and
267:43 

267:43 orderly through intersections,
267:46 

267:46 constraints ensure data flows properly
267:48 

267:48 through your database tables. Let's look
267:51 

267:51 at some specific examples in terms of
267:54 

267:54 what constraints can do. Think about the
267:58 

267:58 database supporting an online store
267:60 

267:60 named as the sci-fi collective. In this
268:02 

268:02 database, there's a table representing
268:04 

268:04 users of the online store. There are a
268:07 

268:07 bunch of columns in this table that
268:09 

268:09 can't tolerate null values such as
268:12 

268:12 username, password, or the first and the
268:14 

268:14 last name of the user. This is something
268:17 

268:17 constraints can help with. Beyond that,
268:20 

268:20 in the same table, there are two columns
268:23 

268:23 that must contain unique values. The
268:26 

268:26 username column and a phone number
268:28 

268:28 columns. This is also something
268:30 

268:30 constraints can help enforce. This user
268:33 

268:33 table is related to another table
268:35 

268:35 representing payment methods of users.
268:38 

268:38 The link between the two tables is the
268:40 

268:40 email column in the payment method.
268:42 

268:42 Email is a foreign key in this table.
268:45 

268:45 Whenever a new row of payment method is
268:48 

268:48 inserted into the payment method table,
268:50 

268:50 it needs to reference a valid existing
268:53 

268:53 user. This is something a foreign key
268:56 

268:56 constraint can help ensure. In this
268:58 

268:58 video, we'll look at two types of
269:01 

269:01 constraints specifically. Starting with
269:03 

269:03 the primary key constraint. As you know,
269:07 

269:07 when it comes to database design, you
269:09 

269:09 probably have been told that you need to
269:12 

269:12 pick a primary key for every table and
269:14 

269:14 indicate the primary key clearly and
269:17 

269:17 explicitly like what you see here on the
269:19 

269:19 screen or this. When your design of an
269:23 

269:23 entity is translated to SQL code for the
269:26 

269:26 implementation purpose, the column that
269:28 

269:28 you pick as a primary key requires a
269:31 

269:31 primary key constraint. The primary key
269:33 

269:33 constraint ensures two things. First, it
269:37 

269:37 guarantees that no duplicate rows with
269:40 

269:40 the same primary key value exists in the
269:43 

269:43 same table. Second, it ensures that no
269:47 

269:47 row with a no primary key value can be
269:50 

269:50 inserted into the table. Without the
269:53 

269:53 primary key constraint, your peak of the
269:56 

269:56 primary key can't carry out the tasks a
269:58 

269:58 primary key is supposed to do. How do
270:01 

270:01 you implement a primary key constraint?
270:04 

270:04 Let's look at as a user table in the
270:06 

270:06 database supporting the same online
270:08 

270:08 store. Again, translating such a design
270:10 

270:10 directly to SQL code, I mean ignoring
270:13 

270:13 all the other requirements for
270:15 

270:15 constraints except for the primary key,
270:18 

270:18 you will have something like this. The
270:21 

270:21 primary key keyword is placed in the
270:24 

270:24 same line as the column that's
270:26 

270:26 designated as a primary key.
270:29 

270:29 Well, a better approach will be to name
270:32 

270:32 the primary key constraint explicitly.
270:36 

270:36 This is how you name a constraint.
270:38 

270:38 Starting with the keyword constraint,
270:41 

270:41 then followed by the constraint name of
270:44 

270:44 your pick. For every constraint or say
270:47 

270:47 every type of constraint that allow you
270:49 

270:49 to name it, the way that you approach
270:52 

270:52 naming will be the same. After you name
270:55 

270:55 the constraint, you will list the
270:57 

270:57 primary key keyword again and you need
271:01 

271:01 to wrap up the column in a bracket.
271:04 

271:04 If the primary key is a composite key,
271:07 

271:07 you will squeeze them all together into
271:10 

271:10 the same bracket and separate them using
271:13 

271:13 commas like what you see here on the
271:15 

271:15 screen. The purchase product table uses
271:18 

271:18 a composite primary key composed of two
271:21 

271:21 columns purchase ID and a product code.
271:25 

271:25 Why is it better to name a primary key
271:27 

271:27 constraint or say any constraints as
271:30 

271:30 long as naming is allowed? The name that
271:34 

271:34 you pick will make it easier for you to
271:36 

271:36 access it and change it in the future if
271:39 

271:39 you ever find it necessary. If you don't
271:42 

271:42 name a constraint, the database system
271:44 

271:44 you use will give it a default name.
271:47 

271:47 It's painful to retrieve the default
271:49 

271:49 name and the default name is typically
271:51 

271:51 very long and composed of random letters
271:54 

271:54 and digits that don't make much sense to
271:57 

271:57 human beings. As a matter of fact, the
271:59 

271:59 choice of not naming constraints
272:01 

272:01 explicitly can be a big inconvenience in
272:05 

272:05 the future and that will slow you down.
272:07 

272:07 As a result, it's recommended for you to
272:10 

272:10 name every type of constraint, even the
272:12 

272:12 primary key constraint. Of course, the
272:15 

272:15 caveat is naming of a constraint is
272:18 

272:18 allowed because there are certain
272:20 

272:20 constraints that don't allow you to name
272:22 

272:22 them at all like not null or default in
272:26 

272:26 some database systems, but that's a
272:28 

272:28 different story.
272:30 

272:30 The next type of constraint is unique.
272:34 

272:34 What does unique constraint do? Unique
272:37 

272:37 constraints ensures that all values in a
272:40 

272:40 column or a combination of columns are
272:42 

272:42 different. When do you want to use a
272:45 

272:45 unique constraint?
272:47 

272:47 When you have some nonkey columns in a
272:50 

272:50 table that should all contain unique
272:52 

272:52 values, it can be a simple column or a
272:55 

272:55 combination of columns. For example, a
272:59 

272:59 single column. Let's look at the user
273:01 

273:01 table again. In this table, the username
273:04 

273:04 column needs to all have unique values.
273:07 

273:07 Translating it to SQL code, it will look
273:10 

273:10 like this.
273:12 

273:12 We start by naming the constraint. After
273:15 

273:15 that, it is followed by the unique
273:17 

273:17 keyword and then the column will be
273:20 

273:20 wrapped up in the bracket.
273:23 

273:23 If you have a combination of columns
273:26 

273:26 that need to have unique values like the
273:28 

273:28 table that you see here, product table
273:31 

273:31 comes from the same database supporting
273:34 

273:34 the same online store. It's using
273:37 

273:37 product code as a primary key. Beyond
273:40 

273:40 the primary key, the combination of name
273:43 

273:43 and manufacturer of each product needs
273:45 

273:45 to be unique. Translating it to SQL
273:48 

273:48 code, you will have this. As you can see
273:51 

273:51 here, the syntax remains the same. We
273:54 

273:54 start by naming this constraint. After
273:57 

273:57 that, it is followed by the unique
273:59 

273:59 keyword. Then we use a bracket to wrap
274:03 

274:03 up the combination of name and
274:05 

274:05 manufacture columns. That's everything
274:07 

274:07 about the unique constraint.
274:10 

274:10 Now, let's compare the primary key
274:13 

274:13 constraint and a unique constraint to
274:15 

274:15 depen our understanding of both. Let's
274:18 

274:18 start by discussing the similarities
274:20 

274:20 between the two. Comparatively speaking,
274:24 

274:24 the similarities between the two
274:26 

274:26 constraints are straightforward.
274:28 

274:28 Both of them are used to prevent
274:30 

274:30 duplicated rows being inserted into a
274:33 

274:33 table. Both use similar data structures
274:36 

274:36 to guarantee quick lookup when it comes
274:39 

274:39 to checking duplications. Duplications
274:42 

274:42 don't only get generated during data
274:44 

274:44 insertion. Sometimes it can happen
274:46 

274:46 during update. So both of the two
274:49 

274:49 constraints will do their job
274:51 

274:51 accordingly whenever it's a data
274:53 

274:53 insertion or update operation. That's
274:56 

274:56 the similarity.
274:58 

274:58 Now let's talk about the differences
275:00 

275:00 between the two constraints.
275:03 

275:03 There are three major differences.
275:06 

275:06 The first one is easy to understand. The
275:10 

275:10 two constraints serve different goals
275:12 

275:12 and happen to have overlap between their
275:15 

275:15 functionalities. A primary key needs to
275:18 

275:18 be picked deliberately for every table
275:20 

275:20 that you design and it serves as the
275:23 

275:23 main identifier for each row in a table.
275:27 

275:27 But unique constraints are typically
275:29 

275:29 added due to some business requirements.
275:32 

275:32 We start our database design by
275:35 

275:35 identifying entities, attributes, and a
275:37 

275:37 primary keys for each entity. But we
275:40 

275:40 don't start the design with unique
275:42 

275:42 constraints. You get the idea.
275:45 

275:45 Second, a table can only have one
275:48 

275:48 primary key, but many different unique
275:51 

275:51 constraints. That's right. Sometimes you
275:54 

275:54 will have several nonkey columns that
275:56 

275:56 require the help from unique
275:58 

275:58 constraints. But no matter what you do,
276:01 

276:01 a table can only have one primary key.
276:04 

276:04 For example, let's look at the user
276:07 

276:07 table again. Beyond the primary key, I
276:10 

276:10 mean the email column, there are two
276:12 

276:12 nonkey columns that require a unique
276:15 

276:15 constraints, the username and a phone
276:18 

276:18 number. If you think about it, it makes
276:21 

276:21 sense as each user should have a
276:23 

276:23 different username and a phone number.
276:26 

276:26 When you translate such a database
276:27 

276:27 design to SQL code, you will have this.
276:31 

276:31 As you can see here, two unique
276:34 

276:34 constraints are defined on the two
276:36 

276:36 corresponding columns. In comparison, no
276:39 

276:39 matter what you do, you can only apply
276:41 

276:41 one primary key constraint per table.
276:45 

276:45 Third, the primary key constraint helps
276:48 

276:48 prevent null values, but the unique
276:50 

276:50 constraint typically doesn't. In most
276:54 

276:54 database systems, a column that has a
276:57 

276:57 unique constraint will allow many null
276:59 

276:59 values to be inserted such as my SQL or
277:02 

277:02 Postgress
277:04 

277:04 because a null value is not a real
277:06 

277:06 value. It's a representation of
277:09 

277:09 something is missing or say the value is
277:11 

277:11 missing or a column a field does not
277:15 

277:15 have a value at all.
277:18 

277:18 For example, if you revisit the user
277:21 

277:21 table that we have implemented a few
277:23 

277:23 slides ago and focus your attention on
277:26 

277:26 the phone number column. Think that the
277:29 

277:29 business logic of the online store
277:31 

277:31 database that contains this table allows
277:34 

277:34 null values for phone numbers. A unique
277:37 

277:37 constraint will serve it very well.
277:40 

277:40 When users register accounts with this
277:43 

277:43 online store initially, they are not
277:46 

277:46 asked to provide phone numbers
277:48 

277:48 immediately, but they need to eventually
277:51 

277:51 if they want to receive text alerts. The
277:54 

277:54 phone numbers they provide, however,
277:57 

277:57 must be unique per user. That's why a
277:60 

277:60 unique constraint serves the phone
278:02 

278:02 number column very well in this table.
278:05 

278:05 Another question that you need to ask
278:08 

278:08 and answer here is a scenario that
278:11 

278:11 involves a comparison between surrogate
278:13 

278:13 key and a composite key. How is that
278:17 

278:17 relevant to the two constraints that we
278:19 

278:19 discussed?
278:20 

278:20 In some cases, you would have a
278:23 

278:23 composite candidate key composed of a
278:26 

278:26 few columns. Regardless if you choose it
278:29 

278:29 as a primary key for the table, it has
278:32 

278:32 to be unique as a combination like the
278:35 

278:35 example that you see here on the screen.
278:38 

278:38 A product table in this database
278:40 

278:40 supporting the same online store. The
278:43 

278:43 combination of the name and a
278:44 

278:44 manufacturer columns need to be unique.
278:48 

278:48 So the two columns can be designated as
278:52 

278:52 the primary key or you could go with a
278:55 

278:55 surrogate key for the table and add a
278:58 

278:58 unique constraint to the two columns as
279:00 

279:00 a combination. Both primary keys would
279:04 

279:04 work fine. But which one is better from
279:06 

279:06 the perspective of database design and
279:09 

279:09 performance? A short answer is that
279:12 

279:12 surrogate keys are typically preferred
279:14 

279:14 in such a scenario. A surrogate key is
279:17 

279:17 easier to index and takes less space to
279:20 

279:20 build a data structure that prevents
279:22 

279:22 duplications. Beyond that, when the
279:25 

279:25 table is related to other tables and
279:28 

279:28 when the primary key of the product
279:30 

279:30 table needs to be referenced elsewhere
279:33 

279:33 in other tables, a surrogate key will be
279:36 

279:36 easier to use and more efficient than a
279:38 

279:38 complex composite key. That said, if you
279:42 

279:42 face a dilemma like thus, think about
279:45 

279:45 going with a surrogate key. In this
279:47 

279:47 video, we covered and compared the
279:49 

279:49 primary key and unique constraints. If
279:52 

279:52 you find this video helpful, give it a
279:54 

279:54 thumb up, subscribe, and leave your
279:56 

279:56 questions in the comments. I would love
279:59 

279:59 to hear what's working for you and what
280:01 

280:01 needs to change. This video is based on
280:03 

280:03 the booking relational database design
280:06 

280:06 which dives deeper into database design
280:08 

280:08 using real world examples to explain
280:11 

280:11 concepts. You can find the link to the
280:13 

280:13 book in the first comment. Most SQL
280:16 

280:16 learners know about foreign keys, but a
280:19 

280:19 few know about how to fine-tune what
280:21 

280:21 happens to a record that's being
280:23 

280:23 referenced when you perform a data
280:25 

280:25 delete or update. Let's fix that. In
280:29 

280:29 this video, we'll cover the foreign key
280:31 

280:31 constraint and the referential actions.
280:34 

280:34 By the end, you'll have a deeper
280:36 

280:36 understanding of how to use foreign key
280:38 

280:38 constraint effectively in implementing
280:41 

280:41 database designs.
280:43 

280:43 In case if your memory on foreign keys
280:45 

280:45 become vague, a foreign key can be
280:48 

280:48 thought as a link between two related
280:50 

280:50 tables. A foreign key is simply one or
280:53 

280:53 more columns in a table that reference
280:56 

280:56 the primary key of another table. The
280:59 

280:59 two tables are of course related. For
281:02 

281:02 example, think about the database
281:05 

281:05 supporting an online store named as the
281:07 

281:07 sci-fi collective.
281:09 

281:09 There are two tables in this database
281:12 

281:12 and they are related to each other. One
281:14 

281:14 table represents products sold by the
281:17 

281:17 online store while the other table
281:19 

281:19 represents product reviews. The product
281:22 

281:22 table uses the product code as a primary
281:26 

281:26 key and the review table uses review ID
281:30 

281:30 as its primary key. The two tables are
281:33 

281:33 in a one to many relationship. The
281:36 

281:36 foreign key is in the many side or say
281:38 

281:38 the review table that makes the product
281:41 

281:41 table the parent table and the review
281:44 

281:44 table the child one. In this
281:46 

281:46 relationship,
281:48 

281:48 the same review table is also related to
281:51 

281:51 the user table in the database. User
281:54 

281:54 table uses the email column as the
281:56 

281:56 primary key. The relationship between
281:59 

281:59 user and review is another one to many
282:02 

282:02 relationship in which the review table
282:05 

282:05 is on the many side. So the foreign key
282:07 

282:07 email goes into the reveal table. That
282:10 

282:10 makes the user table the parent one and
282:13 

282:13 the reveal table the child one. In this
282:15 

282:15 relationship,
282:17 

282:17 as you can see, the review table
282:20 

282:20 contains two different foreign keys.
282:22 

282:22 This is common in database design. How
282:25 

282:25 do you translate the design of foreign
282:27 

282:27 keys or say the review table into SQL
282:30 

282:30 code when it comes to implementing your
282:32 

282:32 database design? The recommended
282:35 

282:35 approach is to name the two constraints
282:38 

282:38 and define them individually like this.
282:42 

282:42 The keyword constraint allows you to
282:45 

282:45 name a constraint. The definition of the
282:47 

282:47 foreign key constraint starts with the
282:50 

282:50 foreign key keyword. After that, you
282:52 

282:52 need to wrap up the target foreign key
282:55 

282:55 column in a bracket. After that, you
282:58 

282:58 need to specify which primary key is
283:01 

283:01 referenced by this foreign key via the
283:04 

283:04 references keyword. It will be followed
283:07 

283:07 by the related table and its primary
283:10 

283:10 key. The primary key goes into another
283:13 

283:13 bracket. Does the foreign key constraint
283:15 

283:15 do anything? Of course, if you try to
283:19 

283:19 insert a new row of review data into the
283:22 

283:22 review table, but references and
283:24 

283:24 non-existing user, SQL will complain and
283:28 

283:28 stop you from doing that.
283:30 

283:30 Similarly, if you are trying to delete a
283:33 

283:33 user from the user table, but that user
283:36 

283:36 is referenced by several rows in the
283:38 

283:38 review table, it will be stopped.
283:42 

283:42 Or if you are trying to update the
283:44 

283:44 primary key value of a user who is
283:47 

283:47 referenced in the reveal table, it will
283:49 

283:49 also be stopped. That's the power of the
283:52 

283:52 foreign key constraint. However, that
283:55 

283:55 may not be what you truly want to happen
283:58 

283:58 when it comes to deleting or updating a
284:00 

284:00 row of data in the parent table like
284:04 

284:04 user or product. Sometimes when we need
284:08 

284:08 to delete or update a row in the parent
284:10 

284:10 table, we want the referenced row in the
284:13 

284:13 child table to be deleted as well. Or
284:17 

284:17 sometimes we want to keep those rows in
284:20 

284:20 the child table but make the referenced
284:23 

284:23 foreign key value to be set to null so
284:26 

284:26 that data integrity of the tables can
284:29 

284:29 still be maintained. Is that something
284:31 

284:31 can be done in SQL? Yes, that's where
284:35 

284:35 referential actions come into play. So
284:38 

284:38 what are referential actions?
284:40 

284:40 Referential actions are about handling
284:43 

284:43 what happened to all the related rows in
284:45 

284:45 the child table when a referenced row in
284:48 

284:48 the parent table is updated or deleted.
284:53 

284:53 For example, think about the product and
284:56 

284:56 review table. In this relationship, the
284:59 

284:59 product table is a parent table and the
285:02 

285:02 review table is a child table. When you
285:05 

285:05 need to delete or update the primary key
285:08 

285:08 of a row in the product table, but it
285:11 

285:11 happens to be referenced by many rows in
285:13 

285:13 the review table, you can specify
285:15 

285:15 whether such an action is allowed or
285:17 

285:17 not, and what should happen to the rows
285:20 

285:20 in the review table that references the
285:22 

285:22 particular product you wanted to delete
285:24 

285:24 or update.
285:26 

285:26 There are three types of referential
285:28 

285:28 actions that are commonly used,
285:31 

285:31 including restrict, cascade, and set.
285:34 

285:34 Now let's talk about each of them one
285:36 

285:36 after another. Restrict prevents any
285:39 

285:39 modifications or deletions of a parent
285:42 

285:42 record if dependent child record exists.
285:46 

285:46 For example, the database will throw an
285:49 

285:49 error if you try to delete a product row
285:52 

285:52 who is referenced in the reveal table.
285:55 

285:55 Restrict is the most conservative
285:57 

285:57 approach that helps prevent accidental
286:00 

286:00 data loss. You may see that I put a no
286:03 

286:03 action in a bracket right by restrict.
286:07 

286:07 Restrict and no actions are actually two
286:10 

286:10 different types of referential actions
286:12 

286:12 but they behave almost the same. No
286:15 

286:15 action also stops any modification or
286:18 

286:18 deletion of parent records if dependent
286:21 

286:21 child records exist. The only difference
286:24 

286:24 between the two is timing which doesn't
286:26 

286:26 matter much here. So for simplicity we
286:29 

286:29 will sync them as one type of
286:31 

286:31 referential actions.
286:33 

286:33 When you create a foreign key constraint
286:36 

286:36 either restrict or no action is used by
286:38 

286:38 default. That said as soon as you create
286:42 

286:42 a foreign key constraint and don't do
286:45 

286:45 anything extra restrict or no action is
286:49 

286:49 already enabled. Of course, you can also
286:52 

286:52 choose to be more explicit about it that
286:55 

286:55 requires you to use two clauses,
286:57 

286:57 unddelete and on update. Undelet is a
287:01 

287:01 clause that defines the referential
287:03 

287:03 action type when a parent record is
287:06 

287:06 deleted. On update is a clause that
287:09 

287:09 defines the referential action type to
287:11 

287:11 use when a parent record is updated.
287:14 

287:14 Both need to be followed by some
287:16 

287:16 referential action type. Such clauses
287:19 

287:19 need to be placed in the definition of
287:21 

287:21 your foreign key constraints. That's
287:24 

287:24 everything about the most conservative
287:26 

287:26 type of referential action. However, it
287:29 

287:29 may not be what you want to happen. In
287:31 

287:31 some cases, you want your delete or
287:33 

287:33 update query to be executed on the
287:36 

287:36 parent table and that requires the help
287:38 

287:38 from other types of referential actions.
287:42 

287:42 Cascade. If this referential action is
287:44 

287:44 chosen, when a record in the parent
287:47 

287:47 table is updated or deleted, the
287:49 

287:49 corresponding change will automatically
287:52 

287:52 propagate to all related records in the
287:55 

287:55 child table. For example,
287:58 

287:58 if you delete a product, all the
288:00 

288:00 referenced reviews are also going to be
288:03 

288:03 deleted.
288:04 

288:04 This maintains a consistency, but can
288:07 

288:07 also result in widespread data loss if
288:10 

288:10 not used carefully. If this is what's
288:13 

288:13 required by the business logic, you need
288:16 

288:16 to specify it accordingly. When it comes
288:18 

288:18 to defining your foreign key constraint
288:21 

288:21 like this,
288:23 

288:23 the cascade should follow the unddelete
288:26 

288:26 and unupdate clauses and they should be
288:30 

288:30 placed in the foreign key constraint
288:32 

288:32 definition. Just for the record, on
288:35 

288:35 delete and on update are totally
288:38 

288:38 independent and they don't have to take
288:40 

288:40 the same type of referential actions and
288:44 

288:44 everything is up to the business logic.
288:46 

288:46 For example, if you want to use cascade
288:49 

288:49 to handle all updates but don't want
288:52 

288:52 delete to happen in the same manner, you
288:54 

288:54 can go with cascade for the unupdate
288:57 

288:57 clause but restrict for unddelete clause
289:01 

289:01 like this.
289:03 

289:03 The third commonly used referential
289:05 

289:05 actions is set. Now if this referential
289:08 

289:08 action is used when a parent record is
289:11 

289:11 deleted or its key value is updated, the
289:14 

289:14 foreign key values in related child
289:17 

289:17 records will be set to null. As you can
289:20 

289:20 imagine, this would require the foreign
289:23 

289:23 key column to allow null values. For
289:26 

289:26 example, if set now is used as the
289:29 

289:29 referential action type, when the
289:32 

289:32 product record is deleted, any review
289:35 

289:35 rules that reference the product record
289:38 

289:38 will have their product code value set
289:40 

289:40 to null. To use this type of referential
289:43 

289:43 actions for delete, set now should
289:46 

289:46 follow the unddelete clause in the
289:49 

289:49 corresponding foreign key constraint. As
289:51 

289:51 you can see from this example, on update
289:54 

289:54 is followed by cascade which handles the
289:57 

289:57 modification in the parent table
289:59 

289:59 differently from the delion.
290:02 

290:02 In this video, we covered what is the
290:04 

290:04 foreign key constraint is and how
290:07 

290:07 referential actions work. If you find
290:09 

290:09 this video helpful, give it a thumb up,
290:12 

290:12 subscribe, and leave your questions in
290:14 

290:14 the comments. I would love to hear
290:16 

290:16 what's working for you and what needs to
290:18 

290:18 change. This video is based on the book
290:21 

290:21 Groing Relational Database Design which
290:23 

290:23 dives deep into database design using
290:26 

290:26 real world examples.
290:28 

290:28 You'll find the link to the book in the
290:30 

290:30 first comment.
290:32 

290:32 Most developers think constraints like
290:35 

290:35 not now default or check are just nice
290:38 

290:38 to have optional at best. But in
290:41 

290:41 reality, ignoring them can silently
290:44 

290:44 corrupt your database. Let me show you
290:46 

290:46 what I mean. Imagine a column that
290:49 

290:49 allows null values for a status column.
290:52 

290:52 What does a null status even mean? Is it
290:56 

290:56 a pending, failed, or forgotten? You
290:59 

290:59 don't know. And that ambiguity can
291:01 

291:01 ripple through your application. Or take
291:04 

291:04 check constraints as another example.
291:07 

291:07 They are your last line of defense
291:09 

291:09 against invalid data unless you skip
291:13 

291:13 them and let a negative salary numbers
291:16 

291:16 leap into your system.
291:18 

291:18 In this video, we'll break down how
291:20 

291:20 these three constraints really work and
291:23 

291:23 how to use them to make your SQL code
291:25 

291:25 safer, smarter, and robust. By the end,
291:29 

291:29 you'll know how to use the three
291:31 

291:31 constraints effectively to enforce
291:33 

291:33 business logics when it comes to
291:35 

291:35 translating your database design to SQL
291:38 

291:38 code. Let's start with the not now
291:41 

291:41 constraint. A null value represents a
291:44 

291:44 value that's missing or unknown. It can
291:47 

291:47 cause unexpected troubles when you are
291:49 

291:49 not aware. For example, when you are
291:53 

291:53 trying to do some aggregation
291:54 

291:54 calculation but accidentally involves
291:57 

291:57 even a single null value, the result can
291:60 

291:60 be null. The not now constraint helps
292:03 

292:03 ensure that a column doesn't accept the
292:06 

292:06 null values in SQL. Whether you are
292:09 

292:09 talking about data insertion or update,
292:12 

292:12 it doesn't matter that a column that
292:15 

292:15 comes with a not null constraint will
292:17 

292:17 not accept any null values. The usage of
292:21 

292:21 not now constraint is simple. For
292:24 

292:24 example, think about a table
292:26 

292:26 representing users in a database
292:28 

292:28 supporting an online store which has
292:30 

292:30 quite a few numbers of columns that
292:33 

292:33 shouldn't accept null values except for
292:36 

292:36 the phone number column. Translating
292:39 

292:39 this business logic to SQL code, you
292:42 

292:42 will have something like this. As you
292:44 

292:44 can see here, the code snippet lists not
292:47 

292:47 now constraint in the same line as soon
292:51 

292:51 as a column is defined. You may wonder
292:54 

292:54 if not now is a constraint. Why we don't
292:57 

292:57 name them in the same manner as other
292:60 

292:60 constraints like the primary key
293:02 

293:02 constraint or foreign key constraint?
293:05 

293:05 People who design database systems see
293:07 

293:07 not now as a fundamental column property
293:10 

293:10 like data type or length. Same as how
293:14 

293:14 you don't name the data type of a
293:16 

293:16 column, you don't need to name a notnull
293:18 

293:18 constraint either. As a matter of fact,
293:21 

293:21 most database systems don't support
293:24 

293:24 naming of not null constraints at all.
293:27 

293:27 Beyond that, you may notice that we gave
293:30 

293:30 a notnow constraint in the primary key
293:33 

293:33 column. I mean the email column. Is that
293:37 

293:37 really necessary?
293:39 

293:39 Technically speaking, it's redundant
293:41 

293:41 because if the primary key constraint is
293:44 

293:44 applied to a column, that column will
293:47 

293:47 not accept no values. However, this is
293:51 

293:51 still a recommended practice because if
293:53 

293:53 you change the primary key constraint in
293:55 

293:55 the future, you don't need to worry
293:58 

293:58 about where to patch up in terms of the
294:00 

294:00 not now constraint. In other words, this
294:04 

294:04 small redundancy is future proof. Last,
294:08 

294:08 not every column requires the not now
294:10 

294:10 constraints. I'm not talking about the
294:12 

294:12 business logic. Sometimes it can come
294:15 

294:15 from the requirement of an optional
294:18 

294:18 relationship. For example, think about
294:21 

294:21 the relationship between customers and
294:23 

294:23 automoils in a database supporting car
294:25 

294:25 dealerships software. The two entities
294:28 

294:28 are in a oneto many relationship. A
294:31 

294:31 customer can correspond to many cars and
294:34 

294:34 the foreign key goes to the many side or
294:37 

294:37 say in the automobile entity. Yet both
294:41 

294:41 main cardalities of this relationship
294:43 

294:43 are zero which makes the relationship
294:46 

294:46 optional. The implication of such a
294:49 

294:49 design is that one row of automo data is
294:53 

294:53 added, it doesn't have to be associated
294:55 

294:55 with a customer at all, which will
294:58 

294:58 translate to the foreign key column in
295:00 

295:00 the automo table allowing no values. In
295:04 

295:04 summary, optional relationship or say
295:07 

295:07 two main cardalities as zero would
295:10 

295:10 translate to foreign key columns that
295:12 

295:12 allow null values. In such a case, you
295:16 

295:16 need to be careful and shouldn't apply
295:18 

295:18 not now constraints to every column
295:21 

295:21 without thinking. That's everything
295:23 

295:23 about the not now constraint. Let's move
295:26 

295:26 to the default constraint. Now,
295:29 

295:29 the default constraint is used to ensure
295:31 

295:31 that a column always has a value. The
295:34 

295:34 value is either specified by the user or
295:37 

295:37 the constraint. Well, it's more often
295:40 

295:40 specified by the constraint rather than
295:42 

295:42 the user. Let's look at an example.
295:46 

295:46 Think about two related tables, purchase
295:48 

295:48 and a payment method from a database
295:51 

295:51 supporting an online store. The purchase
295:53 

295:53 table represents the order records and
295:56 

295:56 the payment method table represents the
295:59 

295:59 payment methods used by users. There's
296:02 

296:02 one column in the purchase table named
296:05 

296:05 the purchase time which is used to
296:07 

296:07 record the exact time when a user puts
296:11 

296:11 an order online. This is a perfect
296:13 

296:13 scenario for you to use the default
296:16 

296:16 constraint. Instead of relying solely on
296:19 

296:19 the software developers to do the right
296:21 

296:21 thing, you can lessen their burden by
296:24 

296:24 setting the default value of the
296:25 

296:25 purchase time to be the time when a new
296:28 

296:28 row of purchase data is added. The
296:32 

296:32 syntax of the default constraint is
296:34 

296:34 simple.
296:35 

296:35 Add the default value to the same line
296:37 

296:37 of your column definition and make sure
296:40 

296:40 to provide the proper target value like
296:43 

296:43 this. The current time stamp stands for
296:46 

296:46 the exact time when new row is inserted
296:50 

296:50 into the table.
296:52 

296:52 Additionally, we chose not to name this
296:55 

296:55 constraint. It's worth noting that some
296:57 

296:57 database systems see default constraints
296:60 

296:60 in the same way as not null. I mean as
297:03 

297:03 an integral part of a column definition.
297:06 

297:06 So they don't support the naming of such
297:08 

297:08 a constraint like my while some others
297:12 

297:12 do. If you're thinking about naming a
297:14 

297:14 default constraint, you can start with a
297:17 

297:17 simple question to generative AI tools
297:20 

297:20 and further look it up in the
297:22 

297:22 corresponding database system menu. So
297:25 

297:25 when would you find yourself in a
297:26 

297:26 situation that needs the help from the
297:28 

297:28 default constraint?
297:31 

297:31 When you have a column that stores
297:32 

297:32 timestamp, a numeric column that you
297:35 

297:35 might perform aggregation on later, or
297:38 

297:38 an optional column that holds
297:39 

297:39 placeholder values, the most common
297:42 

297:42 scenario is definitely the first one.
297:44 

297:44 So, let's dive deeper into that by
297:47 

297:47 answering two questions. First, does the
297:50 

297:50 default constraint replace not now
297:53 

297:53 constraint? Remember we looked at this
297:56 

297:56 table for the purchase time column in
297:59 

297:59 the purchase table. We added two
298:02 

298:02 constraints not now and the default.
298:05 

298:05 They can't replace each other. The
298:07 

298:07 default constraint can't prevent users
298:10 

298:10 from adding null values to the
298:12 

298:12 corresponding column. So the answer to
298:14 

298:14 this question is a no. If you need to
298:17 

298:17 prevent null values, the not constraint
298:20 

298:20 is definitely needed.
298:23 

298:23 The second question has something to do
298:25 

298:25 with the commonly used default values
298:28 

298:28 for time stamps current time stamp. A
298:32 

298:32 common confusion on this is whether
298:35 

298:35 current time stamp adopts coordinated
298:38 

298:38 universal time or the UTC automatically.
298:42 

298:42 Well, the answer to this question has
298:44 

298:44 two parts. First, it's safe to say that
298:46 

298:46 the current time stamp is widely
298:49 

298:49 supported by most database systems such
298:52 

298:52 as MySQL, Maran DB, SQL, and a
298:55 

298:55 Postgress. However,
298:58 

298:58 when it comes to whether current time
299:00 

299:00 stamp adopts UTC or not is a totally
299:04 

299:04 different story. In some database
299:07 

299:07 systems like MySQL, current time stamp
299:10 

299:10 adopts UTC automatically.
299:13 

299:13 So default current time stamp will
299:15 

299:15 support auto time zone conversion
299:17 

299:17 without any issues. But in some other
299:20 

299:20 systems like postgress things are a
299:24 

299:24 little bit complicated. Whether current
299:26 

299:26 time stamp adopts UTC automatically or
299:29 

299:29 not depends on what data type you choose
299:32 

299:32 for the corresponding column. There are
299:35 

299:35 two different flavors for the data type
299:37 

299:37 time step. One comes with auto time zone
299:41 

299:41 conversion and adopts UTC automatically
299:45 

299:45 which is a timestamp voice time zone
299:48 

299:48 while the other does not. If you are
299:50 

299:50 dealing with a database supporting users
299:53 

299:53 across different time zones, you want to
299:56 

299:56 use the one If you are dealing with a
299:58 

299:58 database supporting users across
300:01 

300:01 different time zones, you want timestamp
300:04 

300:04 with time zone like this. You need to
300:08 

300:08 specify the data type as time stamp with
300:11 

300:11 time zone. By doing that, the default
300:14 

300:14 current time stamp will do the job as is
300:17 

300:17 intended.
300:18 

300:18 How to create a column representing the
300:21 

300:21 current time stamp and a support time
300:23 

300:23 zone conversion is one of those things
300:26 

300:26 that you want to double check by talking
300:28 

300:28 to a generative AI tool and further look
300:31 

300:31 things up in the menu of the
300:33 

300:33 corresponding database system.
300:36 

300:36 That's everything about the default
300:38 

300:38 constraint. Let's move to the check
300:41 

300:41 constraint. Now check constraints are to
300:44 

300:44 some extent more capable and complex in
300:48 

300:48 comparison with not now and default
300:50 

300:50 constraints.
300:52 

300:52 Let's start with a simple question. What
300:54 

300:54 does a check constraint do? A check
300:57 

300:57 constraint is a rule that specifies a
301:00 

301:00 condition for a column so that each row
301:04 

301:04 must meet this condition to be
301:06 

301:06 considered valid for this column. For
301:09 

301:09 example, think about the table
301:12 

301:12 representing persons. The table comes
301:15 

301:15 with a few columns like person ID, last
301:18 

301:18 name, first name, and age. The valid age
301:22 

301:22 number is between zero and 120. The
301:25 

301:25 check constraint can check the validity
301:28 

301:28 of age values based on such a rule and
301:31 

301:31 reject any data insertion or update that
301:34 

301:34 is considered invalid.
301:37 

301:37 When do you need to use check
301:38 

301:38 constraints?
301:40 

301:40 When you need to enforce business logic,
301:43 

301:43 set valid value ranges or predefined
301:46 

301:46 data sets or validate basic pattern. You
301:50 

301:50 can always rely on the help from the
301:52 

301:52 check constraints. Let's look at these
301:55 

301:55 scenarios one after another. First,
301:58 

301:58 enforcing business rules. As you conduct
302:02 

302:02 the requirement analysis for database
302:04 

302:04 design, you may gather some information
302:06 

302:06 about certain columns in an entity.
302:09 

302:09 Translating such information to SQL code
302:12 

302:12 requires the help from the check
302:14 

302:14 constraint.
302:16 

302:16 For example, think about a table
302:18 

302:18 representing book loans in a database
302:21 

302:21 supporting libraries.
302:24 

302:24 There are a few columns representing
302:26 

302:26 borrow date, due date, and return date
302:28 

302:28 of books. The rules that you gathered
302:32 

302:32 require that the due date to be later
302:34 

302:34 than the borrow date and as a return
302:37 

302:37 date to be the same or later than the
302:39 

302:39 borrow date. Enforcing such rules
302:41 

302:41 require the help from the check
302:43 

302:43 constraint like this. As you can see
302:46 

302:46 here, we started the definition of this
302:49 

302:49 constraint by naming it first. After
302:52 

302:52 that, we made two rules and concatenated
302:56 

302:56 them using the end logical operator.
302:59 

302:59 Both rules are wrapped up in the bracket
303:02 

303:02 that follows the check keyword. The
303:05 

303:05 first rule says that due date needs to
303:08 

303:08 be later than the borrow date. And the
303:10 

303:10 second rule says that the return date
303:13 

303:13 needs to be the same or later than the
303:15 

303:15 borrow date. Such business rules won't
303:18 

303:18 get reflected in database design. But if
303:21 

303:21 they are enforced during database
303:23 

303:23 implementation, they need the help from
303:25 

303:25 the check constraint.
303:28 

303:28 The second scenario is range validation.
303:32 

303:32 When you have some columns that are
303:34 

303:34 numeric data, their data may not make
303:37 

303:37 much sense when they are not in a
303:39 

303:39 certain range. Although requirement
303:42 

303:42 analysis may not reveal such information
303:44 

303:44 explicitly, a qualified database
303:47 

303:47 designer can catch such issues and
303:50 

303:50 translate them into constraints.
303:52 

303:52 Remember the person table and age column
303:55 

303:55 that we visited a few slides ago. The
303:58 

303:58 requirement analysis of the database may
304:01 

304:01 not contain anything about valid data
304:04 

304:04 range of age, but it's not hard to
304:07 

304:07 deduce a valid range for human age and
304:10 

304:10 translated to a check constraint
304:13 

304:13 like this.
304:15 

304:15 For another example, think about a
304:18 

304:18 product table from a database supporting
304:20 

304:20 an online store. Its price column can
304:23 

304:23 use some help from a check constraint.
304:26 

304:26 If the price range is known to you, for
304:29 

304:29 example, between 1 cent and 1 cent short
304:32 

304:32 of a million, the check constraint may
304:35 

304:35 look like this.
304:37 

304:37 If the upper range is unknown, it could
304:40 

304:40 simply be like this.
304:45 

304:45 The next scenario is least validation.
304:48 

304:48 Sometimes you have a column whose valid
304:51 

304:51 values are limited to a small and a
304:54 

304:54 predefined set like transaction status
304:57 

304:57 or gender selection. In such a case, a
305:01 

305:01 check constraint can prevent invalid
305:04 

305:04 data from being added into the database.
305:07 

305:07 For example, think about a table
305:09 

305:09 representing user address in a database
305:11 

305:11 supporting online store that only
305:14 

305:14 operates in US. So only 51 state names I
305:18 

305:18 mean 50 state names and a DC should be
305:21 

305:21 considered valid. To make sure invalid
305:25 

305:25 data won't be added into a table
305:27 

305:27 accidentally like this you can apply a
305:30 

305:30 check constraint to this column like
305:33 

305:33 this. A list validation requires the
305:36 

305:36 help from the in keyword. The syntax is
305:39 

305:39 relatively straightforward stating that
305:42 

305:42 is followed by 51 state names while the
305:45 

305:45 51 state names are being wrapped up in a
305:48 

305:48 bracket. Of course, everything still
305:51 

305:51 needs to be wrapped up inside a bracket
305:54 

305:54 that follows the check keyword.
305:57 

305:57 The next scenario that often requires
305:60 

305:60 the help from check constraints is
306:02 

306:02 format validation.
306:05 

306:05 When it comes to pattern matching or
306:07 

306:07 format validation, SQL is not as strong
306:10 

306:10 as imperative programming languages. But
306:13 

306:13 you can still do many necessary and
306:15 

306:15 basic checking so that you can have this
306:18 

306:18 last line of defense to lessen the
306:21 

306:21 burden on the back end and front- end
306:24 

306:24 developers. One caveat about doing
306:27 

306:27 format validation using check
306:29 

306:29 constraints is inconsistency across
306:32 

306:32 different database systems. Let's look
306:35 

306:35 at an example to demonstrate that. Think
306:38 

306:38 about the same user address table that
306:40 

306:40 we've visited. The data type of postal
306:43 

306:43 code is car 5 which means a fixed size
306:46 

306:46 string. If we want to apply a check
306:49 

306:49 constraint to further limit it to
306:52 

306:52 exactly five characters long, the
306:55 

306:55 constraint may look like this.
306:58 

306:58 You may wonder whether this is redundant
306:60 

306:60 because car 5 can already handle this
307:03 

307:03 task by itself. Indeed, it's
307:06 

307:06 unnecessary.
307:07 

307:07 But this is one of the few pattern
307:09 

307:09 matching syntax that is consistent
307:12 

307:12 across different database systems.
307:15 

307:15 Specifically, I'm talking about the like
307:17 

307:17 keyword. It is followed by five
307:20 

307:20 underscores that are wrapped up in a
307:23 

307:23 single quotation mark. That's how you
307:25 

307:25 express five characters. Can we go one
307:29 

307:29 step further by limiting the characters
307:31 

307:31 to be digitally? Yes, we can. But the
307:35 

307:35 syntax for that part will vary from one
307:38 

307:38 database system to another. For example,
307:41 

307:41 in my SQL, it will look like this. By
307:44 

307:44 taking advantage of regular expression
307:47 

307:47 in postgress you need to take a
307:50 

307:50 different approach like what is it here
307:51 

307:51 on the screen I'm not asking you to
307:54 

307:54 master format validation here instead
307:57 

307:57 I'm asking you to have some awareness
307:59 

307:59 that you can do basic format validation
308:01 

308:01 in SQL as well but the syntax varies
308:04 

308:04 from one database system to another.
308:08 

308:08 It's actually where generative AIS can
308:10 

308:10 help quite a lot as long as your
308:12 

308:12 question is explicit about which
308:15 

308:15 database system you are dealing with.
308:17 

308:17 That's everything about using check
308:19 

308:19 constraints for format validation.
308:22 

308:22 In this video, we covered three
308:24 

308:24 constraints that can be handy during
308:27 

308:27 database implementation including not
308:29 

308:29 null, default, and check. What they are
308:33 

308:33 and when to use each of them. If you
308:36 

308:36 find this video helpful, give it a thumb
308:38 

308:38 up, subscribe, and leave your questions
308:41 

308:41 in the comments. I would love to hear
308:43 

308:43 what's working for you and what needs to
308:46 

308:46 change. This video is based on the book
308:48 

308:48 graing relational database design, which
308:51 

308:51 dives deeper into database design using
308:53 

308:53 real world examples to explain the
308:55 

308:55 concepts. You can find the link to the
308:58 

308:58 book in the first comment. Databases
309:01 

309:01 store the most valuable assets of modern
309:03 

309:03 systems, user credentials, financial
309:06 

309:06 data, health records, or even internal
309:09 

309:09 company secrets. A breach doesn't just
309:12 

309:12 hurt your users. It can ruin your
309:14 

309:14 company, your reputation, and your legal
309:17 

309:17 standing. So, how do we keep this
309:19 

309:19 information safe in your database?
309:22 

309:22 Broadly speaking, we rely on two lines
309:24 

309:24 of defense. Access control that governs
309:27 

309:27 who can interact with the data. An
309:31 

309:31 encryption that protects what the data
309:33 

309:33 looks like if someone does access it.
309:36 

309:36 Access control decides who can use your
309:39 

309:39 data. Encryption decides whether anyone
309:42 

309:42 can understand it. Miss either one, your
309:45 

309:45 database will become a toast.
309:48 

309:48 In this video, we'll unpack these two
309:50 

309:50 pillars of database security, access
309:53 

309:53 control and encryption, and show you how
309:56 

309:56 they work together to protect sensitive
309:58 

309:58 data from misuse, licks, and attacks.
310:02 

310:02 Let's start with access control. Access
310:05 

310:05 control governs who can do what in a
310:07 

310:07 database. It prevents unauthorized users
310:11 

310:11 from reading, modifying, or deleting
310:13 

310:13 data. There are three types of access
310:16 

310:16 control that you should be aware of
310:19 

310:19 including discretionary control,
310:21 

310:21 mandatory access control and the role
310:23 

310:23 based access control.
310:26 

310:26 Discretionary access control is the
310:28 

310:28 simplest one but it only works for a
310:31 

310:31 small application or toy project because
310:34 

310:34 it doesn't scale very well. This type of
310:37 

310:37 access control is about the database
310:40 

310:40 admin manually determining who can
310:43 

310:43 access the database and what permissions
310:45 

310:45 they have. So there's not much to say
310:49 

310:49 about it. In contrast, the two other
310:52 

310:52 types of access control deserve some
310:55 

310:55 good explanation.
310:57 

310:57 Mandatory access control is about
310:60 

310:60 assigning a hierarchical levels of
311:02 

311:02 clearance to databases, tables, columns
311:05 

311:05 or even rows. And at the same time, it
311:09 

311:09 will assign the same hierarchical levels
311:12 

311:12 of clearance to its users. A user can
311:15 

311:15 only access some data if his or her
311:18 

311:18 assigned clearance level matches or
311:21 

311:21 exceeds that of the corresponding data.
311:24 

311:24 For example, think that we are running a
311:27 

311:27 database of a higher education
311:29 

311:29 institution and define three clearance
311:32 

311:32 levels public, sensitive and
311:35 

311:35 confidential. We then assign them to
311:38 

311:38 different tables in the database. Tables
311:41 

311:41 like student and a course are assigned
311:43 

311:43 as a public clearance level. Grade is
311:46 

311:46 classified as sensitive and medical is
311:49 

311:49 classified as confidential. They would
311:52 

311:52 also assign such clearance levels to
311:55 

311:55 different users who need to access the
311:57 

311:57 database.
311:59 

311:59 Only when one's clearance level is the
312:02 

312:02 same or higher than the corresponding
312:03 

312:03 table, the user can access its data. In
312:07 

312:07 short, mandatory access control is about
312:10 

312:10 managing and assigning a hierarchical of
312:13 

312:13 clearance levels to both users and
312:15 

312:15 tables.
312:17 

312:17 Mandatory access control is mostly used
312:19 

312:19 in government, military and other high
312:22 

312:22 security environments where protecting
312:25 

312:25 classified information is critical.
312:28 

312:28 In comparison, row based access control
312:31 

312:31 is about assigning access permissions to
312:34 

312:34 rows rather than each individuals. Users
312:38 

312:38 are then assigned rows based on their
312:41 

312:41 job functions. This simplifies the
312:44 

312:44 management by grouping permissions into
312:46 

312:46 roles that reflect organization
312:49 

312:49 structures. For example, think about
312:52 

312:52 that we are running a database for
312:54 

312:54 another higher education institution and
312:56 

312:56 we defined three roles including
312:59 

312:59 student, teacher and admin. We then
313:02 

313:02 associate the access permissions to each
313:05 

313:05 of the roles like this. After that we
313:10 

313:10 can assign the roles to different users
313:12 

313:12 who need to access the database like
313:15 

313:15 this. As you can see sometimes a single
313:18 

313:18 user can be assigned to multiple
313:20 

313:20 different roles. Row-based access
313:23 

313:23 control is used almost everywhere
313:25 

313:25 outside of a strict military or
313:27 

313:27 government settings especially in
313:29 

313:29 business, education, healthcare and
313:32 

313:32 online platforms because it's much
313:35 

313:35 easier to manage the mandatory access
313:38 

313:38 control and scales very well in
313:40 

313:40 comparison to the discretionary access
313:42 

313:42 control.
313:44 

313:44 When you use SQL to achieve role based
313:47 

313:47 access control, the syntax varies from
313:49 

313:49 one database to another.
313:52 

313:52 So here we will only show you an example
313:54 

313:54 in which there's some baseline
313:56 

313:56 consistency. After all, SQL standards do
313:60 

313:60 define a few of the clauses that are key
314:02 

314:02 to access control.
314:05 

314:05 Think about an online store named the
314:08 

314:08 sci-fi collective.
314:10 

314:10 A database may need to define three
314:12 

314:12 roles. Users, analysts, and
314:15 

314:15 administrators.
314:17 

314:17 By users, I mean the ones who are the
314:19 

314:19 customers of the Sci-Fi Collective and
314:21 

314:21 who shop online. The users have read and
314:25 

314:25 write access to a few tables, but they
314:28 

314:28 can't delete or update anything. The SQL
314:31 

314:31 code of creating the user row may look
314:34 

314:34 like this for my SQL or Marend DB.
314:39 

314:39 It starts by creating the row using the
314:41 

314:41 create row clause followed by the name
314:44 

314:44 of the row up to your choice.
314:47 

314:47 After that we grant different access
314:50 

314:50 permissions to this row such as the
314:52 

314:52 permissions to view data insert new data
314:55 

314:55 into different tables. Dot notation is
314:58 

314:58 used here to concat the database name
315:02 

315:02 and the table name. As you can see the
315:05 

315:05 grant to clause is used to achieve this
315:08 

315:08 goal. Select means view permissions.
315:12 

315:12 Select on sci-fi collective product
315:15 

315:15 means the view permission on the product
315:18 

315:18 table.
315:20 

315:20 Insert means the permission to add a
315:22 

315:22 data. Insert on database name dot
315:25 

315:25 purchase means the permission to add a
315:27 

315:27 data into the purchase table. After
315:30 

315:30 these two steps, the row of users is
315:33 

315:33 created.
315:35 

315:35 Whenever you have a new user who should
315:37 

315:37 have this row, it will grant it to that
315:40 

315:40 user. For the admin row, you will follow
315:43 

315:43 the same three steps to create the row
315:46 

315:46 first, then grant different permissions
315:48 

315:48 on different tables to this row. In the
315:51 

315:51 end, when you have a new user who should
315:53 

315:53 have this row, it will assign the admin
315:56 

315:56 role to him or her. One thing worth
315:58 

315:58 noting here is that you can combine
316:01 

316:01 multiple permissions such as viewing,
316:04 

316:04 adding new data, updating or deleting
316:06 

316:06 data of the same table to this row and
316:09 

316:09 pack them into a single grant to clause.
316:12 

316:12 The different permissions are separated
316:15 

316:15 using commas.
316:17 

316:17 That's everything about access control.
316:20 

316:20 Let's move to discuss encryption. Now
316:23 

316:23 encryption is about protecting data by
316:25 

316:25 converting it into a coded format. So if
316:29 

316:29 the data is stolen, it can't be read in
316:32 

316:32 plain text at all.
316:34 

316:34 The two most commonly used encryption
316:36 

316:36 approaches include one-way encryption
316:39 

316:39 and symmetric encryption. Maybe we'll
316:42 

316:42 look at the two types of encryptions one
316:44 

316:44 after another. Starting with the one-way
316:47 

316:47 encryption. Oneway encryption, also
316:50 

316:50 known as hashing, is a cryptographic
316:52 

316:52 process where data is transformed into a
316:55 

316:55 fixed size string of characters, which
316:58 

316:58 is typically a hash value or hash code.
317:02 

317:02 The key characteristic of one-way
317:04 

317:04 encryption is that it's designed to be
317:07 

317:07 oneway function. It's easy to compute
317:10 

317:10 the hash value from the input data but
317:13 

317:13 extremely difficult or say practically
317:15 

317:15 impossible to reverse the process and
317:18 

317:18 obtain the original input data from the
317:21 

317:21 hash value. This makes it useful for
317:24 

317:24 scenarios where you need to verify data
317:27 

317:27 without storing the original data in a
317:30 

317:30 readable format. Some of the popular
317:32 

317:32 one-way encryption algorithms include
317:35 

317:35 BCrypt, PDKDF2,
317:38 

317:38 and SH 512.
317:41 

317:41 To enhance security, a unique salt value
317:45 

317:45 or see a randomly generated string is
317:47 

317:47 often added to each password before
317:50 

317:50 hashing. This prevents the attackers
317:53 

317:53 from using precomputed hash tables to
317:56 

317:56 crack passwords. Regardless of the
317:58 

317:58 varying password length, the hashed
318:01 

318:01 results always have the same length as
318:04 

318:04 some binary data. It's a common and
318:07 

318:07 recommended practice to actually encode
318:09 

318:09 the hashed by bite data into strings and
318:12 

318:12 store them using car in your database.
318:16 

318:16 Encoded hashes can be printed, locked,
318:19 

318:19 copied, and debugged more easily. Plus
318:22 

318:22 encoded strings are safe to include in
318:25 

318:25 URLs or JSON because the raw binary data
318:29 

318:29 may include null values, controlled
318:32 

318:32 characters or invalid encoding
318:34 

318:34 sequences. Let's see an example where
318:37 

318:37 oneway encryption is applied and explore
318:40 

318:40 its implications for database design.
318:44 

318:44 Think about the database of the same
318:46 

318:46 online store, the sci-fi collective.
318:48 

318:48 Again,
318:50 

318:50 in your first iteration of database
318:52 

318:52 design, you developed a table named user
318:55 

318:55 to store all the information of the
318:58 

318:58 customers. There's a column named
319:00 

319:00 password that currently stores passwords
319:03 

319:03 without any encryption. And now with the
319:06 

319:06 newly acquired knowledge on encryption,
319:09 

319:09 you decide to use brypt as a one-way
319:13 

319:13 encryption algorithm to encrypt the
319:15 

319:15 passwords.
319:18 

319:18 Brypt generates salt internally and
319:21 

319:21 results in a 23 byt binary data after
319:24 

319:24 hashing. When you convert the binary
319:26 

319:26 data to strings using base 64 encoding,
319:30 

319:30 all hashed passwords will be 60
319:32 

319:32 characters long. Since salts are unique
319:36 

319:36 to each password, you do need to store
319:39 

319:39 them. But BCrypt handles that as well.
319:43 

319:43 So all you need to do is to make sure
319:45 

319:45 that the password column has enough
319:48 

319:48 space to store the strings in the hashed
319:51 

319:51 format like this. As you can see, the
319:56 

319:56 change introduced to the database design
319:58 

319:58 is very small yet critical. It relies on
320:02 

320:02 your understanding that passwords cannot
320:05 

320:05 be stored in plain text, which
320:07 

320:07 encryption algorithms to use and how
320:10 

320:10 that algorithm handles things in its own
320:13 

320:13 way. If a different encryption algorithm
320:16 

320:16 is used, things could be slightly
320:18 

320:18 different. For example, if you decide to
320:22 

320:22 use SHA 512 to handle the one-way
320:25 

320:25 encryption, things will be slightly
320:28 

320:28 different. This encryption algorithm
320:31 

320:31 doesn't generate salt internally
320:34 

320:34 but still requires salt for hashing.
320:37 

320:37 When passwords of different lenses are
320:40 

320:40 hashed using this algorithm, they all
320:43 

320:43 end up as 64 byt binary data. When such
320:47 

320:47 data is encoded using base 64 as
320:50 

320:50 strings, they will all be 88 characters
320:54 

320:54 long.
320:55 

320:55 Remember, SHA 512 still requires salt
320:60 

320:60 for hashing. So, the backend server
321:03 

321:03 needs to generate salt, use it to
321:06 

321:06 encrypt the passwords before ever
321:08 

321:08 storing them into databases. Beyond
321:11 

321:11 that, since salts are unique to each
321:14 

321:14 hashed passwords, you do need to store
321:17 

321:17 them just in case you need to repeat the
321:20 

321:20 same process in the future for the same
321:23 

321:23 given password.
321:25 

321:25 You can choose to prepare a separate
321:28 

321:28 column named the password salt to store
321:31 

321:31 the salt as binary data or you can
321:35 

321:35 follow the lead of brypt and concata
321:38 

321:38 salt with its corresponding hashed
321:40 

321:40 password as a single piece of data and
321:44 

321:44 then encode it as a string. This
321:47 

321:47 decision may not be all yours to make
321:49 

321:49 because it requires the backend server
321:52 

321:52 to cooperate. Regardlessly, such a
321:55 

321:55 decision will impact the design of the
321:58 

321:58 user table.
321:60 

322:00 Now, let's move to talk about the
322:02 

322:02 symmetric encryption. Symmetric
322:05 

322:05 encryption is like having a single key
322:08 

322:08 that both locks and unlocks a box. You
322:11 

322:11 use the same secret key to encrypt the
322:14 

322:14 data when you need to store it into the
322:16 

322:16 database and you use the same key to
322:20 

322:20 decrypt the data when you need to read
322:22 

322:22 it back. This is different from oneway
322:25 

322:25 encryption or say hashing where data is
322:28 

322:28 scrambled but can never be unscrumbled.
322:31 

322:31 In contrast, symmetric encryption is
322:34 

322:34 reversible, meaning that you can always
322:37 

322:37 get your original data back if you have
322:40 

322:40 the key, making it ideal for storing
322:43 

322:43 sensitive information that you need to
322:45 

322:45 retrieve later, like credit card numbers
322:48 

322:48 or personal details.
322:51 

322:51 Common symmetric encryption algorithms
322:53 

322:53 include advanced encryption standard,
322:56 

322:56 tripleds, and a blowfish. Each algorithm
322:59 

322:59 has a set of variants depending on the
323:02 

323:02 key lenses such as 128 bits or 256 bits.
323:08 

323:08 AES has three variants as 128,
323:15 

323:15 192 and 256.
323:18 

323:18 The key length determines the level of
323:21 

323:21 security and the computation that's
323:24 

323:24 required for encryption and decryption.
323:27 

323:27 The longer the key is, the higher the
323:29 

323:29 security and the computational
323:32 

323:32 requirements are. Let's use AES 256 as
323:37 

323:37 an example of symmetric encryption and
323:40 

323:40 think about its application to the same
323:43 

323:43 database that we just visited. As you
323:46 

323:46 can imagine, the database needs to store
323:49 

323:49 the payment information from its
323:51 

323:51 customers.
323:52 

323:52 The table for this task is named payment
323:55 

323:55 method. When a customer or a user buys
323:58 

323:58 anything and takes the option to receive
324:01 

324:01 the payment information for future use,
324:04 

324:04 this table stores the credit card
324:05 

324:05 information accordingly, including the
324:08 

324:08 card number and expiration date. As it
324:11 

324:11 stands now, neither of the two columns
324:14 

324:14 are encrypted.
324:16 

324:16 In case if you wonder okay we are
324:19 

324:19 storing the credit card numbers the
324:22 

324:22 expiration date but why don't we store
324:24 

324:24 the CVC numbers too it's all about
324:27 

324:27 following the payment card industry data
324:29 

324:29 security standards
324:31 

324:31 which recommends not to store the CVC
324:34 

324:34 numbers at all after all CVC number is
324:38 

324:38 either requested again from users or not
324:40 

324:40 necessary when the card is used again
324:43 

324:43 for future transactions assuming that is
324:46 

324:46 the first The transaction of course
324:47 

324:47 would through successfully based on this
324:50 

324:50 standard. Card numbers and expiration
324:53 

324:53 date can be stored in your own database
324:56 

324:56 but both need to be encrypted. As you
324:59 

324:59 know a card number is 16 characters long
325:02 

325:02 and expiration date is four characters
325:05 

325:05 long if you skip the slash that
325:08 

325:08 separates the month and year numbers. As
325:11 

325:11 it stands now, the table fails to
325:14 

325:14 encrypt either of the two columns
325:16 

325:16 considering their data sizes. If we use
325:19 

325:19 AES 256
325:21 

325:21 to encrypt both columns, we need to get
325:25 

325:25 on the same page in terms of how AES
325:27 

325:27 works. First, these algorithms use a
325:31 

325:31 256bit
325:33 

325:33 secret key for encryption and a
325:35 

325:35 decryption. It operates on blocks of 16
325:38 

325:38 byt of data. If the input data is not a
325:42 

325:42 multiple of 16 byt, one block will
325:45 

325:45 require some padding so that you always
325:47 

325:47 have a set of 16 byte blocks. Beyond
325:50 

325:50 that, AES uses a 16 byt initialization
325:55 

325:55 vector to strengthen encryption.
325:57 

325:57 Although the initialization vector is
325:60 

325:60 not a secret, it has to be unique per
326:03 

326:03 encryption. For example, think about how
326:07 

326:07 an expiration date August 2029 is
326:11 

326:11 encrypted using AS 256.
326:15 

326:15 First, the four characters I mean 08 29
326:21 

326:21 need to be padded and encoded as a set
326:24 

326:24 of 16 byte blocks of data. After that,
326:28 

326:28 it is concatenated with the
326:31 

326:31 initialization vector. In the end, the
326:34 

326:34 combined string will be encrypted using
326:37 

326:37 AES algorithm and encoded as a string.
326:42 

326:42 This process will turn both a 16
326:45 

326:45 character long card number and a four
326:47 

326:47 character long expiration date into 64
326:51 

326:51 characters. In other words, if you
326:54 

326:54 intend to encrypt both the card numbers
326:57 

326:57 and the expiration dates, you need to
326:60 

327:00 increase the data sizes of the two
327:02 

327:02 corresponding columns to 64 characters.
327:05 

327:05 In addition to that, you will need to
327:07 

327:07 store the initialization vectors for
327:09 

327:09 both the card numbers and expiration
327:12 

327:12 dates because an initialization vector
327:15 

327:15 is unique per encryption. If you were to
327:18 

327:18 store the initialization vectors
327:20 

327:20 separately from the encrypted data, you
327:23 

327:23 will need two more separate columns that
327:26 

327:26 store binary data of 16 byt. It's
327:29 

327:29 typically unnecessary to encode the
327:32 

327:32 initialization vectors as strings. So
327:35 

327:35 binary data will do. If you don't like
327:38 

327:38 this option, you may choose to encode
327:40 

327:40 the initialization vectors as strings
327:43 

327:43 and concat them to the encrypted data.
327:46 

327:46 For example, always add as a beginning
327:48 

327:48 of the encrypted card numbers and as a
327:51 

327:51 beginning of encrypted expression date.
327:54 

327:54 In such a case, the encrypted card
327:56 

327:56 number plus initialization vector will
327:59 

327:59 be 88 characters long. The same can be
328:02 

328:02 said for expression data column. As you
328:06 

328:06 can see, encryption only leads to change
328:09 

328:09 as minor as increasing the data size of
328:12 

328:12 the target column in a database. But a
328:15 

328:15 lot of thinking and understanding goes
328:17 

328:17 behind the same. You may also wonder if
328:20 

328:20 the secret that's used for encryption
328:22 

328:22 and decryption needs to be stored into
328:24 

328:24 the database as well. The answer is no.
328:27 

328:27 Absolutely no. Each initialization
328:30 

328:30 vector is unique per encryption. So they
328:33 

328:33 need to be stored and mapped to each
328:35 

328:35 encrypted data. However, the key that's
328:38 

328:38 used for encryption and decryption
328:41 

328:41 remains the same all the time. So it is
328:44 

328:44 typically stored in key management
328:46 

328:46 services or hardware security modules.
328:49 

328:49 Regardlessly, you shouldn't store the
328:52 

328:52 key in the same database because it
328:55 

328:55 makes a single point of compromise. If
328:57 

328:57 an attacker gains access to your
328:60 

328:60 database, they get both the encrypted
329:02 

329:02 credit card numbers and the key to
329:05 

329:05 decrypt them. This makes the encryption
329:08 

329:08 essentially worthless. It's like locking
329:10 

329:10 your front door but leaving the key
329:12 

329:12 under the doormat.
329:15 

329:15 In this video, we covered two different
329:18 

329:18 aspects of database security, including
329:21 

329:21 access control and encryption. If you
329:23 

329:23 find this video helpful, give it a thumb
329:26 

329:26 up, subscribe, and leave your questions
329:28 

329:28 in the comments. I would love to hear
329:30 

329:30 what's working for you and what needs to
329:33 

329:33 change. This video is based on the book
329:35 

329:35 graing relational database design which
329:38 

329:38 dives deep into the database design
329:40 

329:40 using real world examples to explain the
329:43 

329:43 concepts. You'll be able to find a link
329:45 

329:45 to the book in the first comment. Have
329:48 

329:48 you ever wondered how the web
329:49 

329:49 applications you use on a databases can
329:52 

329:52 find whatever you need quickly when you
329:54 

329:54 perform a searching task like a shopping
329:57 

329:57 app or a movie streaming service. Index
330:00 

330:00 is one of the powerful tools that makes
330:03 

330:03 this possible. An index is like a book's
330:07 

330:07 table of content for your database. It
330:09 

330:09 creates a separate organized lookup
330:12 

330:12 structure that points to where the data
330:14 

330:14 is actually stored. So the database can
330:17 

330:17 find rows quickly without scanning the
330:19 

330:19 entire table. Instead of checking every
330:22 

330:22 single row to find what you need, the
330:25 

330:25 index lets the database jump directly to
330:28 

330:28 the right location. However, indexes are
330:31 

330:31 not created equal. Bri indexes help with
330:34 

330:34 filtering, sorting, or ordering, but
330:37 

330:37 they struggle with natural language
330:39 

330:39 search. That's where full text indexes
330:42 

330:42 shine. In this video, we will cover two
330:45 

330:45 major types of indexes that are widely
330:48 

330:48 used in relational databases that
330:50 

330:50 support scalable applications, including
330:53 

330:53 the Bry index and a full text index. Of
330:56 

330:56 course, there are other types of indexes
330:59 

330:59 as well, such as clustered index and
331:02 

331:02 hash index, but we won't cover them
331:05 

331:05 here. In combination, B tree and full
331:08 

331:08 text indexes cover about 85 to 90% of
331:12 

331:12 indexing tasks in typical application
331:14 

331:14 development. Many developers work their
331:17 

331:17 entire career using primarily Bri
331:20 

331:20 indexes.
331:22 

331:22 Full index is common enough in modern
331:25 

331:25 apps that is worth knowing. As a result,
331:28 

331:28 we will focus our attention only on
331:30 

331:30 these two types of indexes.
331:33 

331:33 Let's start with the Bry index. So a B
331:36 

331:36 tree index organizes data in a sorted
331:39 

331:39 treel like structure with multiple
331:41 

331:41 levels. When you create a B tree index
331:44 

331:44 for a column in a table of your
331:46 

331:46 database, the database builds this
331:49 

331:49 additional B tree structure and stores
331:51 

331:51 it alongside your table data. So it's
331:55 

331:55 basically a copy of your indexed columns
331:57 

331:57 organized in a tree format with pointers
332:00 

332:00 back to the actual table rows.
332:04 

332:04 When data is inserted, updated or
332:06 

332:06 deleted, their corresponding operations
332:09 

332:09 will be slightly slower because you need
332:11 

332:11 to add data to this B tree data
332:14 

332:14 structure or update or delete data from
332:16 

332:16 it. However, an advantage of a B tree
332:20 

332:20 index is how fast it can speed up
332:23 

332:23 searching and sorting due to its
332:25 

332:25 balanced structure. Every path from top
332:28 

332:28 to bottom has the same length,
332:31 

332:31 guaranteeing that there would be a
332:33 

332:33 consistent and a faster lookup.
332:36 

332:36 Typically finding any record in just
332:38 

332:38 three to four steps, even in a table
332:41 

332:41 with millions of rows. Let's look at an
332:44 

332:44 example of how to create a B tree index
332:47 

332:47 and what that may look like.
332:50 

332:50 Think about a table named movie from a
332:53 

332:53 database that supports a movie streaming
332:55 

332:55 web app. The table comes with five
332:58 

332:58 attributes including movie ID, title,
333:01 

333:01 date, rating, and a photo. The rating
333:04 

333:04 column stores the average rating from
333:07 

333:07 all users. The developers of this app
333:11 

333:11 noticed that their users frequently sort
333:14 

333:14 movies by their ratings. Since this
333:17 

333:17 column is not a primary key, it doesn't
333:19 

333:19 come with an automatically generated
333:22 

333:22 index. Oh, by the way, when a column is
333:25 

333:25 designated as the primary key in a
333:27 

333:27 table, a B tree index is typically
333:30 

333:30 automatically generated for that column
333:32 

333:32 in most database systems. It helps speed
333:36 

333:36 up potential searching and sorting tasks
333:38 

333:38 on the primary key. Since rating is not
333:42 

333:42 a primary key, it doesn't come with an
333:44 

333:44 automatically generated Bry index. So to
333:48 

333:48 speed up searching or sorting on this
333:50 

333:50 column, you need to create an index
333:52 

333:52 yourself.
333:54 

333:54 To achieve that, all it takes is a
333:57 

333:57 simple SQL command. Create index then is
334:01 

334:01 going to be followed by the index name.
334:04 

334:04 Then on the target table in the bracket
334:07 

334:07 that follows the table name, you will
334:09 

334:09 include the columns that you want to
334:11 

334:11 create an index on. So what does this
334:14 

334:14 SQL command do?
334:17 

334:17 It leads to the creation of a B tree
334:20 

334:20 data structure using the data from the
334:23 

334:23 rating column alongside your table data.
334:26 

334:26 When searching or sorting is performed
334:28 

334:28 on the rating data from the movie table,
334:31 

334:31 the database system will be able to use
334:33 

334:33 this tree structure to speed up things.
334:36 

334:36 Similarly, if you are the database admin
334:38 

334:38 of this database and notice that sorting
334:41 

334:41 is performed frequently on the date
334:43 

334:43 column, I mean it's natural for users to
334:46 

334:46 sort movies by release dates so that
334:49 

334:49 they can check out the latest movies
334:52 

334:52 since date is not a primary key either.
334:55 

334:55 So you need to create an index on it to
334:57 

334:57 speed up the corresponding sorting task.
335:00 

335:00 You can do it in the same manner as you
335:02 

335:02 create the index on the rating column.
335:05 

335:05 So far you have seen two examples in
335:08 

335:08 which index is created on individual
335:11 

335:11 columns. What if it involves more than
335:14 

335:14 one column?
335:16 

335:16 Say the streaming app comes with a set
335:18 

335:18 of filters that allow users to filter or
335:21 

335:21 sort movies by more than one attributes
335:24 

335:24 like both rating and the release date.
335:28 

335:28 If you notice the corresponding query is
335:30 

335:30 frequently triggered, then you need to
335:33 

335:33 create an index on the combination of
335:35 

335:35 the two columns like this.
335:39 

335:39 All it takes is to put both of the two
335:42 

335:42 columns in the same bracket and separate
335:45 

335:45 them using a comma. Remember, you need
335:48 

335:48 the help from B3 index when you need
335:50 

335:50 fast lookups, filtering, sorting on a
335:53 

335:53 non- primary key column. You typically
335:56 

335:56 don't take preemptive measures on
335:58 

335:58 indexing until you know enough about how
336:01 

336:01 users use your application. The
336:03 

336:03 empirical data is golden because it
336:06 

336:06 tells you where searching, filtering or
336:08 

336:08 sorting actually happen frequently. Many
336:11 

336:11 developers work their entire career
336:13 

336:13 using only the B tree index because it
336:16 

336:16 is indeed the most important index.
336:19 

336:19 Standing alone, it probably accounts for
336:22 

336:22 80 to 85% of indexing tasks in
336:25 

336:25 relational databases. Dry index is great
336:28 

336:28 for many things that have been mentioned
336:30 

336:30 like fast lookups, filtering, sorting,
336:32 

336:32 or range queries, unstructured data like
336:35 

336:35 numbers, states, or exact matches. But
336:38 

336:38 it doesn't work well when you have to
336:40 

336:40 deal with a lot of texts. Imagine that
336:43 

336:43 you are building a blog and want users
336:45 

336:45 to search for articles containing the
336:47 

336:47 word database.
336:49 

336:49 With a regular B tree index, you could
336:52 

336:52 only match exact values or use slow
336:55 

336:55 pattern matching like this query.
336:59 

336:59 This query uses a like clause that scans
337:02 

337:02 every row which is painfully slow on
337:05 

337:05 large tables. Speeding up the search in
337:08 

337:08 a large amount of text data requires the
337:11 

337:11 help from a different type of index. The
337:14 

337:14 full text index. A full text index is
337:18 

337:18 specifically designed for searching
337:19 

337:19 within text content. It works by
337:22 

337:22 breaking down your text into individual
337:24 

337:24 words. We call them tokens. Then
337:27 

337:27 removing common words like uh and the
337:31 

337:31 then creating a searchable catalog of
337:34 

337:34 where each meaningful word appears.
337:37 

337:37 Think of it like the index at the back
337:40 

337:40 of the textbook, but for every
337:43 

337:43 significant word in your content. What
337:46 

337:46 makes full text indexes special is how
337:49 

337:49 it builds this data structure to support
337:52 

337:52 searching of text. Results of full text
337:55 

337:55 index aren't just a match or no match.
337:58 

337:58 Full text index can score results by how
338:02 

338:02 well they match your search terms.
338:04 

338:04 Putting the most relevant results first.
338:07 

338:07 With the help of full text search, you
338:10 

338:10 can search for phrases handle plural
338:13 

338:13 forms and even find similar words. For
338:17 

338:17 example, with the help from full text
338:20 

338:20 index, searching running might also find
338:23 

338:23 a run and runs.
338:26 

338:26 Last, full text index makes searching
338:29 

338:29 much faster. Instead of scanning entire
338:32 

338:32 text columns, full text index lets you
338:36 

338:36 instantly find out the rows containing
338:38 

338:38 specific words even across millions of
338:42 

338:42 records.
338:43 

338:43 Now let's use an example to make further
338:46 

338:46 sense of when and how to use the full
338:48 

338:48 text index and what that may lead to.
338:53 

338:53 Think about the movie table again, but
338:55 

338:55 this time it comes with a small
338:58 

338:58 modification, which is an extra column
339:00 

339:00 named the description. This column
339:03 

339:03 stores the descriptive information of
339:05 

339:05 each movie. So, it's all text of course
339:08 

339:08 and each row would probably be very long
339:11 

339:11 for this target column. If the streaming
339:13 

339:13 app of this database supports users to
339:17 

339:17 search for movies by typing keywords or
339:19 

339:19 anything into a search bar and primarily
339:23 

339:23 matches the input against the movie
339:25 

339:25 description,
339:27 

339:27 it is only natural that we speed up
339:29 

339:29 searching by creating a full text index
339:32 

339:32 on the description column. The syntax of
339:36 

339:36 creating a full text index varies a lot
339:38 

339:38 from one database system to another. So
339:41 

339:41 here we will only quickly show you one
339:44 

339:44 example that works with postrass and I
339:46 

339:46 focus on what a structure is actually
339:49 

339:49 created instead.
339:51 

339:51 To add a full text index on the
339:54 

339:54 description column using postgress, you
339:56 

339:56 need to add an extra column that
339:59 

339:59 conforms to a special data structure and
340:02 

340:02 is then create an index on that newly
340:05 

340:05 created column. So what exactly is
340:08 

340:08 generated when a full text index is
340:11 

340:11 created?
340:13 

340:13 Think about a movie table with three
340:15 

340:15 rows and each row contains a description
340:18 

340:18 for the corresponding movie. When a full
340:21 

340:21 text index is created on this table, it
340:24 

340:24 will generate a structure that looks
340:27 

340:27 like this. As you can see, this
340:30 

340:30 structure stores the mapping from each
340:32 

340:32 word to the corresponding movie. You can
340:36 

340:36 think this as flipping the original data
340:38 

340:38 or say inverting the original data which
340:42 

340:42 makes searching extremely fast. You can
340:45 

340:45 directly look up each searching term and
340:47 

340:47 then get back the matching movies
340:49 

340:49 instantly rather than scanning through
340:52 

340:52 every movie's description. You may also
340:54 

340:54 noticed that some common words like uh
340:58 

340:58 and the are filtered out in this
341:01 

341:01 inverted index structure. So in a sense
341:04 

341:04 we only kept the meaningful words. This
341:08 

341:08 structure is the reason why full text
341:10 

341:10 index can speed up searching tasks in
341:13 

341:13 texts.
341:15 

341:15 In this video we covered two major types
341:18 

341:18 of indexing in relational databases
341:20 

341:20 including Bry index and a full text
341:23 

341:23 index. If you find this video helpful
341:26 

341:26 give it a thumb up, subscribe and leave
341:28 

341:28 your questions in the comments. I would
341:31 

341:31 love to hear what's working for you and
341:33 

341:33 what needs to change. This video is
341:35 

341:35 based on the book graing relational
341:37 

341:37 database design which dives deeper into
341:39 

341:39 database design using real world
341:41 

341:41 examples to explain concepts. You can
341:44 

341:44 find the link to the book in the first
341:46 

341:46 comment. We spend so much time learning
341:49 

341:49 how to normalize databases,
341:52 

341:52 remove redundancies
341:54 

341:54 and split table apart so that we can
341:56 

341:56 avoid anomalies. But here's the twist.
341:59 

341:59 Sometimes we put redundancies back on
342:03 

342:03 purpose. Sounds backward, right? Why
342:06 

342:06 would we undo what we just worked so
342:09 

342:09 hard to achieve? Think of a normalized
342:12 

342:12 database like a clean modular circuit.
342:15 

342:15 Every component has one job. But
342:18 

342:18 sometimes performance demands a shortcut
342:20 

342:20 while connecting distant part directly.
342:23 

342:23 That shortcut saves time but makes the
342:26 

342:26 system more fragile. The normalization
342:29 

342:29 is that shortcut in database design.
342:32 

342:32 Imagine a dashboard that joins five
342:34 

342:34 tables to display a report. It's
342:37 

342:37 perfectly normalized, but every query
342:40 

342:40 takes seconds. Your users don't care
342:43 

342:43 about perfect design. They care about
342:46 

342:46 speed. Denormalization is one of the
342:49 

342:49 ways that we trade a little data purity
342:51 

342:51 for performance. In this video, we'll
342:55 

342:55 see what denormalization really means,
342:57 

342:57 why it's sometimes necessary, and how to
343:00 

343:00 do it responsibly. Let's start by
343:03 

343:03 properly defining denormalization first.
343:06 

343:06 Denormalization is the last ditch
343:09 

343:09 database optimization technique that
343:11 

343:11 intentionally introduces some redundancy
343:14 

343:14 to a database to improve query
343:16 

343:16 performance. It typically involves two
343:20 

343:20 steps. The first step is to identify
343:23 

343:23 query that are used frequently and
343:26 

343:26 require joining more than two tables.
343:29 

343:29 The identification process requires you
343:31 

343:31 to observe how users use your database
343:34 

343:34 and again empirical data first. The
343:37 

343:37 second step is to decide whether and how
343:40 

343:40 to duplicate some columns in the
343:42 

343:42 identified tables so that it can reduce
343:45 

343:45 the number of tables that are required
343:48 

343:48 to achieve the same result. Let's
343:51 

343:51 illustrate this via some examples.
343:54 

343:54 Think about a music app like Spotify.
343:58 

343:58 You design this database and you are
344:00 

344:00 responsible for optimizing it. Now, one
344:03 

344:03 thing that you noticed is that the query
344:06 

344:06 that retrieves top songs of famous
344:09 

344:09 artists are executed frequently.
344:13 

344:13 Yet famous artists have typically issued
344:16 

344:16 many different albums and sounds are
344:19 

344:19 associated with album table instead of
344:22 

344:22 artist table in the database. That said,
344:26 

344:26 this query always needs to join three
344:28 

344:28 tables together in order to retrieve top
344:31 

344:31 sounds from famous artists like what you
344:35 

344:35 have seen here on the screen. two one to
344:38 

344:38 manying relationships
344:40 

344:40 and a cubersome SQL query that joins all
344:43 

344:43 the three tables you saw.
344:46 

344:46 Now you have completed the first step in
344:49 

344:49 denormalization. You have identified the
344:52 

344:52 query that is frequently used and
344:55 

344:55 require joining in more than two tables
344:57 

344:57 and you have found a solid evidence that
344:60 

344:60 this query takes a bit longer than what
345:02 

345:02 users expect for a very basic task.
345:06 

345:06 After trying all different optimization
345:08 

345:08 techniques, you realize that none of
345:11 

345:11 them works very well. So you decided to
345:14 

345:14 apply the normalization to address this
345:16 

345:16 issue.
345:18 

345:18 We already know that the only way to
345:20 

345:20 associate an artist to a song is via the
345:24 

345:24 album table. To reduce this middleman
345:27 

345:27 table, you can build a relationship
345:29 

345:29 between artist table and a sound table
345:32 

345:32 directly. The link between the two
345:35 

345:35 tables allows you to bypass the album
345:38 

345:38 table whenever you need to find the top
345:40 

345:40 songs from a famous artist
345:43 

345:43 which will simplify the SQL query. Of
345:46 

345:46 course, the old query on the left side
345:49 

345:49 needs to join three tables, but now you
345:52 

345:52 just can join two tables to achieve the
345:55 

345:55 same result at a faster speed. Sounds
345:58 

345:58 awesome, right? Such a change doesn't
346:01 

346:01 come without a cost. If you focus your
346:04 

346:04 attention on the album table, you can
346:06 

346:06 tell that it's in a classical one to
346:09 

346:09 many relationship with artist table. And
346:12 

346:12 as a foreign key, artist ID is in the
346:15 

346:15 album table. Since the primary key in a
346:18 

346:18 table functionally determines all other
346:20 

346:20 attributes, we have this functional
346:23 

346:23 dependency.
346:25 

346:25 From album ID to artist ID or say album
346:29 

346:29 ID functionally determines artist ID
346:32 

346:32 because album ID is the primary key in
346:35 

346:35 the album table and artist ID is not a
346:38 

346:38 primary key as a foreign key. It's
346:41 

346:41 functionally determined by the primary
346:43 

346:43 key.
346:45 

346:45 Similarly, if you focus your attention
346:47 

346:47 on the sound table, now it's in two one
346:50 

346:50 to many relationships with both the
346:52 

346:52 artist table and the album table. And
346:56 

346:56 both the foreign keys are placed in the
346:59 

346:59 sound table, including album ID and
347:02 

347:02 artist ID. Since a primary key in a
347:05 

347:05 table functionally determines all other
347:07 

347:07 attributes, we have this functional
347:10 

347:10 dependency that goes from some ID to
347:13 

347:13 album ID or say the song ID functionally
347:18 

347:18 determines album ID.
347:21 

347:21 If you put the two identified functional
347:23 

347:23 dependencies side by side, you can
347:26 

347:26 clearly see that they would form a
347:29 

347:29 transitive dependency like this, which
347:32 

347:32 violates the third normal form.
347:35 

347:35 Now you have completed the second step
347:38 

347:38 in denormalization and perhaps have a
347:40 

347:40 better understanding of why it's named
347:43 

347:43 denormalization
347:44 

347:44 because this change will lead to a
347:47 

347:47 violation of normal forms. That's the
347:50 

347:50 cost of increased speed and a simplified
347:52 

347:52 SQL query. To make sure that we are on
347:55 

347:55 the same page, let's look at another
347:57 

347:57 example. Think about a database that
348:01 

348:01 supports an online shopping store that
348:03 

348:03 you design and manage. After a few
348:06 

348:06 months of operation, you noticed that
348:09 

348:09 this query that joins three tables
348:11 

348:11 together is performed frequently. The
348:14 

348:14 three tables include user, payment
348:17 

348:17 methods, and purchases. As users view
348:21 

348:21 their order history, they will keep
348:23 

348:23 triggering the query that joins users
348:25 

348:25 and purchase table. Yet, the two tables
348:28 

348:28 are not directly related to each other.
348:31 

348:31 To join the two tables, you have to go
348:34 

348:34 through a middle man, which is the
348:36 

348:36 payment method table. As time goes, you
348:39 

348:39 realize that this query is triggered so
348:42 

348:42 frequently that it warrants some
348:44 

348:44 optimization.
348:46 

348:46 That said, you have completed the first
348:49 

348:49 step in doing normalization. You have
348:52 

348:52 identified a query that is frequently
348:54 

348:54 used and require joining more than two
348:57 

348:57 tables and you have found evidence that
348:60 

348:60 this query takes longer than what users
349:02 

349:02 normally expect for a basic task. After
349:06 

349:06 trying all different optimization
349:08 

349:08 techniques, you realize that none of
349:11 

349:11 them works very well. So you decided to
349:13 

349:13 apply denormalization to address this
349:15 

349:15 issue. To reduce the number of tables
349:19 

349:19 that need to be joined, you can add a
349:22 

349:22 direct relationship between user and
349:25 

349:25 purchase tables. By doing so, you can
349:28 

349:28 bypass the payment method table to
349:30 

349:30 complete the same task.
349:33 

349:33 As you can see, this change will lead
349:36 

349:36 you to join two tables instead of three
349:39 

349:39 tables to achieve the same result. The
349:43 

349:43 updated SQL query is much more
349:45 

349:45 simplified in comparison. Of course, the
349:48 

349:48 newly gained efficiency and simplicity
349:51 

349:51 has some cost. If you focus your
349:54 

349:54 attention on the payment method table,
349:56 

349:56 it's in a classical one to many
349:58 

349:58 relationship with user table. And as the
350:01 

350:01 foreign key is placed in the payment
350:03 

350:03 method table, we know that the primary
350:06 

350:06 key functionally determines all other
350:08 

350:08 attributes including the foreign key. So
350:11 

350:11 from payment method table, we can deduce
350:14 

350:14 this functional dependency that goes
350:17 

350:17 from payment ID to email or say payment
350:20 

350:20 ID functionally determines email.
350:24 

350:24 Now if you focus your attention on the
350:26 

350:26 purchase table, you can see that it's in
350:30 

350:30 two one to many relationships with both
350:32 

350:32 payment method and user tables and it
350:36 

350:36 contains two foreign keys including
350:38 

350:38 payment ID and email. Since the primary
350:41 

350:41 key determines all other attributes in a
350:44 

350:44 table, we would have this functional
350:46 

350:46 dependency that goes from purchase ID to
350:49 

350:49 payment ID.
350:51 

350:51 If you put these two functional
350:52 

350:52 dependencies side by side, you can tell
350:55 

350:55 that they can clearly be chained up to
350:58 

350:58 form a transitive dependency like this,
351:01 

351:01 which is in violation of the third
351:04 

351:04 normal form.
351:07 

351:07 Now you have completed the second step
351:10 

351:10 in denormalization and I gone through
351:12 

351:12 two examples. It's time for us to
351:15 

351:15 discuss the best practice on
351:16 

351:16 denormalization.
351:19 

351:19 Denormalization may sound tempting, but
351:22 

351:22 you want to be careful and never misuse
351:24 

351:24 it.
351:26 

351:26 For example, you don't want to
351:28 

351:28 denormalize unless you have measured a
351:30 

351:30 real performance problem and tried other
351:33 

351:33 approaches first.
351:37 

351:37 For another example, if your tables are
351:39 

351:39 updated often, like inventory, balances,
351:42 

351:42 or user profiles, redundant fields will
351:46 

351:46 quickly fall out of sync.
351:48 

351:48 In such cases, denormalization can lead
351:51 

351:51 to disastrous results. Think about when
351:54 

351:54 you need to duplicate a customer's
351:56 

351:56 address in multiple tables. As addresses
351:60 

351:60 are updated, when your customers move,
352:03 

352:03 their data will become a nightmare.
352:06 

352:06 or when your database has a relatively
352:09 

352:09 small size, drawing on small or indexed
352:13 

352:13 tables are usually very fast even if
352:16 

352:16 multiple tables are involved. In such a
352:18 

352:18 case, denormalization gives no
352:21 

352:21 measurable benefits or if you work in a
352:24 

352:24 corporate environment in which multiple
352:26 

352:26 teams or systems write to the same
352:29 

352:29 tables, denormalization would complicate
352:32 

352:32 coordination and increase the risk of
352:34 

352:34 inconsistency.
352:36 

352:36 Now you may think that I have cautioned
352:38 

352:38 you against the denormalization and I
352:40 

352:40 wonder what other techniques you should
352:42 

352:42 try first. Let's talk about them. First
352:46 

352:46 you certainly want to start with
352:48 

352:48 indexing the columns that are frequently
352:50 

352:50 used for sorting ordering and filtering.
352:55 

352:55 Next based on indexing you want to
352:57 

352:57 rewrite your queries to use better
352:60 

352:60 drawing order window functions or
353:02 

353:02 aggregations. This often brings huge
353:05 

353:05 gain without schema change
353:09 

353:09 indexing and a curial optimization. You
353:12 

353:12 also want to try caching application
353:14 

353:14 level cache like radius or meme cacheed
353:18 

353:18 can help reduce repeated read load or
353:21 

353:21 even better you can keep your database
353:23 

353:23 design normalized and only denormalize
353:26 

353:26 it in cache. If all these techniques
353:30 

353:30 fail and you still urgently need more
353:32 

353:32 efficiency, then you move to
353:35 

353:35 denormalization.
353:37 

353:37 If you have to denormalize,
353:39 

353:39 you want to document redundancies
353:41 

353:41 explicitly in your rhyme file and work
353:45 

353:45 with your backend developers to use
353:47 

353:47 triggers to propagate changes
353:49 

353:49 automatically once the source data
353:52 

353:52 changes.
353:53 

353:53 He also wanted to run consistency checks
353:56 

353:56 to detect divergence between redundant
353:59 

353:59 copies and verify that a performance
354:01 

354:01 improvement would justify the added
354:04 

354:04 maintenance complexity.
354:07 

354:07 In this video, we covered what
354:09 

354:09 denormalization is, used two examples to
354:13 

354:13 explain its consequence and trade-offs,
354:16 

354:16 and discussed the best practice on the
354:18 

354:18 normalization. If you find this video
354:20 

354:20 helpful, give it a thumb up, subscribe,
354:23 

354:23 and leave your questions in the
354:25 

354:25 comments. I would love to hear what's
354:27 

354:27 working for you and what needs to
354:29 

354:29 change. This video is based on the book
354:31 

354:31 Groing Relational Database Design, which
354:34 

354:34 dives deeper into database design using
354:36 

354:36 real world examples to explain the
354:39 

354:39 concepts. You can find the link to the
354:41 

354:41 book in the first comment.